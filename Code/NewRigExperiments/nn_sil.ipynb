{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import getsizeof\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "path=\"/its/home/drs25/RoboSkin/Code/NewRigExperiments/\"\n",
    "datapath=\"/its/home/drs25/datasets/\"\n",
    "if os.name == 'nt':\n",
    "    path=\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/\"\n",
    "    datapath=\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/texture-tactip/\"\n",
    "from IPython.display import clear_output\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:18230\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "csfont = {'fontname':'Times New Roman'}\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loaded:\n",
    "    def __init__(self,t=20,filename=\"X_data_15.npz\"):\n",
    "        data = np.load(datapath+filename) #load data\n",
    "        for array_name in data:\n",
    "            self.X=(data[array_name].astype(np.uint8))\n",
    "        data = np.load(datapath+filename.replace(\"X\",\"y\")) #load data\n",
    "        for array_name in data:\n",
    "            self.y=(data[array_name].astype(np.uint8))\n",
    "        self.keys=['Leather', 'Cork', 'wool', 'LacedMatt', 'Gfoam', 'Plastic', 'Carpet', 'bubble', 'Efoam', 'cotton', 'LongCarpet', 'Flat', 'felt', 'Jeans', 'Ffoam']\n",
    "\n",
    "        print(\"Dataset size:\",self.X.shape[0],\"\\nWindow size:\",self.X.shape[1],\"\\nImage:\",self.X.shape[2:])\n",
    "        print(\"Memory needed:\",round(getsizeof(self.X)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "        assert self.X.shape[0]==self.y.shape[0],\"Incorrect data size match y=\"+str(self.y.shape[0])+\" x=\"+str(self.X.shape[0])\n",
    "        self.X=self.X[:,0:t]\n",
    "        #randomize order\n",
    "        n_samples = self.X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        shuffled_data = self.X[indices]\n",
    "        shuffled_labels = self.y[indices]\n",
    "        self.X=shuffled_data\n",
    "        self.y=shuffled_labels\n",
    "    def shuffle(self):\n",
    "        n_samples = self.X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        shuffled_data = self.X[indices]\n",
    "        shuffled_labels = self.y[indices]\n",
    "        self.X=shuffled_data\n",
    "        self.y=shuffled_labels\n",
    "    def augment(self):\n",
    "        #create rotations\n",
    "        self.AugmentedX=np.zeros((len(self.X)*3,*self.X.shape[1:]),dtype=np.uint8)\n",
    "        self.Augmentedy=np.zeros_like(np.concatenate((self.y,self.y,self.y)))\n",
    "        for k,i in enumerate(range(0,len(self.AugmentedX),3)): #loop through the normal data and new data\n",
    "            for j in range(len(self.X[0])):\n",
    "                self.AugmentedX[i][j]=np.copy(self.X[k][j])\n",
    "                self.AugmentedX[i+1][j]=cv2.resize(cv2.rotate(self.X[k][j].copy(), cv2.ROTATE_90_CLOCKWISE),(self.X[k][j].shape[1],self.X[k][j].shape[0]),interpolation=cv2.INTER_AREA)\n",
    "                self.AugmentedX[i+2][j]=cv2.resize(cv2.rotate(self.X[k][j].copy(), cv2.ROTATE_180),(self.X[k][j].shape[1],self.X[k][j].shape[0]),interpolation=cv2.INTER_AREA)\n",
    "                self.Augmentedy[i+1]=self.y[k]\n",
    "                self.Augmentedy[i+2]=self.y[k]\n",
    "                self.Augmentedy[i]=self.y[k]\n",
    "                #self.AugmentedX[i+3][j]=cv2.rotate(self.X[k][j], cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        x,y=self.zoom_augment(self.AugmentedX.copy(),self.Augmentedy.copy(),[10,20,30,40])\n",
    "        print(x.shape,self.AugmentedX.shape)\n",
    "        self.AugmentedX=np.concatenate([self.AugmentedX,x])\n",
    "        self.Augmentedy=np.concatenate([self.Augmentedy,y])\n",
    "\n",
    "        print(\"Dataset size:\",self.AugmentedX.shape[0],\"\\nWindow size:\",self.X.shape[1],\"\\nImage:\",self.X.shape[2:])\n",
    "        print(\"Memory needed:\",round(getsizeof(self.AugmentedX)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "        self.X = self.AugmentedX\n",
    "        self.y = self.Augmentedy\n",
    "        n_samples = self.X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        shuffled_data = self.X[indices]\n",
    "        shuffled_labels = self.y[indices]\n",
    "        self.X=shuffled_data\n",
    "        self.y=shuffled_labels\n",
    "        del self.AugmentedX\n",
    "        del self.Augmentedy\n",
    "    def applySobel(self):\n",
    "        for i in range(len(self.X)): #crop all images individually\n",
    "            for j in range(len(self.X[0])):\n",
    "                image=self.X[i][j]\n",
    "                # Apply Sobel filter in x-direction\n",
    "                sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # ksize=3 for a 3x3 Sobel kernel\n",
    "\n",
    "                # Apply Sobel filter in y-direction\n",
    "                sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "                # Convert the results back to uint8\n",
    "                sobel_x = np.uint8(np.absolute(sobel_x))\n",
    "                sobel_y = np.uint8(np.absolute(sobel_y))\n",
    "\n",
    "                # Combine the results to get the final edge-detected image\n",
    "                sobel_combined = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "                self.X[i][j]=sobel_combined\n",
    "    def zoom_augment(self,dataX, dataY, zoom_factors):\n",
    "        \"\"\"\n",
    "        Augment a dataset by zooming into the central region and resizing back to original dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "            dataX (numpy array): Dataset of shape (n, t, h, w).\n",
    "            dataY (numpy array): Corresponding labels of shape (n, ...).\n",
    "            zoom_factors (list): List of zoom-in percentages (e.g., [10, 20, 30]).\n",
    "        \n",
    "        Returns:\n",
    "            augmented_dataX (list): List of augmented datasets, one for each zoom factor.\n",
    "            augmented_dataY (list): List of label arrays corresponding to each augmented dataset.\n",
    "        \"\"\"\n",
    "        n, t, h, w = dataX.shape\n",
    "        augmented_dataX = []\n",
    "        augmented_dataY = []\n",
    "        \n",
    "        for zoom in zoom_factors:\n",
    "            crop_margin = int((zoom / 100) * min(h, w) / 2)\n",
    "            cropped_and_resized = np.zeros_like(dataX)\n",
    "            \n",
    "            for i in range(n):\n",
    "                for j in range(t):\n",
    "                    # Crop the central region\n",
    "                    cropped = dataX[i, j, crop_margin:h-crop_margin, crop_margin:w-crop_margin]\n",
    "                    # Resize back to original dimensions\n",
    "                    cropped_and_resized[i, j] = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            augmented_dataX.append(cropped_and_resized)\n",
    "            augmented_dataY.append(dataY.copy())  # Labels remain the same\n",
    "        augmented_dataX=np.array(augmented_dataX)\n",
    "        augmented_dataY=np.array(augmented_dataY)\n",
    "        return augmented_dataX.reshape((len(zoom_factors)*augmented_dataX.shape[1],*augmented_dataX.shape[2:])), augmented_dataY.reshape((len(zoom_factors)*augmented_dataY.shape[1],))\n",
    "    def resize(self,percentage):\n",
    "        h=int(self.X.shape[2]*percentage)\n",
    "        w=int(self.X.shape[3]*percentage)\n",
    "        new_array=np.zeros((*self.X.shape[0:2],h,w))\n",
    "\n",
    "        for i in range(len(self.X)): #crop all images individually\n",
    "            for j in range(len(self.X[0])):\n",
    "                image=self.X[i][j]\n",
    "                iamge = cv2.resize(image,(w,h),interpolation=cv2.INTER_AREA)\n",
    "                new_array[i][j]=iamge\n",
    "        self.X=new_array.copy()\n",
    "#data=loaded(t=4)\n",
    "#data.applySobel()\n",
    "#data.augment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(frm,to,percentage=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    data=loaded(to,filename=\"X_data_newMorph.npz\") #X_data_15.npz\n",
    "    data.applySobel()\n",
    "    \"\"\"data2=loaded(to,filename=\"X_data_15.npz\")\n",
    "    data2.applySobel()\n",
    "    data.X=np.concatenate([data.X,data2.X])\n",
    "    data.y=np.concatenate([data.y,data2.y])\n",
    "    del data2\n",
    "    data3=loaded(to,filename=\"X_data_newMorph.npz\")\n",
    "    data3.applySobel()\n",
    "    data.X=np.concatenate([data.X,data3.X])\n",
    "    data.y=np.concatenate([data.y,data3.y])\n",
    "    del data3\"\"\"\n",
    "    data.resize(percentage)\n",
    "    data.augment()\n",
    "    data.shuffle()\n",
    "    #add lowest unseen \n",
    "    #d=loaded(t=4,filename=\"X_data_newMorph.npz\")\n",
    "    #d.applySobel()\n",
    "    #p20=d.X[np.where(d.y==3)]\n",
    "    #data.X=np.concatenate([data.X,p20])\n",
    "    #data.y=np.concatenate([data.y,np.zeros((len(p20)))+11])\n",
    "    #data.resize(percentage)\n",
    "    print(\"LOADED DATASET...\")\n",
    "    #data.augment()\n",
    "    n=int(len(data.X)*0.7)\n",
    "    # Example: if train_labels are strings, use LabelEncoder to convert them to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(data.y[0:n])\n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "    print(\"Memory left\",round(torch.cuda.mem_get_info()[1]/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    x_data=data.X[0:n].reshape((len(data.X[0:n]),1,abs(frm-to)*data.X.shape[2],data.X.shape[3]))\n",
    "    del data\n",
    "    x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "    \n",
    "    train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "    print(\"Using\",round(getsizeof(x_data)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    del x_data\n",
    "    train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "    # Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "\n",
    "    print(train_images_tensor.shape)\n",
    "    print(train_labels_tensor.shape)\n",
    "    \n",
    "    return train_loader,test_loader\n",
    "\n",
    "def genDataANN(frm,to,percentage=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    data=loaded(to,filename=\"X_data_15.npz\")\n",
    "    data.applySobel()\n",
    "    data.shuffle()\n",
    "    data.resize(percentage)\n",
    "    print(\"LOADED DATASET...\")\n",
    "    data.augment()\n",
    "    n=int(len(data.X)*0.7)\n",
    "    # Example: if train_labels are strings, use LabelEncoder to convert them to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(data.y[0:n])\n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "    print(\"Memory left\",round(torch.cuda.mem_get_info()[1]/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    x_data=data.X[0:n].reshape((len(data.X[0:n]),abs(frm-to)*data.X.shape[2]*data.X.shape[3]))\n",
    "    del data\n",
    "    x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "    \n",
    "    train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "    print(\"Using\",round(getsizeof(x_data)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    del x_data\n",
    "    train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "    # Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "\n",
    "    print(train_images_tensor.shape)\n",
    "    print(train_labels_tensor.shape)\n",
    "    \n",
    "    return train_loader,test_loader\n",
    "def gen3DData(frm,to,percentage=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    data=loaded(frm=frm,t=to)\n",
    "    data.applySobel()\n",
    "    data.augment()\n",
    "    data.resize(percentage)\n",
    "    n=int(len(data.X)*0.6)\n",
    "    # Example: if train_labels are strings, use LabelEncoder to convert them to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(data.y[0:n])\n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "    print(\"Memory left\",round(torch.cuda.mem_get_info()[1]/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    x_data=data.X[0:n].reshape((len(data.X[0:n]),1,abs(frm-to),data.X.shape[2],data.X.shape[3]))\n",
    "    x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "    train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "    print(\"Using\",round(getsizeof(x_data)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "    # Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "\n",
    "    print(train_images_tensor.shape)\n",
    "    print(train_labels_tensor.shape)\n",
    "    unique={}\n",
    "    for i in range(len(train_labels_encoded)):\n",
    "        if unique.get(train_labels_encoded[i],False)==False:\n",
    "            unique[train_labels_encoded[i]]=data.y[0:n][i]\n",
    "    return train_loader,test_loader,unique\n",
    "def genLSTMData(frm,to,percentage=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    data=loaded(to,filename=\"X_data_15.npz\")\n",
    "    #data.applySobel()\n",
    "    #data=loaded(to,filename=\"X_data_15.npz\")\n",
    "    data.applySobel()\n",
    "    #data3=loaded(to,filename=\"X_data_gel_15.npz\")\n",
    "    #data3.applySobel()\n",
    "    #data.X=np.concatenate([data.X,data2.X,data3.X])\n",
    "    #data.y=np.concatenate([data.y,data2.y,data3.y])\n",
    "    #del data2\n",
    "    #del data3\n",
    "    data.augment()\n",
    "    #add lowest unseen \n",
    "    #d=loaded(t=4,filename=\"X_flat_unseen_pressures.npz\")\n",
    "    #d.applySobel()\n",
    "    #data.augment()\n",
    "    data.resize(percentage)\n",
    "    n=-1#int(len(data.X)*0.7)\n",
    "    # Example: if train_labels are strings, use LabelEncoder to convert them to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(data.y[0:n])\n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "    print(\"Memory left\",round(torch.cuda.mem_get_info()[1]/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    x_data=data.X[0:n].reshape((len(data.X[0:n]),abs(frm-to),data.X.shape[2]*data.X.shape[3]))\n",
    "    x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "    train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "    print(\"Using\",round(getsizeof(x_data)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "    # Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "\n",
    "    print(train_images_tensor.shape)\n",
    "    print(train_labels_tensor.shape)\n",
    "    unique={}\n",
    "    for i in range(len(train_labels_encoded)):\n",
    "        if unique.get(train_labels_encoded[i],False)==False:\n",
    "            unique[train_labels_encoded[i]]=data.y[0:n][i]\n",
    "    return train_loader,test_loader\n",
    "def genCNNLSTMData(frm,to,percentage=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    data=loaded(to,filename=\"X_data_newMorph.npz\")\n",
    "    data.applySobel()\n",
    "    data2=loaded(to,filename=\"X_data_15.npz\")\n",
    "    data2.applySobel()\n",
    "    data3=loaded(to,filename=\"X_data_gel_15.npz\")\n",
    "    data3.applySobel()\n",
    "    data.X=np.concatenate([data.X,data2.X,data3.X])\n",
    "    data.y=np.concatenate([data.y,data2.y,data3.y])\n",
    "    del data2\n",
    "    del data3\n",
    "    data.augment()\n",
    "\n",
    "    #data.resize(percentage)\n",
    "    n=-1#int(len(data.X)*0.7)\n",
    "    # Example: if train_labels are strings, use LabelEncoder to convert them to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(data.y[0:n])\n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "    print(\"Memory left\",round(torch.cuda.mem_get_info()[1]/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    x_data=data.X[0:n].reshape((len(data.X[0:n]),1,abs(frm-to),data.X.shape[2],data.X.shape[3]))\n",
    "    x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "    train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "    print(\"Using\",round(getsizeof(x_data)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "    train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "    # Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader for training and testing sets\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "\n",
    "    print(train_images_tensor.shape)\n",
    "    print(train_labels_tensor.shape)\n",
    "    unique={}\n",
    "    for i in range(len(train_labels_encoded)):\n",
    "        if unique.get(train_labels_encoded[i],False)==False:\n",
    "            unique[train_labels_encoded[i]]=data.y[0:n][i]\n",
    "    return train_loader,test_loader,unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self,input_height, input_width,output=15):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.flatten_size = 10 * (input_height // 4) * (input_width // 4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), self.flatten_size)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self,input, hidden,output=15):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, output)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, input_depth, input_height, input_width):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(32, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        depth = input_depth\n",
    "        height = input_height\n",
    "        width = input_width\n",
    "        \n",
    "        depth = depth // 2  # after first pooling\n",
    "        height = height // 2\n",
    "        width = width // 2\n",
    "        \n",
    "        depth = depth // 2  # after second pooling\n",
    "        height = height // 2\n",
    "        width = width // 2\n",
    "        \n",
    "        # Number of output features from conv layers (channels * depth * height * width)\n",
    "        self.flatten_size = 10 * depth * height * width\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 13)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.flatten_size)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_width, input_height, hidden_size, num_layers, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        # Define the CNN part\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Calculate the CNN output size\n",
    "        cnn_output_size = self._get_cnn_output_size(input_width, input_height)\n",
    "        \n",
    "        # Define the LSTM part\n",
    "        self.lstm = nn.LSTM(input_size=cnn_output_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the final fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def _get_cnn_output_size(self, width, height):\n",
    "        # Create a dummy tensor with the given width and height\n",
    "        dummy_input = torch.zeros(1, 1, height, width)\n",
    "        \n",
    "        # Pass the dummy tensor through the CNN\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        \n",
    "        # Calculate the output size by flattening the output\n",
    "        return dummy_output.view(-1).size(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, c, sequence_length, h, w = x.size()\n",
    "        # Reshape input to (batch_size * sequence_length, c, h, w) for CNN\n",
    "        c_in = x.view(batch_size * sequence_length, c, h, w)\n",
    "        \n",
    "        # Pass through CNN\n",
    "        c_out = self.cnn(c_in)\n",
    "        \n",
    "        # Flatten the CNN output\n",
    "        c_out = c_out.view(c_out.size(0), -1)\n",
    "        \n",
    "        # Reshape to (batch_size, sequence_length, cnn_output_size) for LSTM\n",
    "        lstm_in = c_out.view(batch_size, sequence_length, -1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(lstm_in)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_loader,frm,to,num_epochs = 100):\n",
    "    train_history=[]\n",
    "    image=next(iter(train_loader))\n",
    "    image=image[0][0][0]\n",
    "    output=len(next(iter(train_loader))[1][0])\n",
    "    model = SimpleCNN(image.shape[0],image.shape[1],output=output).to(device)#.half()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Train the Model\n",
    "    \n",
    "    clip_value = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape,outputs.shape,labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        train_history.append(loss.cpu().detach().numpy())\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss :.4f}')\n",
    "    print('Finished Training')\n",
    "    return model,train_history\n",
    "\n",
    "def runANN(train_loader,frm,to,num_epochs = 100):\n",
    "    train_history=[]\n",
    "    image=next(iter(train_loader))\n",
    "    image=image[0][0]\n",
    "    output=len(next(iter(train_loader))[1][0])\n",
    "    print(\"SHAPE....\",image.shape)\n",
    "    model = SimpleANN(image.shape[0],400,output=output).to(device)#.half()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Train the Model\n",
    "    \n",
    "    clip_value = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape,outputs.shape,labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        train_history.append(loss.cpu().detach().numpy())\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss :.4f}')\n",
    "    print('Finished Training')\n",
    "    return model,train_history\n",
    "\n",
    "def run3D(train_loader,frm,to):\n",
    "    train_history=[]\n",
    "    image=next(iter(train_loader))\n",
    "    image=image[0][0][0]\n",
    "    model = Simple3DCNN(image.shape[0],image.shape[1],image.shape[2]).to(device)#.half()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Train the Model\n",
    "    num_epochs = 100\n",
    "    clip_value = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape,outputs.shape,labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        train_history.append(loss.cpu().detach().numpy())\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss :.4f}')\n",
    "    print('Finished Training')\n",
    "    return model,train_history\n",
    "def calc(model,data_loader):\n",
    "        correct=0\n",
    "        summed=0.1\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            outputs = model(inputs)\n",
    "            a=torch.argmax(outputs,axis=1)==torch.argmax(labels,axis=1)\n",
    "\n",
    "            summed+=len(inputs)\n",
    "            correct+=len(a[a==1])\n",
    "        print(\"Accuracy:\",(correct/summed)*100,\"%\")\n",
    "        return correct/summed\n",
    "def runLSTM(train_loader,frm,to,num_epochs = 100):\n",
    "    train_history=[]\n",
    "    image=next(iter(train_loader))\n",
    "    image=image[0][0][0]\n",
    "    output=len(next(iter(train_loader))[1][0])\n",
    "    model = SimpleLSTM(image.shape[0],350,output,3).to(device)#.half()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Train the Model\n",
    "    \n",
    "    clip_value = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape,outputs.shape,labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        train_history.append(loss.cpu().detach().numpy())\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss :.4f}')\n",
    "    print('Finished Training')\n",
    "    return model,train_history\n",
    "def runLSTMcnn(train_loader,frm,to,num_epochs=150):\n",
    "    train_history=[]\n",
    "    image=next(iter(train_loader))\n",
    "    image=image[0][0][0][0]\n",
    "    output=len(next(iter(train_loader))[1][0])\n",
    "    model = CNN_LSTM(image.shape[1],image.shape[0],1000,1,output).to(device)#.half()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Train the Model\n",
    "    \n",
    "    clip_value = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape,outputs.shape,labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        train_history.append(loss.cpu().detach().numpy())\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss :.4f}')\n",
    "    print('Finished Training')\n",
    "    return model,train_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_loader,test_loader=genDataANN(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,history=runANN(train_loader,0,4,num_epochs = 100)\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "torch.save(model.state_dict(), path+\"/model/mymodelANN\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelANN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_loader,test_loader=genData(0,4)\n",
    "model,history=run(train_loader,0,4,num_epochs = 100)\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "torch.save(model.state_dict(), path+\"/model/mymodelCNN_augmented\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test=[]\n",
    "accuracy_train=[]\n",
    "\n",
    "for i in range(20):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader,test_loader=genData(0,4)\n",
    "    model,history=run(train_loader,0,4)\n",
    "    print(calc(model,test_loader))\n",
    "    print(calc(model,train_loader))\n",
    "    torch.save(model.state_dict(), path+\"/model/mymodel_CNN\")\n",
    "    accuracy_test.append(calc(model,test_loader))\n",
    "    accuracy_train.append(calc(model,train_loader))\n",
    "\n",
    "np.save(path+\"/data/train_CNN_NM_accuracies_20_trials\",np.array(accuracy_train))\n",
    "np.save(path+\"/data/test_CNN_NM_accuracies_20_trials\",np.array(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)\n",
    "plt.grid(True)\n",
    "plt.title(\"Loss of CNN on both datasets\",fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(path+\"/images/loss_of_uber_model.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## play with alt dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3000 \n",
      "Window size: 20 \n",
      "Image: (110, 120)\n",
      "Memory needed: 0.74 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1605527/3293731508.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path+\"/model/mymodelCNN_augmented\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=33000, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=15, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=loaded(4)\n",
    "image=data.X[0][0]\n",
    "data\n",
    "model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodelCNN_augmented\"))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4, 110, 120)\n",
      "(50, 1, 440, 120)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data=np.load(\"/its/home/drs25/RoboSkin/Code/NewRigExperiments/texture-tactip/concatenated_data_handheld.npy\").astype(np.uint8)\n",
    "labels=np.load(\"/its/home/drs25/RoboSkin/Code/NewRigExperiments/texture-tactip/concatenated_labels_handheld.npy\").astype(np.uint8)\n",
    "\n",
    "test_data_sliced=test_data[:,::4,:,:]\n",
    "test_data_sliced=test_data_sliced[:,10:10+4,:,:]\n",
    "crop=[60,180,40,150]\n",
    "im=test_data_sliced[:,:,crop[2]:crop[3],crop[0]:crop[1]]\n",
    "new=np.zeros((im.shape[0],im.shape[1],im.shape[2],im.shape[3]))\n",
    "for i in range(len(test_data_sliced)): #crop all images individually\n",
    "    for j in range(len(test_data_sliced[0])):\n",
    "        image=cv2.resize(test_data_sliced[i][j],(int(640*0.4),int(480*0.4)),interpolation=cv2.INTER_AREA)\n",
    "        image=cv2.cvtColor(image[crop[2]:crop[3],crop[0]:crop[1]], cv2.COLOR_BGR2GRAY)\n",
    "        # Apply Sobel filter in x-direction\n",
    "        sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # ksize=3 for a 3x3 Sobel kernel\n",
    "\n",
    "        # Apply Sobel filter in y-direction\n",
    "        sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "        # Convert the results back to uint8\n",
    "        sobel_x = np.uint8(np.absolute(sobel_x))\n",
    "        sobel_y = np.uint8(np.absolute(sobel_y))\n",
    "\n",
    "        # Combine the results to get the final edge-detected image\n",
    "        sobel_combined = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "        new[i][j]=sobel_combined\n",
    "print(new.shape)\n",
    "new=new.reshape(new.shape[0],1,4*new.shape[2],new.shape[3])\n",
    "print(new.shape)\n",
    "new=(new-np.mean(new))/(np.max(new)-np.min(new))\n",
    "test_data_sliced_tensor=torch.tensor(new,dtype=torch.float64).float().to(device)\n",
    "plt.imshow(new[1][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "0.0 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "preds=model(test_data_sliced_tensor)\n",
    "names=['Carpet', 'LacedMatt', 'wool', 'Cork', 'Felt', 'LongCarpet', 'cotton', 'Plastic', 'Flat', 'Ffoam', 'Gfoam', 'bubble', 'Efoam', 'jeans', 'Leather']\n",
    "the_other_labels=['Cork','FoamF','FoamG','LacedMatt','Leather','LongCarpet','Plastic','ShortCarpet','Bubble','FoamE']\n",
    "mapping={0:7,1:3,3:0,5:5,7:6,9:1,10:2,10:8,11:9,13:4}\n",
    "classes=torch.argmax(preds,axis=1)\n",
    "correct=0\n",
    "print(len(labels),len(classes), classes.cpu().numpy().tolist())\n",
    "for i in range(len(classes)):\n",
    "    if mapping.get(classes[i],-1) >0:\n",
    "        if mapping[classes[i]]==labels[i]: correct+=1\n",
    "print(correct/len(classes) *100,\"% Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar=[]\n",
    "acc=[]\n",
    "for i in range(5):\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        train_loader,test_loader,unique=genData(0,4)\n",
    "        model,history=run(train_loader,0,4)\n",
    "        ar.append(history)\n",
    "        print(calc(model,test_loader))\n",
    "        print(calc(model,train_loader))\n",
    "        acc.append(calc(model,test_loader))\n",
    "        if acc[-1]>=max(acc):\n",
    "            torch.save(model.state_dict(), path+\"/model/mymmodel_\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")\n",
    "    except MemoryError as e:\n",
    "        try:\n",
    "            del train_loader\n",
    "            del test_loader\n",
    "            del model\n",
    "        except: \n",
    "            pass\n",
    "ar=np.array(ar)\n",
    "acc=np.array(acc)\n",
    "\n",
    "\n",
    "np.save(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/saves/accuracies_of_NM_uber\",acc)\n",
    "np.save(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/saves/loss_of_NM_uber\",ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T EXPERIEMTN\n",
    "\n",
    "ar_=[]\n",
    "acc_=[]\n",
    "for j in range(1,20,2):\n",
    "    clear_output(wait=True)\n",
    "    print(\"t SIZE:\",j)\n",
    "    ar=[]\n",
    "    acc=[]\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loader,test_loader,unique=genLSTMData(0,j)\n",
    "            model,history=runLSTM(train_loader,0,j)\n",
    "            ar.append(history)\n",
    "            print(calc(model,test_loader))\n",
    "            print(calc(model,train_loader))\n",
    "            acc.append(calc(model,test_loader))\n",
    "            if acc[-1]>=max(acc):\n",
    "                torch.save(model.state_dict(), path+\"/models/lstm_\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")\n",
    "            del train_loader\n",
    "            del test_loader\n",
    "            del model\n",
    "        except MemoryError as e:\n",
    "            try:\n",
    "                del train_loader\n",
    "                del test_loader\n",
    "                del model\n",
    "            except: \n",
    "                pass\n",
    "    ar=np.array(ar)\n",
    "    acc=np.array(acc)\n",
    "    ar_.append(ar)\n",
    "    acc_.append(acc)\n",
    "    np.save(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/saves/LSTMaccuracies_of_NM\",np.array(acc_))\n",
    "    np.save(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/saves/LSTMloss_of_NM\",np.array(ar_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores=np.zeros((10,14,3))\n",
    "train_scores=np.zeros((10,14,3))\n",
    "t_averages=np.zeros((10*14*3))\n",
    "\n",
    "#test_scores=np.load(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/saves/test_scores.npy\")\n",
    "#train_scores=np.load(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/saves/train_scores.npy\")\n",
    "c=0\n",
    "for i in range(1,10): #loop through frm dimention\n",
    "    for j in range(i+1,15): #loop though to dimention\n",
    "        print(\">>>>\",i,j,\"\\nMax:\",np.max(test_scores)*100,\"%\",\"\\nEstimated time left:\",(np.average(t_averages[t_averages!=0])*len(t_averages[t_averages==0]))/60,\"minutes\")\n",
    "        for trial in range(3):\n",
    "            t=time.time()\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loader,test_loader,__=genData(i,j)\n",
    "            model=run(train_loader,i,j)\n",
    "            test_scores[i][j-1][trial]=calc(model,test_loader)\n",
    "            train_scores[i][j-1][trial]=calc(model,train_loader)\n",
    "            torch.cuda.empty_cache()\n",
    "            del train_loader\n",
    "            del test_loader\n",
    "            del model\n",
    "            t2=time.time()\n",
    "            t_averages[c]=t2-t\n",
    "            c+=1\n",
    "        clear_output(wait=True)\n",
    "        np.save(path+\"saves/test_scores_NM_\",test_scores)\n",
    "        np.save(path+\"saves/train_scores_NM\",train_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resolutions=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.7,0.8,0.9,1]\n",
    "scores_test=np.zeros((len(resolutions),3))#np.load(path+\"/data/test_resolutions_new.npy\")#np.load(path+\"/data/test_resolutions_uber.npy\")#np.zeros((len(resolutions),3))\n",
    "scores_train=np.zeros((len(resolutions),3))#np.load(path+\"/data/train_resolutions_new.npy\")#np.load(path+\"/data/train_resolutions_uber.npy\")#np.zeros((len(resolutions),3))\n",
    "#ind=resolutions.index(0.7)\n",
    "ind=0\n",
    "try:\n",
    "    for i, res in enumerate((resolutions[ind:])): #\n",
    "        i+=ind\n",
    "        print(\"Testing resolution\",res)\n",
    "        for j in range(3): #three trials on each\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loader,test_loader=genData(0,4,res)\n",
    "            model,history=run(train_loader,0,4,num_epochs = 130)\n",
    "            test_acc=calc(model,test_loader)\n",
    "            train_acc=calc(model,train_loader)\n",
    "            #look at other stuff\n",
    "            del train_loader\n",
    "            del test_loader\n",
    "            torch.cuda.empty_cache()\n",
    "            #scores_test_NM[i][j]=len(a[a==0])/len(preds)\n",
    "            scores_test[i][j]=test_acc\n",
    "            scores_train[i][j]=train_acc\n",
    "            print(\"\\t\\t\",test_acc*100,\"%\",train_acc*100,\"%\")\n",
    "            del model\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(res)\n",
    "np.save(path+\"/data/test_resolutions_new\",scores_test)\n",
    "np.save(path+\"/data/train_resolutions_new\",scores_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path+\"/data/test_resolutions_uber\",scores_test)\n",
    "np.save(path+\"/data/train_resolutions_uber\",scores_train)\n",
    "#np.save(path+\"/data/resolutions_uberNM\",scores_test_NM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_test=np.average(scores_test,axis=1)*100\n",
    "max_test=np.max(scores_test,axis=1)*100\n",
    "average_train=np.average(scores_train,axis=1)*100\n",
    "max_train=np.max(scores_train,axis=1)*100\n",
    "average_NM=np.average(scores_test_NM,axis=1)*100\n",
    "max_NM=np.max(scores_test_NM,axis=1)*100\n",
    "\n",
    "\n",
    "plt.plot(average_test,c=\"r\",label=\"Average test\")\n",
    "plt.plot(max_test,\"--\",c=\"r\",label=\"Max test\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(average_train,c=\"b\",label=\"Average train\")\n",
    "plt.plot(max_train,\"--\",c=\"b\",label=\"Max train\")\n",
    "plt.title(\"Accuracy vs resolution\")\n",
    "plt.plot(average_NM,c=\"g\",label=\"Average new morphology\")\n",
    "plt.plot(max_NM,\"--\",c=\"g\",label=\"Max new morphology\")\n",
    "plt.grid(True)\n",
    "plt.xlim([0,0.7])\n",
    "plt.xticks([i for i in range(0,len(resolutions),2)],labels=[resolutions[i] for i in range(0,len(resolutions),2)])\n",
    "plt.xlabel(\"Resolution\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path+\"/images/NM_resolution_uber.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3000 \n",
      "Window size: 20 \n",
      "Image: (110, 120)\n",
      "Memory needed: 0.74 GB\n",
      "(36000, 4, 110, 120) (9000, 4, 110, 120)\n",
      "Dataset size: 45000 \n",
      "Window size: 4 \n",
      "Image: (110, 120)\n",
      "Memory needed: 2.21 GB\n",
      "Memory left 23.59 GB\n",
      "Using 17.7 GB\n",
      "torch.Size([44999, 4, 13200])\n",
      "torch.Size([44999, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1212728/963571293.py:160: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [900/900], Loss: 2436.5461\n",
      "Epoch [11/100], Step [900/900], Loss: 2043.8872\n",
      "Epoch [21/100], Step [900/900], Loss: 1492.0620\n",
      "Epoch [31/100], Step [900/900], Loss: 1170.4946\n"
     ]
    }
   ],
   "source": [
    "train_loader,test_loader=genLSTMData(0,4)\n",
    "model,history=runLSTM(train_loader,0,4)\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "torch.save(model.state_dict(), path+\"/model/mymodel_lstm_augment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test=[]\n",
    "accuracy_train=[]\n",
    "\n",
    "for i in range(20):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader,test_loader=genLSTMData(0,4)\n",
    "    model,history=runLSTM(train_loader,0,4)\n",
    "    print(calc(model,test_loader))\n",
    "    print(calc(model,train_loader))\n",
    "    torch.save(model.state_dict(), path+\"/model/mymodel_lstm_nm\")\n",
    "    accuracy_test.append(calc(model,test_loader))\n",
    "    accuracy_train.append(calc(model,train_loader))\n",
    "\n",
    "np.save(path+\"/data/train_LSTM_NM_accuracies_20_trials\",np.array(accuracy_train))\n",
    "np.save(path+\"/data/test_LSTM_NM_accuracies_20_trials\",np.array(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "torch.cuda.empty_cache()\n",
    "train_loader,test_loader,unique=genCNNLSTMData(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader,unique=genCNNLSTMData(0,4)\n",
    "model,history=runLSTMcnn(train_loader,0,4,num_epochs=120)\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "torch.save(model.state_dict(), path+\"/model/uber_lstmCNN\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resolutions=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.7,0.8,0.9,1]\n",
    "scores_test=np.zeros((len(resolutions),3))#np.load(path+\"/data/test_resolutions_new.npy\")#np.load(path+\"/data/test_resolutions_uber.npy\")#np.zeros((len(resolutions),3))\n",
    "scores_train=np.zeros((len(resolutions),3))#np.load(path+\"/data/train_resolutions_new.npy\")#np.load(path+\"/data/train_resolutions_uber.npy\")#np.zeros((len(resolutions),3))\n",
    "#ind=resolutions.index(0.7)\n",
    "ind=0\n",
    "try:\n",
    "    for i, res in enumerate((resolutions[ind:])): #\n",
    "        i+=ind\n",
    "        print(\"Testing resolution\",res)\n",
    "        for j in range(3): #three trials on each\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loader,test_loader=genLSTMData(0,4,res)\n",
    "            print(\"*************\")\n",
    "            model,history=runLSTM(train_loader,0,4,num_epochs = 130)\n",
    "            print(\"*************\")\n",
    "            test_acc=calc(model,test_loader)\n",
    "            train_acc=calc(model,train_loader)\n",
    "            #look at other stuff\n",
    "            del train_loader\n",
    "            del test_loader\n",
    "            torch.cuda.empty_cache()\n",
    "            #scores_test_NM[i][j]=len(a[a==0])/len(preds)\n",
    "            scores_test[i][j]=test_acc\n",
    "            scores_train[i][j]=train_acc\n",
    "            print(\"\\t\\t\",test_acc*100,\"%\",train_acc*100,\"%\")\n",
    "            del model\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(res)\n",
    "np.save(path+\"/data/test_lstm_resolutions_new\",scores_test)\n",
    "np.save(path+\"/data/train_lstm_resolutions_new\",scores_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_cnn=np.load(path+\"/data/test_resolutions.npy\")*100\n",
    "tr_cnn=np.load(path+\"/data/train_resolutions.npy\")*100\n",
    "te_cnn_n=np.load(path+\"/data/test_resolutions_new.npy\")*100\n",
    "tr_cnn_n=np.load(path+\"/data/train_resolutions_new.npy\")*100\n",
    "\n",
    "te_lstm=np.load(path+\"/data/test_lstm_resolutions.npy\")*100\n",
    "tr_lstm=np.load(path+\"/data/train_lstm_resolutions.npy\")*100\n",
    "te_lstm_n=np.load(path+\"/data/test_lstm_resolutions_new.npy\")*100\n",
    "tr_lstm_n=np.load(path+\"/data/train_lstm_resolutions_new.npy\")*100\n",
    "\n",
    "plt.plot(np.average(tr_cnn,axis=1),c=\"b\",label=\"CNN standard marker train\")\n",
    "plt.plot(np.average(te_cnn,axis=1),\"--\",c=\"b\",label=\"CNN standard marker test\")\n",
    "plt.plot(np.average(tr_cnn_n,axis=1),c=\"g\",label=\"CNN new marker train\")\n",
    "plt.plot(np.average(te_cnn_n,axis=1),\"--\",c=\"g\",label=\"CNN new marker test\")\n",
    "plt.plot(np.average(tr_lstm,axis=1),c=\"r\",label=\"LSTM standard marker train\")\n",
    "plt.plot(np.average(te_lstm,axis=1),\"--\",c=\"r\",label=\"LSTM standard marker test\")\n",
    "plt.plot(np.average(tr_lstm_n,axis=1),c=\"m\",label=\"LSTM new marker train\")\n",
    "plt.plot(np.average(te_lstm_n,axis=1),\"--\",c=\"m\",label=\"LSTM new marker test\")\n",
    "plt.grid(1)\n",
    "plt.xlabel(\"Resolution scale\")\n",
    "plt.xticks([i for i in range(len(resolutions))],resolutions)\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Average performance of models with varying resolution\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/images/resolution_models.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path+\"/model/mymodel_lstm_cnn\")\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.7,0.8,0.9,1]\n",
    "test_scores=np.zeros((len(resolutions),5))\n",
    "train_scores=np.zeros((len(resolutions),5))\n",
    "for i,res in enumerate(resolutions):\n",
    "    print(\"RESOLUTIONS\",i,\"/\",len(resolutions))\n",
    "    for trial in range(5):\n",
    "        train_loader,test_loader=genDataANN(0,4,res)\n",
    "        model,history=runANN(train_loader,0,4)\n",
    "        test_scores[i][trial]=calc(model,test_loader)\n",
    "        train_scores[i][trial]=calc(model,train_loader)\n",
    "        del train_loader\n",
    "        del model\n",
    "    np.save(path+\"/saves/ANNresolutions_train\",train_scores)\n",
    "    np.save(path+\"/saves/ANNresolutions_test\",test_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=next(iter(train_loader))\n",
    "print(image[0][0][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader,test_loader,unique=genData(0,4)\n",
    "    model,history=run(train_loader,0,4)\n",
    "    print(calc(model,test_loader))\n",
    "    print(calc(model,train_loader))\n",
    "except MemoryError as e:\n",
    "    try:\n",
    "        del train_loader\n",
    "        del test_loader\n",
    "        del model\n",
    "    except: \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d \n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader,test_loader,unique=gen3DData(0,4)\n",
    "    model,history=run3D(train_loader,0,4)\n",
    "    print(calc(model,test_loader))\n",
    "    print(calc(model,train_loader))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    del train_loader\n",
    "    del test_loader\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)\n",
    "plt.show()\n",
    "np.save(\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/data/ubermodellstmcnn.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=None\n",
    "real=None\n",
    "keys=['Carpet', 'LacedMatt', 'wool', 'Cork', 'Felt', 'LongCarpet', 'Cotton', 'Plastic', 'Flat', 'Ffoam', 'Gfoam', 'bubble', 'Efoam', 'Jeans', 'Leather']\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    outputs = model(inputs)\n",
    "    a=torch.argmax(outputs.detach(),axis=1)\n",
    "    if type(preds)==type(None):\n",
    "        preds=a.numpy().copy()\n",
    "        real=np.argmax(labels.numpy().copy(),axis=1)\n",
    "    else:\n",
    "        preds=np.concatenate([preds,a.numpy().copy()])\n",
    "        real=np.concatenate([real,np.argmax(labels.numpy().copy(),axis=1)])\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    un=np.unique(real)\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        matrix[t, p] += 1\n",
    "    return matrix/len(true_labels)\n",
    "\n",
    "print(real.shape,preds.shape)\n",
    "# Combine all predictions\n",
    "predictions = [preds]\n",
    "model_names = ['Neural Network']\n",
    "num_classes = len(np.unique(real))\n",
    "un=np.unique(real)\n",
    "indices = np.arange(num_classes)\n",
    "# Plotting confusion matrices for each model\n",
    "fig, axes = plt.subplots(1, len(predictions), figsize=(6, 6))\n",
    "\n",
    "for i, preds in enumerate(predictions):\n",
    "    cm = compute_confusion_matrix(real, preds, num_classes=len(np.unique(real)))\n",
    "    \n",
    "    ax = axes[i] if len(predictions) > 1 else axes\n",
    "    cax = ax.matshow(cm, cmap='Blues')\n",
    "    fig.colorbar(cax, ax=ax)\n",
    "    \n",
    "    for (j, k), value in np.ndenumerate(cm):\n",
    "        ax.text(k, j, f'{round(value*100) if round(value*100,1) == 0.0 else round(value*100,1)}', ha='center', va='center', color='red')\n",
    "    ax.set_title(f'Confusion Matrix for {model_names[i]}',fontsize=14)\n",
    "    ax.set_xlabel('Predicted Labels',fontsize=14)\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels([keys[unique[j]] for j in range(num_classes)],rotation=90,fontsize=14)\n",
    "    ax.set_ylabel('True Labels',fontsize=12)\n",
    "    ax.set_yticks(indices)\n",
    "    ax.set_yticklabels([keys[unique[j]] for j in range(num_classes)],rotation=0,fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/images/NM_confusionCNN.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodel\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(abs(0-4)*110,120,15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/uber_model\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform on unseen pressures\n",
    "\n",
    "#load in numpy\n",
    "data=np.load(datapath+\"datasets/X_texture_pout.npz\")\n",
    "for array_name in data:\n",
    "    x_unseen=(data[array_name].astype(np.uint8))\n",
    "data=np.load(datapath+\"datasets/y_texture_pout.npz\")\n",
    "for array_name in data:\n",
    "    y_unseen=(data[array_name].astype(np.uint8))\n",
    "print(x_unseen.shape)\n",
    "#cut temporal size\n",
    "x_unseen=x_unseen[:,0:4]\n",
    "X=np.zeros_like(x_unseen)\n",
    "#apply sobel filter\n",
    "for i in range(len(x_unseen)): #crop all images individually\n",
    "    for j in range(len(x_unseen[0])):\n",
    "        image=x_unseen[i][j]\n",
    "        # Apply Sobel filter in x-direction\n",
    "        sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # ksize=3 for a 3x3 Sobel kernel\n",
    "\n",
    "        # Apply Sobel filter in y-direction\n",
    "        sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "        # Convert the results back to uint8\n",
    "        sobel_x = np.uint8(np.absolute(sobel_x))\n",
    "        sobel_y = np.uint8(np.absolute(sobel_y))\n",
    "\n",
    "        # Combine the results to get the final edge-detected image\n",
    "        sobel_combined = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "        X[i][j]=sobel_combined\n",
    "unique_=[\"Flat\",\"wool\",\"Efoam\",\"LongCarpet\",\"LacedMatt\",\"cotton\",\"Gfoam\",\"Carpet\",\"felt\",\"Ffoam\",\"bubble\",\"Cork\",\"Jeans\"]\n",
    "keys={'Cork': 38, 'wool': 19, 'LacedMatt': 28, 'Gfoam': 30, 'Carpet': 31, 'bubble': 37, 'Efoam': 21, 'cotton': 29, 'LongCarpet': 25, 'Flat': 16, 'felt': 34, 'Jeans': 39, 'Ffoam': 36}\n",
    "        \n",
    "unique_={i:keys[unique_[i]] for i in range(len(unique_))}\n",
    "\n",
    "#concat\n",
    "X=X.reshape((4,len(X)//4,1,abs(0-4)*110,120))\n",
    "X=torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y=y_unseen.reshape(4,len(y_unseen)//4)\n",
    "print(X.shape,unique_)\n",
    "#run through model\n",
    "predsA=torch.argmax(model(X[0]),axis=1).detach().numpy()\n",
    "predsB=torch.argmax(model(X[1]),axis=1).detach().numpy()\n",
    "predsC=torch.argmax(model(X[2]),axis=1).detach().numpy()\n",
    "predsD=torch.argmax(model(X[3]),axis=1).detach().numpy()\n",
    "data=np.array([predsA,predsB,predsC,predsD])\n",
    "for i,batch in enumerate(data):\n",
    "    for key in np.unique(batch):\n",
    "        batch[batch==key]=unique_[key]\n",
    "data=data.reshape((len(x_unseen)))\n",
    "y=y.reshape((len(x_unseen)))\n",
    "\n",
    "#show accuracy (maybe confusion matrix of sorts)\n",
    "correct=0\n",
    "summed=0.1\n",
    "a=data==y\n",
    "summed+=len(x_unseen)\n",
    "correct+=len(a[a==1])\n",
    "print(\"Accuracy:\",(correct/summed)*100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUm of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader,test_loader=genData(0,4)\n",
    "#model,history=run(train_loader,0,4,num_epochs = 100)\n",
    "train_loader,test_loader=genLSTMData(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_loaders_by_classes(train_loader, test_loader, n):\n",
    "    # Get the class labels from the train_loader (assuming the label is in the second position of each batch)\n",
    "    train_classes = []  # Use a set to automatically handle uniqueness\n",
    "    for data, labels in train_loader:\n",
    "        # Flatten the labels and update the set with unique labels\n",
    "        l=np.argmax(labels.cpu().numpy().astype(np.uint8),axis=1)\n",
    "        train_classes.extend(l)  # Flatten to 1D and convert to numpy array\n",
    "    train_classes=np.array(train_classes).flatten()\n",
    "    train_classes = train_classes.tolist()  # Convert the set back to a list\n",
    "    \n",
    "    # Randomly select 'n' classes\n",
    "    selected_classes = random.sample(train_classes, n)\n",
    "    # Function to filter a loader by selected classes\n",
    "    def filter_loader(loader, selected_classes):\n",
    "        filtered_data = []\n",
    "        filtered_labels = []\n",
    "\n",
    "        for data, labels in loader:\n",
    "            batch_size = data.size(0)  # Get the batch size\n",
    "            # Iterate through the batch and check if any label in the sample matches the selected classes\n",
    "            for i in range(batch_size):\n",
    "                sample_labels = torch.argmax(labels[i])  # Labels for the i-th sample in the batch\n",
    "                if sample_labels.cpu().item() in selected_classes:\n",
    "                    filtered_data.append(data[i])  # Append the corresponding data sample\n",
    "                    filtered_labels.append(labels[i])  # Append the corresponding labels\n",
    "\n",
    "        # Convert lists back to tensors\n",
    "        filtered_data = torch.stack(filtered_data) if filtered_data else torch.Tensor()\n",
    "        filtered_labels = torch.stack(filtered_labels) if filtered_labels else torch.Tensor()\n",
    "        # Return a new DataLoader with the filtered data\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(filtered_data, filtered_labels),\n",
    "            batch_size=loader.batch_size\n",
    "        )\n",
    "\n",
    "    # Create the new filtered train and test loaders\n",
    "    filtered_train_loader = filter_loader(train_loader, selected_classes)\n",
    "    filtered_test_loader = filter_loader(test_loader, selected_classes)\n",
    "\n",
    "    return filtered_train_loader, filtered_test_loader\n",
    "\n",
    "test_acc=np.zeros((15,5))\n",
    "train_acc=np.zeros((15,5))\n",
    "\n",
    "for i in range(2,15):\n",
    "    for j in range(5):\n",
    "        print(\"Size\",i,\"trial\",j)\n",
    "        tl,tl2=filter_loaders_by_classes(train_loader, test_loader, i)\n",
    "        model,history=runLSTM(tl,0,4,num_epochs = 100)\n",
    "        test_acc[i][j]=calc(model,tl2)\n",
    "        train_acc[i][j]=calc(model,tl)\n",
    "\n",
    "np.save(path+\"/data/test_acc_varying_dataset_size_optical_lstm\",test_acc)\n",
    "np.save(path+\"/data/train_acc_varying_dataset_size_optical_lstm\",train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot code for above\n",
    "train_tactip_cnn=np.average(np.load(path+\"/data/train_acc_varying_dataset_size_optical_cnn.npy\"),axis=1)[2:]*100\n",
    "test_tactip_cnn=np.average(np.load(path+\"/data/test_acc_varying_dataset_size_optical_cnn.npy\"),axis=1)[2:]*100\n",
    "train_press_cnn=np.average(np.load(path+\"/data/train_acc_varying_dataset_size_electricalALL_lstm.npy\"),axis=1)[2:]\n",
    "test_press_cnn=np.average(np.load(path+\"/data/test_acc_varying_dataset_size_electricalALL_lstm.npy\"),axis=1)[2:]\n",
    "train_tactip_lstm=np.average(np.load(path+\"/data/train_acc_varying_dataset_size_optical_lstm.npy\"),axis=1)[2:]*100\n",
    "test_tactip_lstm=np.average(np.load(path+\"/data/test_acc_varying_dataset_size_optical_lstm.npy\"),axis=1)[2:]*100\n",
    "\n",
    "plt.plot(train_tactip_cnn,c=\"g\",label=\"Train CNN TacTip\")\n",
    "plt.plot(test_tactip_cnn,\"--\",c=\"g\",label=\"Test CNN TacTip\")\n",
    "\n",
    "plt.plot(train_press_cnn,c=\"r\",label=\"Train LSTM Electrical\")\n",
    "plt.plot(test_press_cnn,\"--\",c=\"r\",label=\"test LSTM Electrical\")\n",
    "\n",
    "plt.plot(train_tactip_lstm,c=\"b\",label=\"Train LSTM TacTip\")\n",
    "plt.plot(test_tactip_lstm,\"--\",c=\"b\",label=\"test LSTM TacTip\")\n",
    "\n",
    "plt.xticks([i for i in range(len(train_tactip_cnn))],[i for i in range(2,14)])\n",
    "plt.grid(1)\n",
    "plt.title(\"How dataset size influences classifier performance\")\n",
    "plt.xlabel(\"Number of Classes\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/images/datasetsize.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.load(path+\"saves/test_scores.npy\")\n",
    "train=np.load(path+\"saves/train_scores.npy\")\n",
    "\n",
    "test=np.max(test,axis=2)*100\n",
    "\"\"\"test=(test-np.min(test))/(np.max(test)-np.min(test))\n",
    "test*=255\"\"\"\n",
    "test[test==0]=80\n",
    "plt.imshow(test,cmap=\"plasma\")\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(test.shape[0]):\n",
    "    for j in range(test.shape[1]):\n",
    "        num=int(test[i, j])\n",
    "        if num==80: num=\"na\"\n",
    "        plt.text(j, i, num, ha='center', va='center', color='white')\n",
    "\n",
    "plt.title(\"Averaged accuracy of CNN models on testing data\")\n",
    "plt.xlabel(\"Window end position\")\n",
    "plt.ylabel(\"Window start position\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "train=np.max(train,axis=2)*100\n",
    "train[train==0]=80\n",
    "plt.imshow(train,cmap=\"plasma\")\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(train.shape[0]):\n",
    "    for j in range(train.shape[1]):\n",
    "        num=int(train[i, j])\n",
    "        if num==80: num=\"na\"\n",
    "        plt.text(j, i, num, ha='center', va='center', color='white')\n",
    "plt.title(\"Averaged accuracy of CNN models on training data\")\n",
    "plt.xlabel(\"Window end position\")\n",
    "plt.ylabel(\"Window start position\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(4,filename=\"X_data_newMorph.npz\")\n",
    "data.applySobel()\n",
    "sample_of_data=data.X[0:10]\n",
    "\n",
    "model = SimpleCNN(abs(4)*data.X.shape[2],data.X.shape[3],output=15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodel\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "x_data=data.X[0:4].reshape((len(data.X[0:4]),1,abs(4)*data.X.shape[2],data.X.shape[3]))\n",
    "x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data))\n",
    "train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#forward pass\n",
    "x = model.pool(model.relu(model.conv1(train_images_tensor)))#.cpu().detach().numpy()\n",
    "x = model.pool(model.relu(model.conv2(x))).cpu().detach().numpy()\n",
    "print(x[0][0].shape)\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "axes = axes.flatten()\n",
    "axes[0].imshow(train_images_tensor[0][0].cpu().detach().numpy(),cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Original image\",fontsize=11)\n",
    "for i in range(1,10):\n",
    "    axes[i].imshow(x[2][i],cmap=\"gray\")\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(\"Feature depth \"+str(i+1),fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"images/featuresALL_uberModel.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data=loaded(t=4)\n",
    "data.applySobel()\n",
    "data.augment()\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "pca = PCA(n_components=833)\n",
    "x_data=data.X[0:].reshape((len(data.X[0:]),1,abs(4)*data.X.shape[2],data.X.shape[3]))\n",
    "labels=data.y[0:]\n",
    "print(x_data.reshape(len(x_data), -1).shape)\n",
    "pca_features = pca.fit_transform(x_data.reshape(len(x_data), -1))\n",
    "\n",
    "# Further reduce dimensionality with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(pca_features)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='tab20')\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE Visualization of Optical Tactile Sensor Images')\n",
    "plt.savefig(path+\"/images/clusters.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/TacTip reader/pickle_imputer_sobel.pkl','rb') as file:\n",
    "    reg=pickle.load(file)\n",
    "data=loaded(t=4)\n",
    "data.applySobel()\n",
    "def predict(reg,dat):\n",
    "    dat=dat.flatten()\n",
    "    p=reg.predict([dat])\n",
    "    p=p.reshape((p.shape[0],p.shape[1]//2,2))\n",
    "    return p\n",
    "sample=data.X[0:750]\n",
    "print(sample.shape)\n",
    "X_prime=np.zeros((len(sample),sample.shape[1],260,270),dtype=np.uint8)\n",
    "coords=np.zeros((len(sample),sample.shape[1],133,2))\n",
    "for i in range(len(sample)): #create data aas linear images\n",
    "    for j in range(len(sample[0])):\n",
    "        X_prime[i][j]=cv2.resize(sample[i][j],(270,260),interpolation=cv2.INTER_AREA)+140\n",
    "        coords[i][j]=predict(reg,X_prime[i][j])[0]\n",
    "X_alt=X_prime\n",
    "X_alt[X_alt>255]=255\n",
    "print(sample.shape,X_alt.shape)\n",
    "plt.imshow(X_alt[0][0],cmap=\"gray\")\n",
    "pred=predict(reg,X_alt[0][0])[0]\n",
    "plt.scatter(pred[:,0],pred[:,1])\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/images/transfertogel.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/TacTip reader/pickle_imputer_sobel.pkl','rb') as file:\n",
    "    reg=pickle.load(file)\n",
    "data=loaded(t=5)\n",
    "data.applySobel()\n",
    "def predict(reg,dat):\n",
    "    dat=dat.flatten()\n",
    "    p=reg.predict([dat])\n",
    "    p=p.reshape((p.shape[0],p.shape[1]//2,2))\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=data.X\n",
    "print(sample.shape)\n",
    "X_prime=np.zeros((len(sample),sample.shape[1],260,270),dtype=np.uint8)\n",
    "coords=np.zeros((len(sample),sample.shape[1],133,2))\n",
    "for i in range(len(sample)): #create data aas linear images\n",
    "    for j in range(len(sample[0])):\n",
    "        X_prime[i][j]=cv2.resize(sample[i][j],(270,260),interpolation=cv2.INTER_AREA)#+10\n",
    "        coords[i][j]=predict(reg,X_prime[i][j])[0]\n",
    "X_alt=X_prime\n",
    "X_alt[X_alt>255]=255\n",
    "print(sample.shape,X_alt.shape)\n",
    "plt.imshow(X_alt[0][0],cmap=\"gray\")\n",
    "pred=predict(reg,X_alt[0][0])[0]\n",
    "plt.scatter(pred[:,0],pred[:,1])\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/images/transfertogel.pdf\")\n",
    "plt.show()\n",
    "del sample\n",
    "del X_prime\n",
    "del X_alt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer1_size, hidden_layer2_size, output_size):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define the first fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer1_size)\n",
    "        \n",
    "        # Define the second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_layer1_size, hidden_layer2_size)\n",
    "\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_layer2_size, 50)\n",
    "        \n",
    "        # Define the output fully connected layer\n",
    "        self.fc4 = nn.Linear(50, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the first fully connected layer followed by ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Apply the second fully connected layer followed by ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Apply the output fully connected layer\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.round(coords.reshape((len(coords),-1)))\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
    "un=np.unique(data.y)\n",
    "# Number of classes\n",
    "num_classes = np.max(data.y) + 1\n",
    "# One-hot encoding\n",
    "one_hot_encoded = np.eye(num_classes)[data.y]\n",
    "y=one_hot_encoded\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_size = len(X[0])       # Number of input features\n",
    "hidden_layer1_size = len(X[0]) //2  # Number of neurons in the first hidden layer\n",
    "hidden_layer2_size = 800  # Number of neurons in the second hidden layer\n",
    "output_size = len(un)     # Number of output features\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNeuralNetwork(input_size, hidden_layer1_size, hidden_layer2_size, output_size).to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "print(X.shape,y.shape,X.dtype,y.dtype)\n",
    "# Create a TensorDataset and DataLoader\n",
    "#dataset = TensorDataset(torch.Tensor(train_X).to(device), torch.Tensor(train_y).to(device))\n",
    "#dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "train_X_=torch.Tensor(train_X).to(device)\n",
    "train_y_=torch.Tensor(train_y).to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Assuming a classification problem\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #for inputs, labels in dataloader:\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(train_X_)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, train_y_)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.Tensor(test_X).to(device)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(test_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")\n",
    "inputs=torch.Tensor(train_X).to(device)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(train_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords.reshape((len(coords),-1)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class SimpleLSTMDrop(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
    "        super(SimpleLSTMDrop, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.round(coords.reshape((len(coords),5,-1)))\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
    "un=np.unique(data.y)\n",
    "# Number of classes\n",
    "num_classes = np.max(data.y) + 1\n",
    "# One-hot encoding\n",
    "one_hot_encoded = np.eye(num_classes)[data.y]\n",
    "y=one_hot_encoded\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_size = len(X[0][0])       # Number of input features\n",
    "output_size = len(un)     # Number of output features\n",
    "print(X.shape,y.shape,X.dtype,train_X.shape)\n",
    "\n",
    "train_X_=torch.Tensor(train_X)\n",
    "train_y_=torch.Tensor(train_y)\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = SimpleLSTM(input_size, 100, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(train_X_)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = criterion(outputs, train_y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.Tensor(test_X)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(test_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")\n",
    "inputs=torch.Tensor(train_X)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(train_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotchange(average):\n",
    "    magnitudes=[]\n",
    "    for t in range(len(average)-1):\n",
    "        magnitudes.append(euclidean_distance(average[t],average[t+1]))\n",
    "    ar=np.array(magnitudes).T\n",
    "    return ar#(ar-np.min(ar))/(np.max(ar)-np.min(ar))\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    # Convert points to numpy arrays\n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "    \n",
    "    # Calculate the distance\n",
    "    distance = np.linalg.norm(point1 - point2,axis=1)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "#create dataset\n",
    "d1=coords[:,:-1,:]\n",
    "d2=coords[:,1:,:]\n",
    "distances=np.linalg.norm(d1 - d2,axis=3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = distances\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
    "un=np.unique(data.y)\n",
    "# Number of classes\n",
    "num_classes = np.max(data.y) + 1\n",
    "# One-hot encoding\n",
    "one_hot_encoded = np.eye(num_classes)[data.y]\n",
    "y=one_hot_encoded\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_size = len(X[0][0])       # Number of input features\n",
    "output_size = len(un)     # Number of output features\n",
    "print(X.shape,y.shape,X.dtype,train_X.shape)\n",
    "\n",
    "train_X_=torch.Tensor(train_X)\n",
    "train_y_=torch.Tensor(train_y)\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = SimpleLSTM(input_size, 300, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(train_X_)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = criterion(outputs, train_y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.Tensor(test_X)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(test_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")\n",
    "inputs=torch.Tensor(train_X)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(train_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = distances\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
    "un=np.unique(data.y)\n",
    "# Number of classes\n",
    "num_classes = np.max(data.y) + 1\n",
    "# One-hot encoding\n",
    "one_hot_encoded = np.eye(num_classes)[data.y]\n",
    "y=one_hot_encoded\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_size = len(X[0][0])       # Number of input features\n",
    "output_size = len(un)     # Number of output features\n",
    "print(X.shape,y.shape,X.dtype,train_X.shape)\n",
    "\n",
    "train_X_=torch.Tensor(train_X)\n",
    "train_y_=torch.Tensor(train_y)\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = SimpleLSTMDrop(input_size, 300, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 600\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(train_X_)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = criterion(outputs, train_y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "inputs=torch.Tensor(test_X)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(test_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")\n",
    "inputs=torch.Tensor(train_X)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(train_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = distances.reshape((len(distances),-1))\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
    "un=np.unique(data.y)\n",
    "# Number of classes\n",
    "num_classes = np.max(data.y) + 1\n",
    "# One-hot encoding\n",
    "one_hot_encoded = np.eye(num_classes)[data.y]\n",
    "y=one_hot_encoded\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_size = len(X[0])       # Number of input features\n",
    "output_size = len(un)     # Number of output features\n",
    "print(X.shape,y.shape,X.dtype,train_X.shape)\n",
    "\n",
    "train_X_=torch.Tensor(train_X).to(device)\n",
    "train_y_=torch.Tensor(train_y).to(device)\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNeuralNetwork(input_size, hidden_layer1_size, hidden_layer2_size, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Assuming a classification problem\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #for inputs, labels in dataloader:\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(train_X_)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, train_y_)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.Tensor(test_X).to(device)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(test_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")\n",
    "inputs=torch.Tensor(train_X).to(device)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(train_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.round(distances.reshape((len(distances),1,4,-1)))\n",
    "X=(X-np.min(X))/(np.max(X)-np.min(X))\n",
    "un=np.unique(data.y)\n",
    "# Number of classes\n",
    "num_classes = np.max(data.y) + 1\n",
    "# One-hot encoding\n",
    "one_hot_encoded = np.eye(num_classes)[data.y]\n",
    "y=one_hot_encoded\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "input_size = len(X[0][0])       # Number of input features\n",
    "output_size = len(un)     # Number of output features\n",
    "print(X.shape,y.shape,X.dtype,train_X.shape)\n",
    "\n",
    "train_X_=torch.Tensor(train_X)\n",
    "train_y_=torch.Tensor(train_y)\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = SimpleCNN(len(distances[0]),len(distances[0][0]))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(train_X_)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = criterion(outputs, train_y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.Tensor(test_X)#.to(device)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(test_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")\n",
    "inputs=torch.Tensor(train_X)#.to(device)\n",
    "outputs = model(inputs)\n",
    "outputs=outputs.cpu().detach().numpy()\n",
    "print(test_y.shape,outputs.shape)\n",
    "correct=np.argmax(train_y,axis=1)==np.argmax(outputs,axis=1)\n",
    "print(np.sum(correct)/len(correct) *100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path+\"/data/distances\",distances)\n",
    "np.save(path+\"/data/distancesy\",data.y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferability of normal model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader,unique=genData(0,4)\n",
    "model,history=run(train_loader,0,4)\n",
    "torch.save(model.state_dict(), path+\"/model/mymodelgel\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")\n",
    "data=loaded(4)\n",
    "image=data.X[0][0]\n",
    "modelA = SimpleCNN(4*image.shape[0],image.shape[1]).to(device)\n",
    "modelA.load_state_dict(torch.load(path+\"/model/mymodelgel\"))\n",
    "modelA.eval()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelA = SimpleCNN(4*image.shape[0],image.shape[1]).to(device)\n",
    "modelA.load_state_dict(torch.load(path+\"/model/mymodelgel\"))\n",
    "modelA.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "\n",
    "import pickle\n",
    "with open('/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/TacTip reader/pickle_imputer_sobel.pkl','rb') as file:\n",
    "    reg=pickle.load(file)\n",
    "data=loaded(t=4)\n",
    "data.applySobel()\n",
    "def predict(reg,dat):\n",
    "    dat=dat.flatten()\n",
    "    p=reg.predict([dat])\n",
    "    p=p.reshape((p.shape[0],p.shape[1]//2,2))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=data.X[0:300]\n",
    "X_prime=np.zeros((len(sample),sample.shape[1],*data.X[0][0].shape),dtype=np.uint8)\n",
    "coords=np.zeros((len(sample),sample.shape[1],133,2)).astype(np.uint16)\n",
    "divh=X_prime[0][0].shape[0]/270\n",
    "divw=X_prime[0][0].shape[1]/260\n",
    "print(divh,divw,X_prime[0][0].shape)\n",
    "for i in range(len(sample)): #create data aas linear images\n",
    "    for j in range(len(sample[0])):\n",
    "        temp=cv2.resize(sample[i][j],(270,260),interpolation=cv2.INTER_AREA)#+10\n",
    "        coords[i][j]=np.round(predict(reg,temp)[0])\n",
    "        #X_prime[i][j]*=0 \n",
    "        for point in zip(coords[i][j][:,0],coords[i][j][:,1]):\n",
    "            p1=int(point[0]*divh)\n",
    "            p2=int(point[1]*divw)\n",
    "            cv2.circle(X_prime[i][j],(p1,p2),1,(255),2)\n",
    "del sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_prime[0][0],cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(data.X[0][0],cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=(X_prime-np.mean(X_prime))/(np.max(X_prime)-np.min(X_prime))\n",
    "x=x.reshape((len(x),1,x.shape[1]*x.shape[2],x.shape[3]))\n",
    "print(x.shape)\n",
    "test=torch.tensor(x, dtype=torch.float32)\n",
    "print(test.dtype)\n",
    "label=data.y[0:300]\n",
    "preds=model(test)\n",
    "preds=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "a=np.abs(label-preds)\n",
    "print(len(a[a==0])/len(a) *100,\"%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(4,filename=\"X_data_nomarker.npz\")\n",
    "data.applySobel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=data.X[0][0]\n",
    "model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodelnomarker\"))\n",
    "model.eval()\n",
    "n=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at frquency \n",
    "def generate_saliency_maps(model, inputs, target_class):\n",
    "    model.eval()\n",
    "    inputs.requires_grad = True  # Ensure gradients can be computed for input\n",
    "\n",
    "    # Forward pass to get the predictions\n",
    "    outputs = model(inputs)  # outputs should be of shape (N, num_classes) if model outputs logits\n",
    "    score = outputs[:, target_class].sum()  # Sum over the batch for a single target class\n",
    "    \n",
    "    # Backward pass to calculate gradients\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "    \n",
    "    # Get the absolute value of the gradients as saliency map\n",
    "    saliency = inputs.grad.abs()  # Shape will be (N, t, w, h)\n",
    "\n",
    "    return saliency\n",
    "\n",
    "# Plotting Function\n",
    "def plot_saliency_map(saliency, original_frames, time_index):\n",
    "    \"\"\"Plot saliency map for a specific time index in the sequence\"\"\"\n",
    "    # Average over the batch if needed, to show a single example\n",
    "    saliency_frame = saliency[0, time_index].detach().cpu().numpy()  # Shape (w, h)\n",
    "    original_frame = original_frames[0, time_index].detach().cpu().numpy()\n",
    "    \n",
    "    # Normalize saliency map for visualization\n",
    "    saliency_frame = (saliency_frame - saliency_frame.min()) / (saliency_frame.max() - saliency_frame.min())\n",
    "    \n",
    "    # Plot the original frame with saliency map overlay\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n",
    "    ax[0].imshow(original_frame[0:110,:], cmap='gray')\n",
    "    ax[0].set_title(\"Original Frame\",fontsize=15,**csfont)\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(original_frame[0:110,:], cmap='gray')\n",
    "    ax[1].imshow(saliency_frame[0:110,:], cmap='hot', alpha=0.5)  # Overlay saliency map\n",
    "    ax[1].set_title(f\"Saliency Map - Frame {time_index}\",fontsize=15,**csfont)\n",
    "    ax[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path+\"/images/saliency_map.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Load a sample from your dataset\n",
    "shape=data.X[0].shape\n",
    "n=10\n",
    "sample_input = torch.tensor(data.X[0:n],dtype=torch.float32).reshape((len(data.X[0:n]),1,4*data.X.shape[2],data.X.shape[3]))  # Shape (1, t, w, h) - single sample for visualization\n",
    "\n",
    "print(sample_input.shape)\n",
    "# Generate saliency maps for the sample\n",
    "target_class = 0  # Replace with the class index of interest\n",
    "saliency_maps = generate_saliency_maps(model, sample_input, target_class)\n",
    "\n",
    "# Plot saliency for each frame in the temporal sequence\n",
    "for time_index in range(sample_input.shape[1]):  # Iterate over t frames\n",
    "    plot_saliency_map(saliency_maps, sample_input, time_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking out segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(4,filename=\"X_data_15.npz\")\n",
    "data.applySobel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample=data.X[0:300].reshape((300,1,4*len(data.X[0][0]),len(data.X[0][0][0]))).copy()\n",
    "images=[sample[0][0].copy(),sample[0][0].copy(),sample[0][0].copy(),sample[0][0].copy(),sample[0][0].copy(),sample[0][0].copy(),sample[0][0].copy(),sample[0][0].copy()]\n",
    "images[0][0:-1,50:75]=0\n",
    "images[1][0:-1,20:45]=0\n",
    "images[2][0:-1,70:95]=0\n",
    "images[3][0:-1,60:70]=0\n",
    "images[4][0:110,0:120]=0\n",
    "images[5][0:220,0:120]=0\n",
    "images[6][0:330,0:120]=0\n",
    "images[7][110:330,0:120]=0\n",
    "label=[\"50:75\",\"20:45\",\"70:95\",\"60:70\",\"Segment removed\",\"Two removed\",\"Three removed\",\"Only first and last\"]\n",
    "label=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "sample[:,0:330,0:120]=0\n",
    "#sample=sample.reshape((300,4,len(data.X[0][0])*len(data.X[0][0][0]))).copy()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(15,8))\n",
    "axes = axes.flatten()\n",
    "for i in range(len(images)):\n",
    "    axes[i].set_title(label[i],fontsize=30,**csfont)\n",
    "    axes[i].imshow(images[i])\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/images/samplesCrops_.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=data.X[0:300].reshape((300,4*len(data.X[0][0]),len(data.X[0][0][0]))).copy()\n",
    "\"\"\"\n",
    "images[0][0:-1,50:75]=0\n",
    "images[1][0:-1,20:45]=0\n",
    "images[2][0:-1,70:95]=0\n",
    "images[3][0:-1,60:70]=0\n",
    "images[4][0:110,0:120]=0\n",
    "images[5][0:220,0:120]=0\n",
    "images[6][0:330,0:120]=0\n",
    "images[7][110:330,0:120]=0\n",
    "\"\"\"\n",
    "sample[:,110:330,0:120]=0\n",
    "#sample=sample.reshape((300,4,len(data.X[0][0])*len(data.X[0][0][0]))).copy() # lstm\n",
    "sample=sample.reshape((300,1,4*len(data.X[0][0]),len(data.X[0][0][0]))).copy()\n",
    "\n",
    "image=sample[0][0]\n",
    "print(image.shape)\n",
    "#model=SimpleCNN(data.X.shape[2]*4,data.X.shape[3],15).to(device)#.half()\n",
    "\"\"\"model=SimpleLSTM(image.shape[1],1000,15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodel_lstm\"))\n",
    "model.eval()\"\"\"\n",
    "model = SimpleCNN(image.shape[0],image.shape[1],output=15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodel_newMorph\"))\n",
    "model.eval()\n",
    "x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "#x=x.reshape((len(x),4,x.shape[2]//4*x.shape[3]))\n",
    "\n",
    "print(x.shape)\n",
    "test=torch.tensor(x, dtype=torch.float32).to(device)\n",
    "print(test.dtype)\n",
    "label=data.y[0:300]\n",
    "preds=model(test)\n",
    "preds=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "a=np.abs(label-preds)\n",
    "print(len(a[a==0])/len(a) *100,\"%\")\n",
    "plt.imshow(image.reshape((4*len(data.X[0][0]),len(data.X[0][0][0]))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixA=np.array([[56.3,98,99.3],[52,100,53.3],[72.3,100,70],[89.33,100,100],[98.6,100,95.6],[91.3,100,64.6],[83.3,97,47.3],[97,100,79.6]]) #cnn\n",
    "matrixB=np.array([[43.4,63,100],[19.3,92.6,12.3],[23.3,86,83.6],[88.6,90.3,100],[86,68.3,86.3],[72.6,45,56.9],[49,34,32.6],[68.6,61.6,60]])\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot matrixA in the top half with 'YlGn' colormap\n",
    "ax.imshow(matrixA.T, cmap='YlGn', aspect='auto', extent=[0, 8, 6, 3])\n",
    "\n",
    "# Plot matrixB in the bottom half with 'gray' colormap\n",
    "ax.imshow(matrixB.T, cmap='YlOrRd', aspect='auto', extent=[0, 8, 3, 0])\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(8) + 0.5)\n",
    "ax.set_xticklabels([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"],fontsize=20,**csfont)\n",
    "ax.set_yticks(np.arange(6)+1)\n",
    "ax.set_yticklabels([\"  NM\", \"  S\", \"  0M\", \"  NM\", \"  S\", \"  0M\"], rotation=90, ha=\"left\", rotation_mode=\"anchor\",fontsize=20,**csfont)\n",
    "\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(matrixA.shape[0]):\n",
    "    for j in range(matrixA.shape[1]):\n",
    "        ax.text(i+0.5, j+0.5+3, f\"{matrixA[i, j]:.1f}%\", ha='center', va='center', color='white' if matrixA[i, j] > 70 else 'black',fontsize=18,**csfont)\n",
    "\n",
    "for i in range(matrixB.shape[0]):\n",
    "    for j in range(matrixB.shape[1]):\n",
    "        ax.text(i+0.5, j+0.5, f\"{matrixB[i, j]:.1f}%\", ha='center', va='center', color='white' if matrixB[i, j] > 90 else 'black',fontsize=18,**csfont)\n",
    "\n",
    "\n",
    "plt.title(\"Model results\",fontsize=20,**csfont)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/images/LSTM_different_markers.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different pressures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader,unique=genData(0,4)\n",
    "model,history=run(train_loader,0,4)\n",
    "torch.save(model.state_dict(), path+\"/model/mymodel_withLowPressure\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")\n",
    "data=loaded(4)\n",
    "image=data.X[0][0]\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(4,filename=\"X_data_15.npz\")\n",
    "image=data.X[0][0]\n",
    "#model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "model=SimpleLSTM(image.shape[0]*image.shape[1],1000,15).to(device)#.half()\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodel_lstm\"))\n",
    "model.eval()\n",
    "#print(calc(model,test_loader))\n",
    "#print(calc(model,train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "d=loaded(t=4,filename=\"X_flat_unseen_pressures.npz\")\n",
    "d.applySobel()\n",
    "sample=d.X\n",
    "label=d.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=d.X\n",
    "x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "x=x.reshape((len(x),x.shape[1],x.shape[2]*x.shape[3]))\n",
    "test=torch.tensor(x, dtype=torch.float32).to(device)\n",
    "preds=model(test)\n",
    "preds=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "a=preds[preds==11]\n",
    "print(len(a)/len(preds) *100,\"%\")\n",
    "del test\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p20=d.X[np.where(d.y==3)]\n",
    "p30=d.X[np.where(d.y==0)]\n",
    "p40=d.X[np.where(d.y==2)]\n",
    "p50=d.X[np.where(d.y==1)]\n",
    "data=[p20,p30,p40,p50]\n",
    "accs=[]\n",
    "for i in range(4):\n",
    "    sample=data[i]\n",
    "    label=11\n",
    "    x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "    x=x.reshape((len(x),1,x.shape[1]*x.shape[2],x.shape[3]))  #n,1,t*h,w for cnn, n,t,h*w for lstm, n,1,t,h,w for cnn-lstm\n",
    "    test=torch.tensor(x, dtype=torch.float32).to(device)\n",
    "    preds=model(test)\n",
    "    preds=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "    a=len(preds[preds==label])\n",
    "    print(a/len(preds) *100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models on other data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(4,filename=\"X_data_nomarker.npz\")\n",
    "image=data.X[0][0].copy()\n",
    "del data\n",
    "#model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "#print(calc(model,test_loader))\n",
    "#print(calc(model,train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "#model=SimpleLSTM(image.shape[0]*image.shape[1],1000,15).to(device)\n",
    "model=CNN_LSTM(image.shape[1],image.shape[0],1000,1,15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/uber_lstmCNN\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "d=loaded(t=4,filename=\"X_data_nomarker.npz\") #X_data_gel_15.npz X_data_newMorph.npz X_data_15.npz\n",
    "d.applySobel()\n",
    "sample=d.X[0:800]\n",
    "label=d.y[0:800]\n",
    "del d\n",
    "x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "x=x.reshape((len(x),1,x.shape[1],x.shape[2],x.shape[3]))\n",
    "test=torch.tensor(x, dtype=torch.float32).to(device)\n",
    "preds=model(test)\n",
    "preds=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "a=preds-label\n",
    "print(len(a[a==0])/len(preds) *100,\"%\")\n",
    "del test\n",
    "del x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring large models to small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(4,filename=\"X_data_15.npz\")\n",
    "image=data.X[0][0]\n",
    "data\n",
    "model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "model.load_state_dict(torch.load(path+\"/model/uber_model\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=loaded(t=4,filename=\"X_data_15.npz\") #X_data_gel_15.npz X_data_newMorph.npz X_data_15.npz\n",
    "d.applySobel()\n",
    "d.resize(0.1)\n",
    "\n",
    "sample=d.X\n",
    "sampley=d.y\n",
    "image=sample[0][0]\n",
    "print(image.shape)\n",
    "#copy over correct convolutions\n",
    "modelB=SimpleCNN(image.shape[0]*d.X.shape[1],image.shape[1],15).to(device)\n",
    "modelB.conv1=model.conv1\n",
    "modelB.conv2=model.conv2\n",
    "#freeze conv layers\n",
    "modelB.conv1.requires_grad=False\n",
    "modelB.conv2.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(sampley)\n",
    "one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "x=x.reshape((len(x),1,x.shape[1]*x.shape[2],x.shape[3]))\n",
    "\n",
    "train_images_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "# Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_history=[]\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(modelB.parameters(), lr=0.005)\n",
    "\n",
    "# Train the Model\n",
    "num_epochs=50\n",
    "clip_value = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = modelB(inputs)\n",
    "        #print(inputs.shape,outputs.shape,labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelB.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    train_history.append(loss.cpu().detach().numpy())\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss :.4f}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc(modelB,test_loader))\n",
    "print(calc(modelB,train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEL\n",
    "# 5%\n",
    "# train % 58.95 test 57.99 % \n",
    "\n",
    "#10%\n",
    "# train % 91.70 test 92.81 % \n",
    "\n",
    "#15%\n",
    "# train % 93.37 test 94.81\n",
    "\n",
    "#20%\n",
    "# train % 96.99 test 98.15\n",
    "\n",
    "#25%\n",
    "# train % 99.9 test 99.98 % \n",
    "\n",
    "#NEWMORPH\n",
    "# 5%\n",
    "# train % 74.32 test 75.37 % \n",
    "\n",
    "#10%\n",
    "# train % 89.17 test 89.02 % \n",
    "\n",
    "#15%\n",
    "# train % 82.92 test 84.98 %\n",
    "\n",
    "#20%\n",
    "# train % 96.87 test 95.75 %\n",
    "\n",
    "#25%\n",
    "# train % 90.1 test 88.25 % \n",
    "\n",
    "#SILICONE\n",
    "# 5%\n",
    "# train % - test - % \n",
    "\n",
    "#10%\n",
    "# train % - test - % \n",
    "\n",
    "#15%\n",
    "# train % 79 test 74 %\n",
    "\n",
    "#20%\n",
    "# train % 88 test 90 %\n",
    "\n",
    "#25%\n",
    "# train % - test - % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automate the shit out of it\n",
    "def long(file,type1,num):\n",
    "    resolution=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.7,0.8]\n",
    "    acc_test=np.zeros((len(resolution),5))\n",
    "    acc_train=np.zeros((len(resolution),5))\n",
    "    num_epochs=80\n",
    "    history=np.zeros((len(resolution),5,num_epochs))\n",
    "\n",
    "    for i in range(len(resolution)):\n",
    "        d=loaded(t=4,filename=file) #X_data_gel_15.npz X_data_newMorph.npz X_data_15.npz\n",
    "        d.applySobel()\n",
    "        image=d.X[0][0]\n",
    "        model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "        model.load_state_dict(torch.load(path+\"/model/mymodelgel\"))\n",
    "        model.eval()\n",
    "        d.resize(resolution[i])\n",
    "        \n",
    "        sample=d.X\n",
    "        sampley=d.y\n",
    "        image=sample[0][0]\n",
    "        print(image.shape)\n",
    "        \n",
    "        #copy over correct convolutions\n",
    "        modelB=SimpleCNN(image.shape[0]*d.X.shape[1],image.shape[1],num).to(device)\n",
    "        modelB.conv1=model.conv1\n",
    "        modelB.conv2=model.conv2\n",
    "        #freeze conv layers\n",
    "        modelB.conv1.requires_grad=False\n",
    "        modelB.conv2.requires_grad=False\n",
    "\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        train_labels_encoded = label_encoder.fit_transform(sampley)\n",
    "        one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "        x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "        x=x.reshape((len(x),1,x.shape[1]*x.shape[2],x.shape[3]))\n",
    "\n",
    "        train_images_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Create a TensorDataset\n",
    "        dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "        # Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        # Create DataLoader for training and testing sets\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "        max_=0\n",
    "        for j in range(5):\n",
    "            clear_output(wait=True)\n",
    "            print(\"Resolution\",resolution[i],\"Trial\",j)\n",
    "            criterion = nn.CrossEntropyLoss().to(device)\n",
    "            optimizer = optim.SGD(modelB.parameters(), lr=0.005)\n",
    "\n",
    "            # Train the Model\n",
    "            \n",
    "            clip_value = 5\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss = 0.0\n",
    "                for k, (inputs, labels) in enumerate(train_loader):\n",
    "                    # Zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = modelB(inputs)\n",
    "                    #print(inputs.shape,outputs.shape,labels.shape)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(modelB.parameters(), clip_value)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Print statistics\n",
    "                    running_loss += loss.item()\n",
    "                history[i][j][epoch]=running_loss/(k+1) #save average loss per epoch\n",
    "            test=calc(modelB,test_loader)\n",
    "            train=calc(modelB,train_loader)\n",
    "            acc_train[i][j]=train\n",
    "            acc_test[i][j]=test\n",
    "            if test>max_:\n",
    "                max_=test\n",
    "                torch.save(modelB.state_dict(), path+\"/model/standard_c_gel/cnn_res\"+str(resolution[i])+\"_\"+str(type1))\n",
    "        del train_loader\n",
    "        del test_loader\n",
    "    \n",
    "    np.save(path+\"/data/train_uber_\"+type1+\"_long_standard_c\",acc_train)\n",
    "    np.save(path+\"/data/test_uber_\"+type1+\"_long_standard_c\",acc_test)\n",
    "    np.save(path+\"/data/histroy_\"+type1+\"_long_standard_c\",history)\n",
    "\n",
    "#X_data_gel_15.npz X_data_newMorph.npz X_data_15.npz\n",
    "long(\"X_data_gel_15.npz\",\"gel\",15)\n",
    "#long(\"X_data_15.npz\",\"sil\",15)\n",
    "#long(\"X_data_newMorph.npz\",\"nm\",13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_train=np.load(path+\"/data/train_uber_sil_long_standard_c.npy\")\n",
    "sil_test=np.load(path+\"/data/test_uber_sil_long_standard_c.npy\")\n",
    "gel_train=np.load(path+\"/data/train_uber_gel_long_standard_c.npy\")\n",
    "gel_test=np.load(path+\"/data/test_uber_gel_long_standard_c.npy\")\n",
    "#nm_train=np.load(path+\"/data/train_uber_nm_long.npy\")\n",
    "#nm_test=np.load(path+\"/data/test_uber_nm_long.npy\")\n",
    "\n",
    "average_sil_test=np.average(sil_test,axis=1)*100\n",
    "max_sil_test=np.max(sil_test,axis=1)*100\n",
    "average_sil_train=np.average(sil_train,axis=1)*100\n",
    "average_gel_test=np.average(gel_test,axis=1)*100\n",
    "max_gel_test=np.max(gel_test,axis=1)*100\n",
    "average_gel_train=np.average(gel_train,axis=1)*100\n",
    "#average_nm_test=np.average(nm_test,axis=1)*100\n",
    "#max_nm_test=np.max(nm_test,axis=1)*100\n",
    "#average_nm_train=np.average(nm_train,axis=1)*100\n",
    "\n",
    "plt.plot(average_sil_test,c=\"b\",label=\"Silicone test\")\n",
    "plt.plot(max_sil_test,\"--\",c=\"b\",label=\"Silicone max\")\n",
    "plt.plot(average_gel_test,c=\"g\",label=\"Gel test\")\n",
    "plt.plot(max_gel_test,\"--\",c=\"g\",label=\"Gel max\")\n",
    "#plt.plot(average_nm_test,c=\"r\",label=\"NM test\")\n",
    "#plt.plot(max_nm_test,\"--\",c=\"r\",label=\"NM max\")\n",
    "\n",
    "#plt.plot(average_sil_train,c=\"b\",label=\"Silicone train\")\n",
    "resolution=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.7,0.8]\n",
    "plt.xticks([i for i in range(len(resolution))],resolution)\n",
    "plt.ylabel(\"Aacuracy %\")\n",
    "plt.xlabel(\"Resolution multiplier\")\n",
    "plt.title(\"Resolution vs accuracy on pretrained conv layer\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path+\"/images/resolution_just_own_transfer.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "sil_history=np.load(path+\"/data/histroy_sil_long_untrained.npy\")\n",
    "gel_history=np.load(path+\"/data/histroy_gel_long_untrained.npy\")\n",
    "nm_history=np.load(path+\"/data/histroy_nm_long_untrained.npy\")\n",
    "print(sil_history.shape)\n",
    "\n",
    "sil_history_av=np.average(sil_history,axis=1)\n",
    "#gel_history_av=np.average(gel_history,axis=1)\n",
    "#nm_history_av=np.average(nm_history,axis=1)\n",
    "print(sil_history_av.shape)\n",
    "# Create a colormap that gets darker\n",
    "colors = cm.viridis(np.linspace(0, 1, len(resolution)))\n",
    "\n",
    "for i in range(len(sil_history_av)):\n",
    "    plt.plot(sil_history_av[i],color=colors[i],label=\"Resolution \"+str(resolution[i]))\n",
    "#plt.plot(gel_history_av,label=\"Average loss gel\")\n",
    "#plt.plot(nm_history_av,label=\"Average loss new morphology\")\n",
    "plt.title(\"Loss over training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(path+\"/images/resolution_loss_untrained.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_X=[]\n",
    "images_f=[]\n",
    "images_o=[]\n",
    "for i in range(len(resolution)-1):\n",
    "    filename=\"cnn_res\"+str(resolution[i])+\"_gel\"\n",
    "\n",
    "    data=loaded(4,filename=\"X_data_15.npz\")\n",
    "    data.applySobel()\n",
    "    data.resize(resolution[i])\n",
    "    image=data.X[0][0]\n",
    "\n",
    "    model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "    model.load_state_dict(torch.load(path+\"/model/res/\"+filename))\n",
    "    model.eval()\n",
    "\n",
    "    sample=data.X[0:13]\n",
    "    sampley=data.y[0:13]\n",
    "\n",
    "    x=(sample-np.mean(sample))/(np.max(sample)-np.min(sample))\n",
    "    x=x.reshape((len(x),1,x.shape[1]*x.shape[2],x.shape[3]))\n",
    "\n",
    "    train_images_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "\n",
    "    #preds=torch.argmax(model(train_images_tensor)).cpu().detach().numpy()\n",
    "    x = model.pool(model.relu(model.conv1(train_images_tensor)))#.cpu().detach().numpy()\n",
    "    x = model.pool(model.relu(model.conv2(x))).cpu().detach().numpy()\n",
    "    images_X.append(train_images_tensor[0][0].cpu().detach().numpy())\n",
    "    images_f.append(x.copy())\n",
    "\n",
    "    model = SimpleCNN(4*image.shape[0],image.shape[1],output=15).to(device)\n",
    "    model.load_state_dict(torch.load(path+\"/model/learnedres/\"+filename))\n",
    "    model.eval()\n",
    "\n",
    "    #preds=torch.argmax(model(train_images_tensor)).cpu().detach().numpy()\n",
    "    x = model.pool(model.relu(model.conv1(train_images_tensor)))#.cpu().detach().numpy()\n",
    "    x = model.pool(model.relu(model.conv2(x))).cpu().detach().numpy()\n",
    "    images_o.append(x.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(3, len(resolution)-1, figsize=(10, 8))\n",
    "#axes = axes.flatten()\n",
    "\n",
    "for i in range(len(images_X)):\n",
    "    axes[0][i].imshow(images_X[i],cmap=\"gray\")\n",
    "    axes[0][i].axis(\"off\")\n",
    "    axes[0][i].set_ylabel(\"Original res \"+str(resolution[i]),fontsize=11)\n",
    "    #print(images_f[i][0].shape)\n",
    "    axes[1][i].imshow(images_f[i][0][0],cmap=\"gray\")\n",
    "    axes[1][i].axis(\"off\")\n",
    "    #axes[1][i].set_title(\"Feature res\"+str(resolution[i]),fontsize=11)\n",
    "    axes[2][i].imshow(images_o[i][1][0],cmap=\"gray\")\n",
    "    axes[2][i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"images/features_norm_transfer.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playing with points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(t=10)\n",
    "data.applySobel()\n",
    "\n",
    "data2=loaded(filename=\"X_data_newMorph.npz\",t=10)\n",
    "data2.applySobel()\n",
    "\n",
    "data3=loaded(filename=\"X_data_nomarker.npz\",t=10)\n",
    "data3.applySobel()\n",
    "\n",
    "#data.augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dense_optical_flows(dataset):\n",
    "    n, t, h, w = dataset.shape\n",
    "    optical_flows = np.zeros((n, t-1, h, w), dtype=np.uint8)  # Change to uint8 for 0-255 range\n",
    "    \n",
    "    # Optical flow parameters for higher sensitivity\n",
    "    pyr_scale = 0.1     # Lowered for more sensitivity to small movements\n",
    "    levels = 20         # Increased number of pyramid levels\n",
    "    winsize = 5         # Reduced window size for finer detail\n",
    "    iterations = 6      # More iterations per level\n",
    "    poly_n = 5          # Polynomial window size (typically fixed)\n",
    "    poly_sigma = 1.0    # Lowered for more detailed flow calculations\n",
    "    \n",
    "    for i in range(n):  # Loop over each sample\n",
    "        for j in range(t - 1):  # Loop over each pair of frames\n",
    "            # Convert frames to grayscale if needed\n",
    "            frame1 = dataset[i, j].astype(np.uint8)\n",
    "            frame2 = dataset[i, j + 1].astype(np.uint8)\n",
    "            \n",
    "            # Calculate dense optical flow with more sensitive parameters\n",
    "            flow = cv2.calcOpticalFlowFarneback(frame1, frame2, None, \n",
    "                                                pyr_scale, levels, winsize, \n",
    "                                                iterations, poly_n, poly_sigma, 0)\n",
    "            \n",
    "            # Calculate the magnitude and angle of the flow\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            \n",
    "            # Normalize the magnitude to the range 0-255 and convert to uint8\n",
    "            mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            optical_flows[i, j] = mag  # Store the normalized magnitude\n",
    "\n",
    "    return optical_flows\n",
    "\n",
    "\n",
    "def compute_dense_optical_flows(dataset):\n",
    "    n, t, h, w = dataset.shape\n",
    "    optical_flows = np.zeros((n, t-1, h, w), dtype=np.uint8)  # Change to uint8 for 0-255 range\n",
    "    \n",
    "    # Parameters for Lucas-Kanade optical flow\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    \n",
    "    for i in range(n):  # Loop over each sample\n",
    "        for j in range(t - 1):  # Loop over each pair of frames\n",
    "            # Convert frames to grayscale if needed\n",
    "            frame1 = dataset[i, j].astype(np.uint8)\n",
    "            frame2 = dataset[i, j + 1].astype(np.uint8)\n",
    "            \n",
    "            # Detect good features to track in the first frame\n",
    "            p0 = cv2.goodFeaturesToTrack(frame1, mask=None, maxCorners=500, qualityLevel=0.01, minDistance=5)\n",
    "            \n",
    "            # Calculate optical flow using Lucas-Kanade\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(frame1, frame2, p0, None, **lk_params)\n",
    "            \n",
    "            # Filter only good points where flow is successfully calculated\n",
    "            if p1 is not None:\n",
    "                good_new = p1[st == 1]\n",
    "                good_old = p0[st == 1]\n",
    "                \n",
    "                # Create a mask to draw the optical flow vectors\n",
    "                mask = np.zeros_like(frame1, dtype=np.uint8)\n",
    "                \n",
    "                # Draw the optical flow vectors\n",
    "                for (new, old) in zip(good_new, good_old):\n",
    "                    a, b = new.ravel()\n",
    "                    c, d = old.ravel()\n",
    "                    cv2.line(mask, (int(c), int(d)), (int(a), int(b)), 255, 1)\n",
    "                    \n",
    "                # Store the mask as the flow field representation for this pair of frames\n",
    "                optical_flows[i, j] = mask\n",
    "\n",
    "    return optical_flows\n",
    "    \n",
    "flow_data=compute_dense_optical_flows(data.X)\n",
    "flow_data2=compute_dense_optical_flows(data2.X)\n",
    "flow_data3=compute_dense_optical_flows(data3.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(dataset, y, segment_size=0.2):\n",
    "    n, t, h, w = dataset.shape\n",
    "    \n",
    "    # Create an empty array for the augmented data\n",
    "    augmented_data = dataset.copy()\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(t):\n",
    "            # Randomly determine the segment's position and size\n",
    "            seg_h = int(h * segment_size)\n",
    "            seg_w = int(w * segment_size)\n",
    "            start_h = np.random.randint(0, h - seg_h)\n",
    "            start_w = np.random.randint(0, w - seg_w)\n",
    "            \n",
    "            # Remove (mask) the segment in the image\n",
    "            augmented_data[i, j, start_h:start_h + seg_h, start_w:start_w + seg_w] = 0\n",
    "    \n",
    "    # Concatenate the original and augmented data\n",
    "    combined_data = np.concatenate((dataset, augmented_data), axis=0)\n",
    "    \n",
    "    # Duplicate the labels for the augmented dataset\n",
    "    combined_y = np.concatenate((y, y), axis=0)\n",
    "    \n",
    "    # Shuffle the combined dataset and labels together\n",
    "    indices = np.arange(2 * n)\n",
    "    np.random.shuffle(indices)\n",
    "    combined_data = combined_data[indices]\n",
    "    combined_y = combined_y[indices]\n",
    "    \n",
    "    return combined_data, combined_y\n",
    "\n",
    "flow_data_a,flow_y=augment(flow_data,data.y)\n",
    "#flow_data_a,flow_y=augment(np.concatenate([flow_data,flow_data2,flow_data3]),np.concatenate([data.y,data2.y,data3.y]))\n",
    "flow_data_a,flow_y=augment(flow_data_a,flow_y)\n",
    "flow_data_a,flow_y=augment(flow_data_a,flow_y)\n",
    "#flow_data_a,flow_y=augment(flow_data2,data2.y)\n",
    "#flow_data_a=flow_data\n",
    "#flow_y=data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_grid(images_grid,data):\n",
    "    \"\"\"\n",
    "    Plots a grid of images, where each sublist in images_grid represents a row of images.\n",
    "    \n",
    "    Parameters:\n",
    "    images_grid (list of lists): List of lists where each sublist contains images for a row.\n",
    "    \"\"\"\n",
    "    num_rows = len(images_grid)\n",
    "    num_cols = max(len(row) for row in images_grid)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "    \n",
    "    # Flatten axes if there's only one row or column for consistency\n",
    "    if num_rows == 1:\n",
    "        axes = [axes]\n",
    "    if num_cols == 1:\n",
    "        axes = [[ax] for ax in axes]\n",
    "    c=0\n",
    "    # Plot each image in the correct position\n",
    "    for row_idx, row_images in enumerate(images_grid):\n",
    "        for col_idx, img in enumerate(row_images):\n",
    "            ax = axes[row_idx][col_idx]\n",
    "            ax.set_title(data.keys[data.y[c]])\n",
    "            ax.imshow(img, cmap='gist_heat' if img.ndim == 2 else None)\n",
    "            ax.axis('off')  # Hide the axes for a cleaner look\n",
    "        c+=1\n",
    "    # Hide any unused subplots if the rows have different lengths\n",
    "    for row_idx in range(num_rows):\n",
    "        for col_idx in range(len(images_grid[row_idx]), num_cols):\n",
    "            axes[row_idx][col_idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_images_grid(flow_data_a[0:5],data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train network on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(flow_y[0:n])\n",
    "one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "x_data=flow_data_a[0:n].reshape((len(flow_data_a[0:n]),1,abs(flow_data_a.shape[1])*flow_data_a.shape[2],flow_data_a.shape[3]))\n",
    "x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "del x_data\n",
    "train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "# Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "model,history=run(train_loader,0,4,num_epochs = 100)\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "torch.save(model.state_dict(), path+\"/model/mymodelcnn_optic_flow\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(flow_y[0:n])\n",
    "one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "x_data=flow_data_a[0:n].reshape((len(flow_data_a[0:n]),abs(flow_data_a.shape[1]),flow_data_a.shape[2]*flow_data_a.shape[3]))\n",
    "#x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "x/=255\n",
    "train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "del x_data\n",
    "train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "# Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)\n",
    "model,history=runLSTM(train_loader,0,10,num_epochs = 200)\n",
    "print(calc(model,test_loader))\n",
    "print(calc(model,train_loader))\n",
    "torch.save(model.state_dict(), path+\"/model/mymodellstm_optic_flow\") #\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/NewRigExperiments/model/mymodelgel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1\n",
    "x_data=flow_data[0:n].reshape((len(flow_data[0:n]),abs(flow_data.shape[1]),flow_data.shape[2]*flow_data.shape[3]))\n",
    "image=x_data[0][0]\n",
    "model = SimpleLSTM(image.shape[0],350,15).to(device)#.half()\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodellstm_optic_flow\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1\n",
    "x_data=flow_data[0:n].reshape((len(flow_data[0:n]),1,abs(flow_data.shape[1])*flow_data.shape[2],flow_data.shape[3]))\n",
    "image=x_data[0][0]\n",
    "model = SimpleCNN(image.shape[0],image.shape[1],15).to(device)#.half()\n",
    "model.load_state_dict(torch.load(path+\"/model/mymodelcnn_optic_flow\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=200\n",
    "torch.cuda.empty_cache()\n",
    "x=torch.tensor(flow_data2.reshape((flow_data2.shape[0],flow_data2.shape[1],flow_data2.shape[2]*flow_data2.shape[3])), dtype=torch.float32).to(device) #lstm\n",
    "#x=torch.tensor(flow_data2.reshape((flow_data2.shape[0],1,flow_data2.shape[1]*flow_data2.shape[2],flow_data2.shape[3])), dtype=torch.float32).to(device) #cnn\n",
    "x/=255\n",
    "#x=(x-torch.mean(x))/(torch.max(x)-torch.min(x)) #preprocessing\n",
    "preds=model(x[0:sample])\n",
    "vals=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "error=np.abs(vals-data2.y[0:sample])\n",
    "\n",
    "print(\"Accuracy \",len(np.where(error==0)[0]),\"/400\",\":\",len(np.where(error==0)[0])/400 *100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "sample=200\n",
    "x=torch.tensor(flow_data3.reshape((flow_data3.shape[0],flow_data3.shape[1],flow_data3.shape[2]*flow_data3.shape[3])), dtype=torch.float32).to(device) #lstm\n",
    "#x=torch.tensor(flow_data3.reshape((flow_data3.shape[0],1,flow_data3.shape[1]*flow_data3.shape[2],flow_data3.shape[3])), dtype=torch.float32).to(device) #cnn\n",
    "x/=255\n",
    "#x=(x-torch.mean(x))/(torch.max(x)-torch.min(x)) #preprocessing\n",
    "preds=model(x[0:sample])\n",
    "vals=torch.argmax(preds,axis=1).cpu().detach().numpy()\n",
    "error=np.abs(vals-data3.y[0:sample])\n",
    "\n",
    "print(\"Accuracy \",len(np.where(error==0)[0]),\"/400\",\":\",len(np.where(error==0)[0])/400 *100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer to convert inputs to model dimension\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer to map to number of classes\n",
    "        self.fc_out = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer encoder expects input as (sequence_length, batch_size, model_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Get the mean of the output sequence (global average pooling)\n",
    "        x = x.mean(dim=0)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.fc_out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_Tmodel(model, train_loader, test_loader, num_epochs, learning_rate=0.001):\n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Train the model on the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.argmax(outputs,axis=1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(torch.argmax(labels,axis=1)).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Evaluate the model on the test data\n",
    "        evaluate_Tmodel(model, test_loader)\n",
    "\n",
    "def evaluate_Tmodel(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.argmax(outputs,axis=1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(torch.argmax(labels,axis=1)).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(flow_y[0:n])\n",
    "one_hot_labels = torch.nn.functional.one_hot(torch.tensor(train_labels_encoded), num_classes=len(np.unique(train_labels_encoded)))\n",
    "x_data=flow_data_a[0:n].reshape((len(flow_data_a[0:n]),abs(flow_data_a.shape[1]),flow_data_a.shape[2]*flow_data_a.shape[3])).astype(np.float64)\n",
    "#x_data=(x_data-np.mean(x_data))/(np.max(x_data)-np.min(x_data)) #preprocessing\n",
    "x_data/=255\n",
    "train_images_tensor = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "train_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "# Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=40,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tomorrow to do\n",
    "#work out paramaters for model\n",
    "model=TransformerModel(x_data.shape[2],20,20,3,15).to(device)\n",
    "train_Tmodel(model, train_loader, test_loader,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General experiments *salute* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def augment_with_missing_data(data, mask_fraction=0.3, random_seed=None):\n",
    "    \"\"\"\n",
    "    Augments a dataset by randomly removing large portions of the data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy array): Input data of shape (n, t, h, w)\n",
    "    - mask_fraction (float): Fraction of each image to be removed (0-1)\n",
    "    - random_seed (int): Optional seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - augmented_data (numpy array): Augmented dataset\n",
    "    \"\"\"\n",
    "    if random_seed:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    n, t, h, w = data.shape\n",
    "    augmented_data = data.copy()\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(t):\n",
    "            # Determine mask size\n",
    "            mask_height = int(h * mask_fraction)\n",
    "            mask_width = int(w * mask_fraction)\n",
    "            \n",
    "            # Choose a random position for the mask\n",
    "            top_left_y = random.randint(0, h - mask_height)\n",
    "            top_left_x = random.randint(0, w - mask_width)\n",
    "            \n",
    "            # Apply the mask\n",
    "            augmented_data[i, j, top_left_y:top_left_y + mask_height, top_left_x:top_left_x + mask_width] = 0\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "# Example usage:\n",
    "data=loaded(t=5)\n",
    "data.applySobel()\n",
    "data.augment()\n",
    "augmented_data = augment_with_missing_data(data.X, mask_fraction=0.3, random_seed=42)\n",
    "data_=augmented_data.reshape((len(augmented_data),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_components = 30  # Number of principal components to retain\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_data = pca.fit_transform(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_data, data.y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train a Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions and evaluate the model\n",
    "y_pred = rfc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loaded(filename=\"X_data_newMorph.npz\",t=5)\n",
    "data.applySobel()\n",
    "data.augment()\n",
    "\n",
    "data_alt=data.X.reshape((len(data.X),-1))\n",
    "pca_data = pca.fit_transform(data_alt)\n",
    "y_pred = rfc.predict(pca_data)\n",
    "accuracy = accuracy_score(data.y, y_pred)\n",
    "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n",
    "print(\"\\nClassification Report:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
