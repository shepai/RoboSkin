{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import getsizeof\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loaded:\n",
    "    def __init__(self,t=20):\n",
    "        data = np.load(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/texture-tactip/X_texture.npz\") #load data\n",
    "        for array_name in data:\n",
    "            self.X=(data[array_name].astype(np.uint8))\n",
    "        data = np.load(\"C:/Users/dexte/Documents/GitHub/RoboSkin/Code/NewRigExperiments/texture-tactip/y_texture.npz\") #load data\n",
    "        for array_name in data:\n",
    "            self.y=(data[array_name].astype(np.uint8))\n",
    "        self.keys={}\n",
    "        self.files=['wool', 'LacedMatt', 'Gfoam', 'bubble', 'Efoam', 'cotton', 'Flat', 'felt', 'Ffoam']\n",
    "        for i, name in enumerate(self.files):\n",
    "            self.keys[i]=name\n",
    "        self.keys={14: 'wool', 23: 'jeans', 20: 'LacedMatt', 22: 'Gfoam', 27: 'bubble', 16: 'Efoam', 21: 'cotton', 11: 'Flat', 24: 'felt', 26: 'Ffoam'}\n",
    "        print(\"Dataset size:\",self.X.shape[0],\"/nWindow size:\",self.X.shape[1],\"/nImage:\",self.X.shape[2:])\n",
    "        print(\"Memory needed:\",round(getsizeof(self.X)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "        assert self.X.shape[0]==self.y.shape[0],\"Incorrect data size match y=\"+str(self.y.shape[0])+\" x=\"+str(self.X.shape[0])\n",
    "        self.X=self.X[:,0:t]\n",
    "        #randomize order\n",
    "        n_samples = self.X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        shuffled_data = self.X[indices]\n",
    "        shuffled_labels = self.y[indices]\n",
    "        self.X=shuffled_data\n",
    "        self.y=shuffled_labels\n",
    "    def augment(self):\n",
    "        #create rotations\n",
    "        self.AugmentedX=np.zeros((len(self.X)*3,*self.X.shape[1:]),dtype=np.uint8)\n",
    "        self.Augmentedy=np.zeros_like(np.concatenate((self.y,self.y,self.y)))\n",
    "        for k,i in enumerate(range(0,len(self.AugmentedX),3)): #loop through the normal data and new data\n",
    "            for j in range(len(self.X[0])):\n",
    "                self.AugmentedX[i][j]=np.copy(self.X[k][j])\n",
    "                self.AugmentedX[i+1][j]=cv2.resize(cv2.rotate(self.X[k][j].copy(), cv2.ROTATE_90_CLOCKWISE),(self.X[k][j].shape[1],self.X[k][j].shape[0]),interpolation=cv2.INTER_AREA)\n",
    "                self.AugmentedX[i+2][j]=cv2.resize(cv2.rotate(self.X[k][j].copy(), cv2.ROTATE_180),(self.X[k][j].shape[1],self.X[k][j].shape[0]),interpolation=cv2.INTER_AREA)\n",
    "                self.Augmentedy[i+1]=self.y[k]\n",
    "                self.Augmentedy[i+2]=self.y[k]\n",
    "                self.Augmentedy[i]=self.y[k]\n",
    "                #self.AugmentedX[i+3][j]=cv2.rotate(self.X[k][j], cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        print(\"Dataset size:\",self.AugmentedX.shape[0],\"/nWindow size:\",self.X.shape[1],\"/nImage:\",self.X.shape[2:])\n",
    "        print(\"Memory needed:\",round(getsizeof(self.AugmentedX)/ 1024 / 1024/ 1024,2),\"GB\")\n",
    "        del self.X\n",
    "        del self.y\n",
    "    def applySobel(self):\n",
    "        for i in range(len(self.X)): #crop all images individually\n",
    "            for j in range(len(self.X[0])):\n",
    "                image=self.X[i][j]\n",
    "                # Apply Sobel filter in x-direction\n",
    "                sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # ksize=3 for a 3x3 Sobel kernel\n",
    "\n",
    "                # Apply Sobel filter in y-direction\n",
    "                sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "                # Convert the results back to uint8\n",
    "                sobel_x = np.uint8(np.absolute(sobel_x))\n",
    "                sobel_y = np.uint8(np.absolute(sobel_y))\n",
    "\n",
    "                # Combine the results to get the final edge-detected image\n",
    "                sobel_combined = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "                self.X[i][j]=sobel_combined\n",
    "data=loaded(t=5)\n",
    "data.applySobel()\n",
    "data.augment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tensor = torch.tensor(data.AugmentedX.reshape((len(data.AugmentedX),4*)), dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(data.Augmentedy, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=120, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
