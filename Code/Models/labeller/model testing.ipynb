{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import letRun #This library can be deleted, it is used for debugging\n",
    "import RoboSkin as sk\n",
    "import cv2 \n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.version.cuda)\n",
    "print(\"GPU:\",torch.cuda.is_available())\n",
    "def sigmoid(x):                                        \n",
    "   return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA SET CREATOR\n",
    "class dataset:\n",
    "    def __init__(self,names=[\"flat_detection.avi\",\"edge_detection.avi\",\"flat_slip_detection.avi\"]):\n",
    "        self.path=letRun.path\n",
    "        self.names=names\n",
    "        self.SIZE=0.3\n",
    "        name=\"C:/Users/dexte/OneDrive/Documents/AI/Data_Labeller/pickle_imputer_small.pkl\" #use standard imputer or one for small\n",
    "        self.reg=None\n",
    "        with open(name,'rb') as file:\n",
    "            self.reg=pickle.load(file)\n",
    "    def predict(self,reg,dat):\n",
    "        p=reg.predict(dat)\n",
    "        p=(p.reshape((p.shape[0],p.shape[1]//2,2))*255/self.SIZE)\n",
    "        return p\n",
    "    def generate(self,STORE=5,y_labels=[],scale=False):\n",
    "        BIG_DATA_X=None\n",
    "        BIG_DATA_y=None\n",
    "        assert len(y_labels)>=len(self.names),\"Incorrect length of labels\"\n",
    "        for j,name in enumerate(self.names):\n",
    "            skin=sk.Skin(videoFile=self.path+name)#videoFile=path+\"Movement4.avi\") #load skin object using demo video\n",
    "            cap = cv2.VideoCapture(self.path+name)\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            skin.sharpen=False #falsify the sharpness if recorded with sharpness\n",
    "            frame=skin.getFrame()\n",
    "            h=frame.shape[1]*self.SIZE\n",
    "            w=frame.shape[0]*self.SIZE\n",
    "            frame=cv2.resize(frame,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "            past=self.predict(self.reg,np.array([frame]))[0]\n",
    "            initial=past.copy()\n",
    "            initial_frame=frame.copy()\n",
    "            counter=0\n",
    "            X=[]\n",
    "            y=[] #label as [edge surface soft hard slippery]\n",
    "            lastFrames=[]\n",
    "            gyro=np.zeros((length,6))\n",
    "            try:\n",
    "                gyro=np.load(self.path+name.replace(\".avi\",\"\")+\"_gyro.npy\")\n",
    "            except:\n",
    "                pass\n",
    "            for i in range(length): #lop through all\n",
    "                frame_=skin.getFrame()\n",
    "                frame=cv2.resize(frame_,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "                points=self.predict(self.reg,np.array([frame]))[0]\n",
    "                #get pressure map\n",
    "                diff=np.sum(np.abs(initial_frame-frame))/(frame.shape[0]*3)\n",
    "                vecs=initial-points\n",
    "                lastFrames.append(np.concatenate((vecs.flatten(),gyro[i]*1)))\n",
    "                if len(lastFrames)>STORE: lastFrames.pop(0)\n",
    "                if diff>0.01 and len(lastFrames)==STORE: #significant contact\n",
    "                    X.append(np.array(lastFrames).flatten()) #store temporal element\n",
    "                    y.append(y_labels[j])\n",
    "                    counter+=1\n",
    "            if type(BIG_DATA_y)==type(None):\n",
    "                BIG_DATA_y=np.array(y.copy())\n",
    "                BIG_DATA_X=np.array(X.copy())\n",
    "            else:\n",
    "                BIG_DATA_y= np.concatenate((np.array(y.copy()),BIG_DATA_y))\n",
    "                BIG_DATA_X= np.concatenate((np.array(X.copy()),BIG_DATA_X))\n",
    "            print(name,\"Length:\",counter)\n",
    "        \n",
    "        a,b=np.array(BIG_DATA_X),np.array(BIG_DATA_y)\n",
    "        if scale:\n",
    "            scaler1 = preprocessing.StandardScaler().fit(a)\n",
    "            a = scaler1.transform(a)\n",
    "        return a,b\n",
    "    def generate_average(self,STORE=5,y_labels=[]):\n",
    "        BIG_DATA_X=None\n",
    "        BIG_DATA_y=None\n",
    "        assert len(y_labels)>=len(self.names),\"Incorrect length of labels\"\n",
    "        for j,name in enumerate(self.names):\n",
    "            skin=sk.Skin(videoFile=self.path+name)#videoFile=path+\"Movement4.avi\") #load skin object using demo video\n",
    "            cap = cv2.VideoCapture(self.path+name)\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            skin.sharpen=False #falsify the sharpness if recorded with sharpness\n",
    "            frame=skin.getFrame()\n",
    "            h=frame.shape[1]*self.SIZE\n",
    "            w=frame.shape[0]*self.SIZE\n",
    "            frame=cv2.resize(frame,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "            past=self.predict(self.reg,np.array([frame]))[0]\n",
    "            initial=past.copy()\n",
    "            initial_frame=frame.copy()\n",
    "\n",
    "            X=[]\n",
    "            y=[] #label as [edge surface soft hard slippery]\n",
    "            lastFrames=[]\n",
    "            gyro=np.zeros((length,6))\n",
    "            try:\n",
    "                gyro=np.load(self.path+name.replace(\".avi\",\"\")+\"_gyro.npy\")\n",
    "            except:\n",
    "                pass\n",
    "            counter=0\n",
    "            for i in range(length): #lop through all\n",
    "                frame_=skin.getFrame()\n",
    "                frame=cv2.resize(frame_,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "                points=self.predict(self.reg,np.array([frame]))[0]\n",
    "                #get pressure map\n",
    "                diff=np.sum(np.abs(initial_frame-frame))/(frame.shape[0]*3)\n",
    "                vecs=initial-points\n",
    "                average=np.average(vecs,axis=0).flatten()\n",
    "                lastFrames.append(np.concatenate((average,gyro[i]/10)))\n",
    "                if len(lastFrames)>STORE: lastFrames.pop(0)\n",
    "                if diff>0.01 and len(lastFrames)==STORE: #significant contact\n",
    "                    X.append(np.array(lastFrames).flatten()) #store temporal element\n",
    "                    y.append(y_labels[j])\n",
    "                    counter+=1\n",
    "            if type(BIG_DATA_y)==type(None):\n",
    "                BIG_DATA_y=np.array(y.copy())\n",
    "                BIG_DATA_X=np.array(X.copy())\n",
    "            else:\n",
    "                BIG_DATA_y= np.concatenate((np.array(y.copy()),BIG_DATA_y))\n",
    "                BIG_DATA_X= np.concatenate((np.array(X.copy()),BIG_DATA_X))\n",
    "            print(name,\"Length:\",counter)\n",
    "        a,b=np.array(BIG_DATA_X),np.array(BIG_DATA_y)\n",
    "        #a=a.reshape((a.shape))\n",
    "        #regulise\n",
    "        print(np.sum(a))\n",
    "        scaler1 = preprocessing.StandardScaler().fit(a)\n",
    "        a = scaler1.transform(a)\n",
    "        return a,b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gyro plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro_edge=np.load(letRun.path+\"edge_detection\"+\"_gyro.npy\")\n",
    "gyro_flat=np.load(letRun.path+\"flat_detection\"+\"_gyro.npy\")\n",
    "gyro_slip=np.load(letRun.path+\"flat_slip_detection\"+\"_gyro.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gyro_edge[:,0],label=\"Accelerometer x\")\n",
    "plt.plot(gyro_edge[:,1],label=\"Accelerometer y\")\n",
    "plt.plot(gyro_edge[:,2],label=\"Accelerometer z\")\n",
    "\n",
    "plt.plot(gyro_flat[:,0],label=\"Accelerometer x\")\n",
    "plt.plot(gyro_flat[:,1],label=\"Accelerometer y\")\n",
    "plt.plot(gyro_flat[:,2],label=\"Accelerometer z\")\n",
    "\n",
    "plt.plot(gyro_slip[:,0],label=\"Accelerometer x\")\n",
    "plt.plot(gyro_slip[:,1],label=\"Accelerometer y\")\n",
    "plt.plot(gyro_slip[:,2],label=\"Accelerometer z\")\n",
    "\n",
    "plt.title(\"Accelerometer over time\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gyro_edge[:,3],c=\"g\",label=\"Accelerometer x\")\n",
    "plt.plot(gyro_edge[:,4],c=\"g\",label=\"Accelerometer y\")\n",
    "plt.plot(gyro_edge[:,5],c=\"g\",label=\"Accelerometer z\")\n",
    "\n",
    "plt.plot(gyro_flat[:,3],c=\"r\",label=\"Accelerometer x\")\n",
    "plt.plot(gyro_flat[:,4],c=\"r\",label=\"Accelerometer y\")\n",
    "plt.plot(gyro_flat[:,5],c=\"r\",label=\"Accelerometer z\")\n",
    "\n",
    "plt.plot(gyro_slip[:,3],c=\"b\",label=\"Accelerometer x\")\n",
    "plt.plot(gyro_slip[:,4],c=\"b\",label=\"Accelerometer y\")\n",
    "plt.plot(gyro_slip[:,5],c=\"b\",label=\"Accelerometer z\")\n",
    "\n",
    "plt.title(\"Gyro over time\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 8\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lin_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/labeller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m d\u001b[39m=\u001b[39mdataset()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X,ya\u001b[39m=\u001b[39md\u001b[39m.\u001b[39;49mgenerate(STORE\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,y_labels\u001b[39m=\u001b[39;49m[[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39msum(X))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Xa=X/10\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 8\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length): \u001b[39m#lop through all\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     frame_\u001b[39m=\u001b[39mskin\u001b[39m.\u001b[39;49mgetFrame()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     frame\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mresize(frame_,(\u001b[39mint\u001b[39m(h),\u001b[39mint\u001b[39m(w)),interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_AREA)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\u001b[39m.\u001b[39mflatten()\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n",
      "File \u001b[1;32mC:\\Users/dexte/github/RoboSkin/Code\\RoboSkin\\__init__.py:109\u001b[0m, in \u001b[0;36mSkin.getFrame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m     frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(frame, (\u001b[39mint\u001b[39m(frame\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mSF),\u001b[39m480\u001b[39m), interpolation \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mINTER_AREA)\n\u001b[0;32m    108\u001b[0m lab\u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2LAB)\n\u001b[1;32m--> 109\u001b[0m l_channel, a, b \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49msplit(lab)\n\u001b[0;32m    110\u001b[0m \u001b[39m# Applying CLAHE to L-channel\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# feel free to try different values for the limit and grid size:\u001b[39;00m\n\u001b[0;32m    112\u001b[0m clahe \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcreateCLAHE(clipLimit\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m, tileGridSize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m8\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path=\"C:/Users/dexte/github/RoboSkin/Code/Models/labeller/\"\n",
    "lin_path=\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/labeller\"\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]])\n",
    "print(np.sum(X))\n",
    "#Xa=X/10\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "print(data_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the size of the input (n) and output (m) layers\n",
    "n_inputs = X.shape[1]\n",
    "m_outputs = len(y[0])\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size,layers=[500,100,50],drop_out_prob=0.2):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        self.fc=[nn.Linear(input_size, layers[0])]\n",
    "        self.fc.append(nn.ReLU())\n",
    "        self.fc.append(nn.Dropout(p=drop_out_prob))\n",
    "        for i in range(1,len(layers)): #create layers \n",
    "                self.fc.append(nn.Linear(layers[i-1], layers[i]))\n",
    "                self.fc.append(nn.ReLU())\n",
    "                self.fc.append(nn.Dropout(p=drop_out_prob))\n",
    "        self.fc.append(nn.Linear(layers[-1], output_size))\n",
    "        self.fc_layers = nn.Sequential(*self.fc)\n",
    "    def forward(self, x):\n",
    "        x=self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def get_acc(predictions,should_be=[]):\n",
    "    predictions*=10 #normalize\n",
    "    pred_array=np.zeros_like(predictions)\n",
    "    inds=np.argmax(predictions[:,0:2],axis=1) #convert at threshold\n",
    "    inds2=np.argmax(predictions[:,2:-1],axis=1) #convert at threshold\n",
    "    for i in range(len(pred_array)):\n",
    "        pred_array[i][inds[i]]=1\n",
    "        pred_array[i][2+inds2[i]]=1\n",
    "    correct=0\n",
    "    for j,pred in enumerate(pred_array): #loopthrough predictions\n",
    "        inds=np.where(pred==1) #\n",
    "        if len(should_be)>0: #validation task\n",
    "            sb=should_be[j]*10\n",
    "            sb=np.where(sb==1)\n",
    "            if len(sb[0])==len(inds[0]):\n",
    "                c=0\n",
    "                for i in range(len(sb[0])):\n",
    "                    if sb[0][i]==inds[0][i]: c+=1\n",
    "                if c==len(sb[0]): correct+=1\n",
    "    return correct/len(should_be)\n",
    "\n",
    "def train(model,num_epochs=2500):\n",
    "    loss_ar=[]\n",
    "    accuracies=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_tensor)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "        # Zero gradients, backward pass, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ar.append(loss.item())\n",
    "        #predict\n",
    "        predictions = model(torch.tensor(X, dtype=torch.float32).to(device))\n",
    "        accuracies.append(get_acc(predictions.cpu().detach().numpy(),should_be=y))\n",
    "        # Print the current loss to monitor training progress\n",
    "        if epoch%1000==0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\",\"Accuracy\",accuracies[-1])\n",
    "    return np.array(loss_ar),np.array(accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2500], Loss: 0.0107 Accuracy 0.34547908232118757\n",
      "Epoch [1001/2500], Loss: 0.0016 Accuracy 0.3697705802968961\n",
      "Epoch [2001/2500], Loss: 0.0014 Accuracy 0.3697705802968961\n"
     ]
    }
   ],
   "source": [
    "plt.title(\"Loss while training tactile model\")\n",
    "plt.ylabel(\"Loss and accuracy/5\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "a,b=train(model)\n",
    "#plt.plot(a,label=\"Loss\")\n",
    "plt.plot(b,label=\"Accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct: 58.06451612903226 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['surface', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['edge', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['surface', 'hard'],\n",
       " ['surface', 'hard']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training, you can use the trained model for predictions on new data.\n",
    "# For example, if you have new input data 'X_new', you can do:\n",
    "with torch.no_grad():\n",
    "     model.eval()\n",
    "     predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "\n",
    "\n",
    "def predict(predictions,should_be=[]):\n",
    "    predictions*=10 #normalize\n",
    "    pred_array=np.zeros_like(predictions)\n",
    "    inds=np.argmax(predictions[:,0:2],axis=1) #convert at threshold\n",
    "    inds2=np.argmax(predictions[:,2:-1],axis=1) #convert at threshold\n",
    "    for i in range(len(pred_array)):\n",
    "        pred_array[i][inds[i]]=1\n",
    "        pred_array[i][2+inds2[i]]=1\n",
    "    names=[\"edge\", \"surface\", \"soft\", \"hard\", \"slippery\"]\n",
    "    array=[]\n",
    "    correct=0\n",
    "    for j,pred in enumerate(pred_array): #loopthrough predictions\n",
    "        inds=np.where(pred==1) #\n",
    "        ar=[]\n",
    "        if len(should_be)>0: #validation task\n",
    "            sb=should_be[j]*10\n",
    "            sb=np.where(sb==1)\n",
    "            if len(sb[0])==len(inds[0]):\n",
    "                c=0\n",
    "                for i in range(len(sb[0])):\n",
    "                    if sb[0][i]==inds[0][i]: c+=1\n",
    "                if c==len(sb[0]): correct+=1\n",
    "        for i in inds[0]:\n",
    "            ar.append(names[i])\n",
    "        array.append(ar)\n",
    "    if correct!=0: print(\"Percentage correct:\",(correct/len(should_be))*100,\"%\")\n",
    "    return array\n",
    "\n",
    "p=predict(predictions.cpu(),should_be=labels_test)\n",
    "p[0:10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "### With and without slippage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "Trial B 0 Acc: 0.0\n",
      "Epoch [1/2500], Loss: 0.0073 Accuracy 0.50625\n",
      "Epoch [1001/2500], Loss: 0.0012 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0012 Accuracy 0.5708333333333333\n",
      "Trial B 1 Acc: 0.26666666666666666\n",
      "Epoch [1/2500], Loss: 0.0173 Accuracy 0.53125\n",
      "Epoch [1001/2500], Loss: 0.0012 Accuracy 0.6604166666666667\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.8416666666666667\n",
      "Trial B 2 Acc: 0.48055555555555557\n",
      "Epoch [1/2500], Loss: 0.0100 Accuracy 0.45208333333333334\n",
      "Epoch [1001/2500], Loss: 0.0012 Accuracy 0.5833333333333334\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.55625\n",
      "Trial B 3 Acc: 0.4875\n",
      "Epoch [1/2500], Loss: 0.0100 Accuracy 0.43125\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.625\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.6958333333333333\n",
      "Trial B 4 Acc: 0.49333333333333335\n",
      "Epoch [1/2500], Loss: 0.0111 Accuracy 0.5583333333333333\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.58125\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.5875\n",
      "Trial B 5 Acc: 0.49722222222222223\n",
      "Epoch [1/2500], Loss: 0.0126 Accuracy 0.14375\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.6958333333333333\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.8583333333333333\n",
      "Trial B 6 Acc: 0.5571428571428572\n",
      "Epoch [1/2500], Loss: 0.0160 Accuracy 0.5333333333333333\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.9166666666666666\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.9541666666666667\n",
      "Trial B 7 Acc: 0.6125\n",
      "Epoch [1/2500], Loss: 0.0115 Accuracy 0.13958333333333334\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.65\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.6166666666666667\n",
      "Trial B 8 Acc: 0.6333333333333333\n",
      "Epoch [1/2500], Loss: 0.0053 Accuracy 0.42916666666666664\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.5729166666666666\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.6375\n",
      "Trial B 9 Acc: 0.6233333333333333\n",
      "Epoch [1/2500], Loss: 0.0106 Accuracy 0.1375\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.7\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.8125\n",
      "Trial B 10 Acc: 0.6477272727272727\n",
      "Epoch [1/2500], Loss: 0.0167 Accuracy 0.45625\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.8895833333333333\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.9541666666666667\n",
      "Trial B 11 Acc: 0.6749999999999999\n",
      "Epoch [1/2500], Loss: 0.0035 Accuracy 0.2875\n",
      "Epoch [1001/2500], Loss: 0.0015 Accuracy 0.6270833333333333\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.75\n",
      "Trial B 12 Acc: 0.6641025641025641\n",
      "Epoch [1/2500], Loss: 0.0202 Accuracy 0.44583333333333336\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.6270833333333333\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.8\n",
      "Trial B 13 Acc: 0.6660714285714285\n",
      "Epoch [1/2500], Loss: 0.0061 Accuracy 0.5479166666666667\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.58125\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.59375\n",
      "Trial B 14 Acc: 0.6572222222222222\n",
      "Epoch [1/2500], Loss: 0.0152 Accuracy 0.5916666666666667\n",
      "Epoch [1001/2500], Loss: 0.0013 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.5125\n",
      "Trial B 15 Acc: 0.6494791666666666\n",
      "Epoch [1/2500], Loss: 0.0130 Accuracy 0.53125\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.69375\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.8604166666666667\n",
      "Trial B 16 Acc: 0.6676470588235294\n",
      "Epoch [1/2500], Loss: 0.0181 Accuracy 0.5604166666666667\n",
      "Epoch [1001/2500], Loss: 0.0016 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Trial B 17 Acc: 0.6601851851851852\n",
      "Epoch [1/2500], Loss: 0.0137 Accuracy 0.475\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.7083333333333334\n",
      "Trial B 18 Acc: 0.6578947368421053\n",
      "Epoch [1/2500], Loss: 0.0132 Accuracy 0.5583333333333333\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.74375\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.8604166666666667\n",
      "Trial B 19 Acc: 0.6729166666666667\n",
      "Epoch [1/2500], Loss: 0.0139 Accuracy 0.3145833333333333\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.8020833333333334\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.8604166666666667\n",
      "Trial B 20 Acc: 0.6853174603174603\n",
      "Epoch [1/2500], Loss: 0.0124 Accuracy 0.44583333333333336\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5854166666666667\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5770833333333333\n",
      "Trial B 21 Acc: 0.678409090909091\n",
      "Epoch [1/2500], Loss: 0.0171 Accuracy 0.5604166666666667\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.58125\n",
      "Epoch [2001/2500], Loss: 0.0011 Accuracy 0.71875\n",
      "Trial B 22 Acc: 0.6855072463768116\n",
      "Epoch [1/2500], Loss: 0.0177 Accuracy 0.5354166666666667\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.5708333333333333\n",
      "Trial B 23 Acc: 0.6791666666666667\n",
      "Epoch [1/2500], Loss: 0.0071 Accuracy 0.5708333333333333\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.7229166666666667\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5666666666666667\n",
      "Trial B 24 Acc: 0.6733333333333335\n",
      "Epoch [1/2500], Loss: 0.0155 Accuracy 0.25625\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.6083333333333333\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.6479166666666667\n",
      "Trial B 25 Acc: 0.6698717948717949\n",
      "Epoch [1/2500], Loss: 0.0207 Accuracy 0.5479166666666667\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.8020833333333334\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.90625\n",
      "Trial B 26 Acc: 0.6817901234567901\n",
      "Epoch [1/2500], Loss: 0.0103 Accuracy 0.42291666666666666\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.6479166666666667\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.6895833333333333\n",
      "Trial B 27 Acc: 0.6889880952380952\n",
      "Epoch [1/2500], Loss: 0.0129 Accuracy 0.44166666666666665\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.59375\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.6541666666666667\n",
      "Trial B 28 Acc: 0.6836206896551725\n",
      "Epoch [1/2500], Loss: 0.0050 Accuracy 0.41458333333333336\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.6375\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.8375\n",
      "Trial B 29 Acc: 0.6872222222222224\n",
      "Epoch [1/2500], Loss: 0.0085 Accuracy 0.0375\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.7208333333333333\n",
      "Epoch [2001/2500], Loss: 0.0011 Accuracy 0.73125\n",
      "Trial B 30 Acc: 0.69247311827957\n",
      "Epoch [1/2500], Loss: 0.0127 Accuracy 0.5458333333333333\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.8333333333333334\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.8916666666666667\n",
      "Trial B 31 Acc: 0.7007812500000001\n",
      "Epoch [1/2500], Loss: 0.0099 Accuracy 0.075\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.575\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.5708333333333333\n",
      "Trial B 32 Acc: 0.6957070707070709\n",
      "Epoch [1/2500], Loss: 0.0139 Accuracy 0.56875\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.5979166666666667\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.6333333333333333\n",
      "Trial B 33 Acc: 0.6928921568627453\n",
      "Epoch [1/2500], Loss: 0.0078 Accuracy 0.5145833333333333\n",
      "Epoch [1001/2500], Loss: 0.0017 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.6833333333333333\n",
      "Trial B 34 Acc: 0.6964285714285716\n",
      "Epoch [1/2500], Loss: 0.0084 Accuracy 0.01875\n",
      "Epoch [1001/2500], Loss: 0.0014 Accuracy 0.5791666666666667\n",
      "Epoch [2001/2500], Loss: 0.0012 Accuracy 0.56875\n",
      "Trial B 35 Acc: 0.6918981481481484\n",
      "Epoch [1/2500], Loss: 0.0139 Accuracy 0.20416666666666666\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.6833333333333333\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.8666666666666667\n",
      "Trial B 36 Acc: 0.6995495495495498\n",
      "Epoch [1/2500], Loss: 0.0046 Accuracy 0.4395833333333333\n",
      "Epoch [1001/2500], Loss: 0.0015 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.6083333333333333\n",
      "Trial B 37 Acc: 0.6951754385964916\n",
      "Epoch [1/2500], Loss: 0.0134 Accuracy 0.37916666666666665\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.5895833333333333\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.60625\n",
      "Trial B 38 Acc: 0.6910256410256413\n",
      "Epoch [1/2500], Loss: 0.0082 Accuracy 0.43333333333333335\n",
      "Epoch [1001/2500], Loss: 0.0017 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Trial B 39 Acc: 0.6870833333333337\n",
      "Epoch [1/2500], Loss: 0.0115 Accuracy 0.56875\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.6208333333333333\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5666666666666667\n",
      "Trial B 40 Acc: 0.6833333333333338\n",
      "Epoch [1/2500], Loss: 0.0182 Accuracy 0.5458333333333333\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.7020833333333333\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.6979166666666666\n",
      "Trial B 41 Acc: 0.6857142857142862\n",
      "Epoch [1/2500], Loss: 0.0090 Accuracy 0.34375\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.60625\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.5520833333333334\n",
      "Trial B 42 Acc: 0.6821705426356593\n",
      "Epoch [1/2500], Loss: 0.0090 Accuracy 0.24583333333333332\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.7958333333333333\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.8645833333333334\n",
      "Trial B 43 Acc: 0.6871212121212126\n",
      "Epoch [1/2500], Loss: 0.0189 Accuracy 0.23125\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.6083333333333333\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.8666666666666667\n",
      "Trial B 44 Acc: 0.6918518518518523\n",
      "Epoch [1/2500], Loss: 0.0057 Accuracy 0.5604166666666667\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.6770833333333334\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.6791666666666667\n",
      "Trial B 45 Acc: 0.6947463768115946\n",
      "Epoch [1/2500], Loss: 0.0188 Accuracy 0.4041666666666667\n",
      "Epoch [1001/2500], Loss: 0.0015 Accuracy 0.6041666666666666\n",
      "Epoch [2001/2500], Loss: 0.0013 Accuracy 0.5708333333333333\n",
      "Trial B 46 Acc: 0.691312056737589\n",
      "Epoch [1/2500], Loss: 0.0060 Accuracy 0.5625\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.68125\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.9020833333333333\n",
      "Trial B 47 Acc: 0.6960069444444447\n",
      "Epoch [1/2500], Loss: 0.0183 Accuracy 0.325\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5791666666666667\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.7479166666666667\n",
      "Trial B 48 Acc: 0.6993197278911567\n",
      "Epoch [1/2500], Loss: 0.0089 Accuracy 0.44166666666666665\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.64375\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.6375\n",
      "Trial B 49 Acc: 0.6978333333333336\n",
      "Epoch [1/2500], Loss: 0.0173 Accuracy 0.56875\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5666666666666667\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.61875\n",
      "Trial B 50 Acc: 0.6954248366013075\n",
      "Epoch [1/2500], Loss: 0.0136 Accuracy 0.45416666666666666\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.575\n",
      "Trial B 51 Acc: 0.6923076923076926\n",
      "Epoch [1/2500], Loss: 0.0052 Accuracy 0.020833333333333332\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.8625\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.9291666666666667\n",
      "Trial B 52 Acc: 0.697798742138365\n",
      "Epoch [1/2500], Loss: 0.0060 Accuracy 0.44375\n",
      "Epoch [1001/2500], Loss: 0.0014 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.5708333333333333\n",
      "Trial B 53 Acc: 0.6947530864197533\n",
      "Epoch [1/2500], Loss: 0.0196 Accuracy 0.33541666666666664\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.56875\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.58125\n",
      "Trial B 54 Acc: 0.691818181818182\n",
      "Epoch [1/2500], Loss: 0.0153 Accuracy 0.13958333333333334\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.70625\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.8770833333333333\n",
      "Trial B 55 Acc: 0.6971726190476193\n",
      "Epoch [1/2500], Loss: 0.0100 Accuracy 0.43125\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.5708333333333333\n",
      "Trial B 56 Acc: 0.6942982456140353\n",
      "Epoch [1/2500], Loss: 0.0057 Accuracy 0.5708333333333333\n",
      "Epoch [1001/2500], Loss: 0.0012 Accuracy 0.5916666666666667\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.5708333333333333\n",
      "Trial B 57 Acc: 0.6915229885057472\n",
      "Epoch [1/2500], Loss: 0.0102 Accuracy 0.05625\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.625\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.6479166666666667\n",
      "Trial B 58 Acc: 0.6930790960451978\n",
      "Epoch [1/2500], Loss: 0.0427 Accuracy 0.45208333333333334\n",
      "Epoch [1001/2500], Loss: 0.0012 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.75\n",
      "Trial B 59 Acc: 0.6904166666666668\n",
      "Epoch [1/2500], Loss: 0.0180 Accuracy 0.17916666666666667\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.675\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.6166666666666667\n",
      "Trial B 60 Acc: 0.6890710382513662\n",
      "Epoch [1/2500], Loss: 0.0177 Accuracy 0.5291666666666667\n",
      "Epoch [1001/2500], Loss: 0.0016 Accuracy 0.5416666666666666\n",
      "Epoch [2001/2500], Loss: 0.0016 Accuracy 0.5166666666666667\n",
      "Trial B 61 Acc: 0.6858870967741937\n",
      "Epoch [1/2500], Loss: 0.0055 Accuracy 0.5666666666666667\n",
      "Epoch [1001/2500], Loss: 0.0012 Accuracy 0.5666666666666667\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.5666666666666667\n",
      "Trial B 62 Acc: 0.6834656084656086\n",
      "Epoch [1/2500], Loss: 0.0079 Accuracy 0.40625\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.6104166666666667\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.5854166666666667\n",
      "Trial B 63 Acc: 0.6811197916666667\n",
      "Epoch [1/2500], Loss: 0.0053 Accuracy 0.5145833333333333\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.6354166666666666\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.9104166666666667\n",
      "Trial B 64 Acc: 0.6853846153846155\n",
      "Epoch [1/2500], Loss: 0.0072 Accuracy 0.010416666666666666\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.65\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.6604166666666667\n",
      "Trial B 65 Acc: 0.6830808080808081\n",
      "Epoch [1/2500], Loss: 0.0127 Accuracy 0.5083333333333333\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.675\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.5708333333333333\n",
      "Trial B 66 Acc: 0.6808457711442786\n",
      "Epoch [1/2500], Loss: 0.0199 Accuracy 0.5458333333333333\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.5916666666666667\n",
      "Trial B 67 Acc: 0.6786764705882353\n",
      "Epoch [1/2500], Loss: 0.0156 Accuracy 0.022916666666666665\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.5666666666666667\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.5666666666666667\n",
      "Trial B 68 Acc: 0.6765700483091787\n",
      "Epoch [1/2500], Loss: 0.0123 Accuracy 0.43125\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.7\n",
      "Trial B 69 Acc: 0.6751190476190476\n",
      "Epoch [1/2500], Loss: 0.0089 Accuracy 0.5229166666666667\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.5666666666666667\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.6375\n",
      "Trial B 70 Acc: 0.6757042253521127\n",
      "Epoch [1/2500], Loss: 0.0120 Accuracy 0.5416666666666666\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.6208333333333333\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.79375\n",
      "Trial B 71 Acc: 0.6737268518518519\n",
      "Epoch [1/2500], Loss: 0.0189 Accuracy 0.55\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.5854166666666667\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.6229166666666667\n",
      "Trial B 72 Acc: 0.6718036529680365\n",
      "Epoch [1/2500], Loss: 0.0110 Accuracy 0.5708333333333333\n",
      "Epoch [1001/2500], Loss: 0.0017 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0012 Accuracy 0.6166666666666667\n",
      "Trial B 73 Acc: 0.6699324324324324\n",
      "Epoch [1/2500], Loss: 0.0166 Accuracy 0.425\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.5729166666666666\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.58125\n",
      "Trial B 74 Acc: 0.6689999999999999\n",
      "Epoch [1/2500], Loss: 0.0044 Accuracy 0.43125\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.6104166666666667\n",
      "Trial B 75 Acc: 0.6672149122807017\n",
      "Epoch [1/2500], Loss: 0.0061 Accuracy 0.39375\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.7791666666666667\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.7041666666666667\n",
      "Trial B 76 Acc: 0.6713203463203463\n",
      "Epoch [1/2500], Loss: 0.0127 Accuracy 0.49583333333333335\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.7\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.6625\n",
      "Trial B 77 Acc: 0.6711538461538461\n",
      "Epoch [1/2500], Loss: 0.0097 Accuracy 0.027083333333333334\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.6395833333333333\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.5458333333333333\n",
      "Trial B 78 Acc: 0.6694092827004219\n",
      "Epoch [1/2500], Loss: 0.0172 Accuracy 0.56875\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.6791666666666667\n",
      "Epoch [2001/2500], Loss: 0.0007 Accuracy 0.8083333333333333\n",
      "Trial B 79 Acc: 0.6729166666666666\n",
      "Epoch [1/2500], Loss: 0.0141 Accuracy 0.5145833333333333\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.59375\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.6270833333333333\n",
      "Trial B 80 Acc: 0.6747942386831276\n",
      "Epoch [1/2500], Loss: 0.0217 Accuracy 0.3729166666666667\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.7041666666666667\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.8291666666666667\n",
      "Trial B 81 Acc: 0.675\n",
      "Epoch [1/2500], Loss: 0.0111 Accuracy 0.022916666666666665\n",
      "Epoch [1001/2500], Loss: 0.0015 Accuracy 0.5625\n",
      "Epoch [2001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Trial B 82 Acc: 0.673293172690763\n",
      "Epoch [1/2500], Loss: 0.0177 Accuracy 0.35625\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.7625\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.9083333333333333\n",
      "Trial B 83 Acc: 0.676686507936508\n",
      "Epoch [1/2500], Loss: 0.0079 Accuracy 0.004166666666666667\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.7666666666666667\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.79375\n",
      "Trial B 84 Acc: 0.6796078431372549\n",
      "Epoch [1/2500], Loss: 0.0053 Accuracy 0.5604166666666667\n",
      "Epoch [1001/2500], Loss: 0.0015 Accuracy 0.6083333333333333\n",
      "Epoch [2001/2500], Loss: 0.0013 Accuracy 0.6\n",
      "Trial B 85 Acc: 0.6787790697674418\n",
      "Epoch [1/2500], Loss: 0.0128 Accuracy 0.04583333333333333\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5875\n",
      "Trial B 86 Acc: 0.6771072796934866\n",
      "Epoch [1/2500], Loss: 0.0209 Accuracy 0.5229166666666667\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.6458333333333334\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.63125\n",
      "Trial B 87 Acc: 0.6765151515151515\n",
      "Epoch [1/2500], Loss: 0.0092 Accuracy 0.19583333333333333\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.5604166666666667\n",
      "Trial B 88 Acc: 0.6749063670411984\n",
      "Epoch [1/2500], Loss: 0.0136 Accuracy 0.4\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.575\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.5958333333333333\n",
      "Trial B 89 Acc: 0.6741666666666666\n",
      "Epoch [1/2500], Loss: 0.0102 Accuracy 0.16041666666666668\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.65625\n",
      "Epoch [2001/2500], Loss: 0.0015 Accuracy 0.66875\n",
      "Trial B 90 Acc: 0.6757326007326008\n",
      "Epoch [1/2500], Loss: 0.0087 Accuracy 0.07708333333333334\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.5708333333333333\n",
      "Trial B 91 Acc: 0.6741847826086956\n",
      "Epoch [1/2500], Loss: 0.0164 Accuracy 0.44375\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.6541666666666667\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.7291666666666666\n",
      "Trial B 92 Acc: 0.6759856630824372\n",
      "Epoch [1/2500], Loss: 0.0109 Accuracy 0.58125\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.66875\n",
      "Epoch [2001/2500], Loss: 0.0017 Accuracy 0.5979166666666667\n",
      "Trial B 93 Acc: 0.6749113475177305\n",
      "Epoch [1/2500], Loss: 0.0087 Accuracy 0.5729166666666666\n",
      "Epoch [1001/2500], Loss: 0.0008 Accuracy 0.7125\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.8\n",
      "Trial B 94 Acc: 0.6778947368421053\n",
      "Epoch [1/2500], Loss: 0.0204 Accuracy 0.35208333333333336\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.5770833333333333\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.575\n",
      "Trial B 95 Acc: 0.6763888888888889\n",
      "Epoch [1/2500], Loss: 0.0125 Accuracy 0.2625\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.5375\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.6166666666666667\n",
      "Trial B 96 Acc: 0.6771477663230241\n",
      "Epoch [1/2500], Loss: 0.0153 Accuracy 0.22291666666666668\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.7479166666666667\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.8666666666666667\n",
      "Trial B 97 Acc: 0.680017006802721\n",
      "Epoch [1/2500], Loss: 0.0151 Accuracy 0.43333333333333335\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5708333333333333\n",
      "Trial B 98 Acc: 0.6785353535353535\n",
      "Epoch [1/2500], Loss: 0.0185 Accuracy 0.4583333333333333\n",
      "Epoch [1001/2500], Loss: 0.0011 Accuracy 0.5708333333333333\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5666666666666667\n",
      "Trial B 99 Acc: 0.6770833333333333\n",
      "Epoch [1/2500], Loss: 0.0111 Accuracy 0.19583333333333333\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.6\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.63125\n",
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial A 0 Acc: 0.0\n",
      "Epoch [1/2500], Loss: 0.0205 Accuracy 0.3333333333333333\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 1 Acc: 0.2956989247311828\n",
      "Epoch [1/2500], Loss: 0.0074 Accuracy 0.3697705802968961\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 2 Acc: 0.3942652329749104\n",
      "Epoch [1/2500], Loss: 0.0119 Accuracy 0.3630229419703104\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 3 Acc: 0.44354838709677424\n",
      "Epoch [1/2500], Loss: 0.0100 Accuracy 0.3144399460188934\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.6437246963562753\n",
      "Trial A 4 Acc: 0.4731182795698925\n",
      "Epoch [1/2500], Loss: 0.0193 Accuracy 0.3360323886639676\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 5 Acc: 0.49372759856630827\n",
      "Epoch [1/2500], Loss: 0.0083 Accuracy 0.33063427800269907\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 6 Acc: 0.5076804915514593\n",
      "Epoch [1/2500], Loss: 0.0130 Accuracy 0.30364372469635625\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 7 Acc: 0.5188172043010753\n",
      "Epoch [1/2500], Loss: 0.0104 Accuracy 0.17273954116059378\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 8 Acc: 0.5280764635603346\n",
      "Epoch [1/2500], Loss: 0.0155 Accuracy 0.044534412955465584\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 9 Acc: 0.5349462365591398\n",
      "Epoch [1/2500], Loss: 0.0108 Accuracy 0.3684210526315789\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 10 Acc: 0.5405669599217986\n",
      "Epoch [1/2500], Loss: 0.0146 Accuracy 0.28205128205128205\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 11 Acc: 0.5448028673835126\n",
      "Epoch [1/2500], Loss: 0.0171 Accuracy 0.34547908232118757\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 12 Acc: 0.5483870967741936\n",
      "Epoch [1/2500], Loss: 0.0075 Accuracy 0.03508771929824561\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 13 Acc: 0.5510752688172044\n",
      "Epoch [1/2500], Loss: 0.0147 Accuracy 0.29554655870445345\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 14 Acc: 0.5537634408602151\n",
      "Epoch [1/2500], Loss: 0.0068 Accuracy 0.006747638326585695\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 15 Acc: 0.5561155913978495\n",
      "Epoch [1/2500], Loss: 0.0087 Accuracy 0.044534412955465584\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 16 Acc: 0.558191018342821\n",
      "Epoch [1/2500], Loss: 0.0082 Accuracy 0.024291497975708502\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 17 Acc: 0.5600358422939069\n",
      "Epoch [1/2500], Loss: 0.0046 Accuracy 0.11605937921727395\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 18 Acc: 0.5616864742501416\n",
      "Epoch [1/2500], Loss: 0.0099 Accuracy 0.31713900134952766\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 19 Acc: 0.5631720430107527\n",
      "Epoch [1/2500], Loss: 0.0155 Accuracy 0.31713900134952766\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 20 Acc: 0.5647721454173068\n",
      "Epoch [1/2500], Loss: 0.0086 Accuracy 0.2807017543859649\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 21 Acc: 0.5657380254154449\n",
      "Epoch [1/2500], Loss: 0.0161 Accuracy 0.3184885290148448\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 22 Acc: 0.5666199158485274\n",
      "Epoch [1/2500], Loss: 0.0155 Accuracy 0.18083670715249664\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 23 Acc: 0.5678763440860216\n",
      "Epoch [1/2500], Loss: 0.0125 Accuracy 0.21187584345479082\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 24 Acc: 0.5688172043010754\n",
      "Epoch [1/2500], Loss: 0.0119 Accuracy 0.3576248313090418\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 25 Acc: 0.5698924731182797\n",
      "Epoch [1/2500], Loss: 0.0060 Accuracy 0.008097165991902834\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 26 Acc: 0.5706889685384311\n",
      "Epoch [1/2500], Loss: 0.0024 Accuracy 0.26180836707152494\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 27 Acc: 0.5710445468509986\n",
      "Epoch [1/2500], Loss: 0.0073 Accuracy 0.04048582995951417\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 28 Acc: 0.5717463848720803\n",
      "Epoch [1/2500], Loss: 0.0080 Accuracy 0.1349527665317139\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 29 Acc: 0.5722222222222224\n",
      "Epoch [1/2500], Loss: 0.0131 Accuracy 0.3427800269905533\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 30 Acc: 0.5728407908428721\n",
      "Epoch [1/2500], Loss: 0.0153 Accuracy 0.28205128205128205\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 31 Acc: 0.5734206989247312\n",
      "Epoch [1/2500], Loss: 0.0083 Accuracy 0.29284750337381915\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 32 Acc: 0.5736396220267189\n",
      "Epoch [1/2500], Loss: 0.0037 Accuracy 0.2874493927125506\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 33 Acc: 0.5738456672991779\n",
      "Epoch [1/2500], Loss: 0.0097 Accuracy 0.3684210526315789\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 34 Acc: 0.5745007680491553\n",
      "Epoch [1/2500], Loss: 0.0109 Accuracy 0.32388663967611336\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 35 Acc: 0.5749701314217445\n",
      "Epoch [1/2500], Loss: 0.0040 Accuracy 0.1484480431848853\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6491228070175439\n",
      "Trial A 36 Acc: 0.5752688172043012\n",
      "Epoch [1/2500], Loss: 0.0103 Accuracy 0.2941970310391363\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 37 Acc: 0.5759762308998303\n",
      "Epoch [1/2500], Loss: 0.0087 Accuracy 0.37516869095816463\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 38 Acc: 0.5763716570168184\n",
      "Epoch [1/2500], Loss: 0.0041 Accuracy 0.3805668016194332\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 39 Acc: 0.576747311827957\n",
      "Epoch [1/2500], Loss: 0.0121 Accuracy 0.23751686909581646\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 40 Acc: 0.577104642014162\n",
      "Epoch [1/2500], Loss: 0.0144 Accuracy 0.3333333333333333\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 41 Acc: 0.5774449564772145\n",
      "Epoch [1/2500], Loss: 0.0074 Accuracy 0.28609986504723345\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 42 Acc: 0.57776944236059\n",
      "Epoch [1/2500], Loss: 0.0134 Accuracy 0.3657219973009447\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 43 Acc: 0.5780791788856304\n",
      "Epoch [1/2500], Loss: 0.0161 Accuracy 0.17813765182186234\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 44 Acc: 0.5783751493428911\n",
      "Epoch [1/2500], Loss: 0.0088 Accuracy 0.358974358974359\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 45 Acc: 0.5786582515194014\n",
      "Epoch [1/2500], Loss: 0.0073 Accuracy 0.3319838056680162\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 46 Acc: 0.5790436970944862\n",
      "Epoch [1/2500], Loss: 0.0080 Accuracy 0.2834008097165992\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.6545209176788124\n",
      "Trial A 47 Acc: 0.5790770609318995\n",
      "Epoch [1/2500], Loss: 0.0094 Accuracy 0.17408906882591094\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 48 Acc: 0.5791090629800306\n",
      "Epoch [1/2500], Loss: 0.0122 Accuracy 0.3562753036437247\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 49 Acc: 0.5793548387096773\n",
      "Epoch [1/2500], Loss: 0.0047 Accuracy 0.3738191632928475\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 50 Acc: 0.5796963946869069\n",
      "Epoch [1/2500], Loss: 0.0095 Accuracy 0.3508771929824561\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 51 Acc: 0.5798180314309346\n",
      "Epoch [1/2500], Loss: 0.0089 Accuracy 0.029689608636977057\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 52 Acc: 0.580036518563603\n",
      "Epoch [1/2500], Loss: 0.0052 Accuracy 0.31039136302294196\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 53 Acc: 0.5802469135802467\n",
      "Epoch [1/2500], Loss: 0.0162 Accuracy 0.3441295546558704\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 54 Acc: 0.5805474095796676\n",
      "Epoch [1/2500], Loss: 0.0084 Accuracy 0.13225371120107962\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 55 Acc: 0.5806451612903224\n",
      "Epoch [1/2500], Loss: 0.0107 Accuracy 0.252361673414305\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 56 Acc: 0.580928126768534\n",
      "Epoch [1/2500], Loss: 0.0194 Accuracy 0.19838056680161945\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 57 Acc: 0.5811086392287725\n",
      "Epoch [1/2500], Loss: 0.0097 Accuracy 0.3697705802968961\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 58 Acc: 0.5810096591944596\n",
      "Epoch [1/2500], Loss: 0.0109 Accuracy 0.19298245614035087\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 59 Acc: 0.5811827956989247\n",
      "Epoch [1/2500], Loss: 0.0087 Accuracy 0.06072874493927125\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 60 Acc: 0.5813502555966861\n",
      "Epoch [1/2500], Loss: 0.0047 Accuracy 0.3765182186234818\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 61 Acc: 0.5815123135622615\n",
      "Epoch [1/2500], Loss: 0.0115 Accuracy 0.3616734143049933\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 62 Acc: 0.581583888035501\n",
      "Epoch [1/2500], Loss: 0.0055 Accuracy 0.37112010796221323\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 63 Acc: 0.5818212365591398\n",
      "Epoch [1/2500], Loss: 0.0102 Accuracy 0.26450742240215924\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 64 Acc: 0.5819685690653432\n",
      "Epoch [1/2500], Loss: 0.0146 Accuracy 0.34547908232118757\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 65 Acc: 0.5820299771912675\n",
      "Epoch [1/2500], Loss: 0.0133 Accuracy 0.11605937921727395\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 66 Acc: 0.5823302840635532\n",
      "Epoch [1/2500], Loss: 0.0087 Accuracy 0.340080971659919\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 67 Acc: 0.5826217583807717\n",
      "Epoch [1/2500], Loss: 0.0076 Accuracy 0.09851551956815115\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 68 Acc: 0.5827489481065918\n",
      "Epoch [1/2500], Loss: 0.0082 Accuracy 0.3225371120107962\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 69 Acc: 0.5828725038402458\n",
      "Epoch [1/2500], Loss: 0.0121 Accuracy 0.021592442645074223\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 70 Acc: 0.5831440254429805\n",
      "Epoch [1/2500], Loss: 0.0115 Accuracy 0.36707152496626183\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 71 Acc: 0.583183990442055\n",
      "Epoch [1/2500], Loss: 0.0156 Accuracy 0.3360323886639676\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 72 Acc: 0.5832965090587716\n",
      "Epoch [1/2500], Loss: 0.0235 Accuracy 0.33738191632928477\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 73 Acc: 0.5834059866317932\n",
      "Epoch [1/2500], Loss: 0.0099 Accuracy 0.2874493927125506\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 74 Acc: 0.5834408602150538\n",
      "Epoch [1/2500], Loss: 0.0163 Accuracy 0.26045883940620784\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 75 Acc: 0.5836162988115451\n",
      "Epoch [1/2500], Loss: 0.0198 Accuracy 0.2726045883940621\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 76 Acc: 0.5837173579109064\n",
      "Epoch [1/2500], Loss: 0.0117 Accuracy 0.28475033738191635\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 77 Acc: 0.5838158257513097\n",
      "Epoch [1/2500], Loss: 0.0095 Accuracy 0.3549257759784076\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Trial A 78 Acc: 0.5839798557234246\n",
      "Epoch [1/2500], Loss: 0.0078 Accuracy 0.32388663967611336\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Trial A 79 Acc: 0.5842069892473118\n",
      "Epoch [1/2500], Loss: 0.0056 Accuracy 0.04183535762483131\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 80 Acc: 0.5842957652993496\n",
      "Epoch [1/2500], Loss: 0.0096 Accuracy 0.3576248313090418\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 81 Acc: 0.5844479412536061\n",
      "Epoch [1/2500], Loss: 0.0125 Accuracy 0.14439946018893388\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 82 Acc: 0.5845316750874466\n",
      "Epoch [1/2500], Loss: 0.0038 Accuracy 0.06477732793522267\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 83 Acc: 0.5845494111623145\n",
      "Epoch [1/2500], Loss: 0.0161 Accuracy 0.3684210526315789\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 84 Acc: 0.5845667299177736\n",
      "Epoch [1/2500], Loss: 0.0129 Accuracy 0.24156545209176788\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 85 Acc: 0.5847086771692923\n",
      "Epoch [1/2500], Loss: 0.0145 Accuracy 0.26045883940620784\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 86 Acc: 0.5848473612656038\n",
      "Epoch [1/2500], Loss: 0.0054 Accuracy 0.37112010796221323\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 87 Acc: 0.5849217986314761\n",
      "Epoch [1/2500], Loss: 0.0252 Accuracy 0.29284750337381915\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 88 Acc: 0.5849945632475535\n",
      "Epoch [1/2500], Loss: 0.0113 Accuracy 0.07017543859649122\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 89 Acc: 0.5850657108721625\n",
      "Epoch [1/2500], Loss: 0.0065 Accuracy 0.37112010796221323\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 90 Acc: 0.5851943755169562\n",
      "Epoch [1/2500], Loss: 0.0095 Accuracy 0.24156545209176788\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 91 Acc: 0.5853202431042543\n",
      "Epoch [1/2500], Loss: 0.0248 Accuracy 0.0931174089068826\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 92 Acc: 0.5853855937102556\n",
      "Epoch [1/2500], Loss: 0.0101 Accuracy 0.19298245614035087\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 93 Acc: 0.5853923587279799\n",
      "Epoch [1/2500], Loss: 0.0073 Accuracy 0.31713900134952766\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 94 Acc: 0.5854555744199208\n",
      "Epoch [1/2500], Loss: 0.0052 Accuracy 0.300944669365722\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 95 Acc: 0.5854054659498208\n",
      "Epoch [1/2500], Loss: 0.0149 Accuracy 0.2699055330634278\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Trial A 96 Acc: 0.5854672430994348\n",
      "Epoch [1/2500], Loss: 0.0055 Accuracy 0.358974358974359\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 97 Acc: 0.5855277594908932\n",
      "Epoch [1/2500], Loss: 0.0077 Accuracy 0.02834008097165992\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 98 Acc: 0.585587053328989\n",
      "Epoch [1/2500], Loss: 0.0043 Accuracy 0.32118758434547906\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6491228070175439\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 99 Acc: 0.5856989247311829\n",
      "Epoch [1/2500], Loss: 0.0040 Accuracy 0.3657219973009447\n",
      "Epoch [1001/2500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/2500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Accuracies 0.5915591397849463 0.6824166666666666\n",
      "0.06323725318344744 0.04466396481079682\n"
     ]
    }
   ],
   "source": [
    "TRIALS=100\n",
    "#without slippage\n",
    "#merge datasets\n",
    "d=dataset(names=[\"flat_detection.avi\",\"edge_detection.avi\"])\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0]])\n",
    "Xa=X/100\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(Xa, ya, test_size=0.20, random_state=42)\n",
    "\n",
    "average_a=np.zeros((2500,))\n",
    "p_=0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial B\",i,\"Acc:\",p_/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model)\n",
    "    average_a+=np.array(a)\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p_+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "\n",
    "average_a=average_a/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "p_/=TRIALS\n",
    "\n",
    "average_a1=np.zeros((2500,))\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]])\n",
    "Xa=X/100\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(Xa, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model)\n",
    "    average_a1+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a1=average_a1/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "p/=TRIALS\n",
    "#plt.plot(average_l,'--',c=\"b\",label=\"Loss with slip data\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"Accuracy with slip data\")\n",
    "\n",
    "\n",
    "print(\"Accuracies\",p,p_)\n",
    "plt.title(\"Performance of models with and without slippage over 100 trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a,c=\"r\",label=\"Accuracy without slip data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(np.std(average_a),np.std(average_a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path+\"BignetNoSlip\",average_a)\n",
    "np.save(path+\"Bignet\",average_a1)\n",
    "\n",
    "\n",
    "ar1=np.load(path+\"40netNoSlip.npy\")\n",
    "ar2=np.load(path+\"40net.npy\")\n",
    "plt.title(\"Performance of models with and without slippage over 100 trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(ar1,c=\"g\",label=\"Accuracy compressed input without slip data\")\n",
    "plt.plot(ar2,c=\"y\",label=\"Accuracy compressed input\")\n",
    "plt.plot(average_a,c=\"r\",label=\"Accuracy without slip data\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial A 0 Acc: 0.0\n",
      "Epoch [1/2500], Loss: 0.0138 Accuracy 0.2941970310391363\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.6032388663967612\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.5870445344129555\n",
      "Trial A 1 Acc: 0.2956989247311828\n",
      "Epoch [1/2500], Loss: 0.0054 Accuracy 0.29554655870445345\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.5843454790823212\n",
      "Epoch [2001/2500], Loss: 0.0008 Accuracy 0.5573549257759784\n",
      "Trial A 2 Acc: 0.36738351254480284\n",
      "Epoch [1/2500], Loss: 0.0173 Accuracy 0.06882591093117409\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.6518218623481782\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.6531713900134952\n",
      "Trial A 3 Acc: 0.42069892473118276\n",
      "Epoch [1/2500], Loss: 0.0207 Accuracy 0.09851551956815115\n"
     ]
    }
   ],
   "source": [
    "TRIALS=50\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]],scale=True)\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(Xa, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "average_a1=np.zeros((2500,))\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model)\n",
    "    average_a1+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a1=average_a1/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]],scale=False)\n",
    "X=X/100\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(Xa, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "average_a=np.zeros((2500,))\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model)\n",
    "    average_a+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a=average_a/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "\n",
    "plt.title(\"Performance of models with and without preprocessing\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a,c=\"r\",label=\"Without preprocessing\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"With preprocessing\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0\n",
      "Epoch [1/2500], Loss: 0.0451 Accuracy 0.16173361522198731\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1416490486257928\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.12790697674418605\n",
      "Epoch [1/2500], Loss: 0.0177 Accuracy 0.12050739957716702\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.113107822410148\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.11416490486257928\n",
      "Epoch [1/2500], Loss: 0.0498 Accuracy 0.22727272727272727\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.15961945031712474\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.1427061310782241\n",
      "Epoch [1/2500], Loss: 0.0107 Accuracy 0.2452431289640592\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.16913319238900634\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.16596194503171247\n",
      "Epoch [1/2500], Loss: 0.0390 Accuracy 0.2769556025369979\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.13530655391120508\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.13530655391120508\n",
      "Epoch [1/2500], Loss: 0.0253 Accuracy 0.0507399577167019\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.12050739957716702\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.16596194503171247\n",
      "Epoch [1/2500], Loss: 0.0481 Accuracy 0.24841437632135308\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16701902748414377\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.11839323467230443\n",
      "Epoch [1/2500], Loss: 0.0246 Accuracy 0.023255813953488372\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.13530655391120508\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.16913319238900634\n",
      "Epoch [1/2500], Loss: 0.0581 Accuracy 0.0708245243128964\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.13002114164904863\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.15010570824524314\n",
      "Epoch [1/2500], Loss: 0.0093 Accuracy 0.15327695560253699\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.160676532769556\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.13530655391120508\n",
      "Epoch [1/2500], Loss: 0.0654 Accuracy 0.004228329809725159\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1744186046511628\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.12473572938689217\n",
      "Epoch [1/2500], Loss: 0.0499 Accuracy 0.2547568710359408\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.17653276955602537\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.1744186046511628\n",
      "Epoch [1/2500], Loss: 0.0666 Accuracy 0.0\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.15750528541226216\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.13107822410147993\n",
      "Epoch [1/2500], Loss: 0.0431 Accuracy 0.006342494714587738\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.1416490486257928\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.12790697674418605\n",
      "Epoch [1/2500], Loss: 0.0702 Accuracy 0.0\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16807610993657504\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.1638477801268499\n",
      "Epoch [1/2500], Loss: 0.0270 Accuracy 0.008456659619450317\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.12579281183932348\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.13107822410147993\n",
      "Epoch [1/2500], Loss: 0.0723 Accuracy 0.08985200845665962\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.16807610993657504\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.160676532769556\n",
      "Epoch [1/2500], Loss: 0.0323 Accuracy 0.12790697674418605\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.14904862579281183\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.17124735729386892\n",
      "Epoch [1/2500], Loss: 0.0448 Accuracy 0.006342494714587738\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.15644820295983086\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.14482029598308668\n",
      "Epoch [1/2500], Loss: 0.0294 Accuracy 0.005285412262156448\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.1321353065539112\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.1321353065539112\n",
      "2 0.0\n",
      "Epoch [1/2500], Loss: 0.0305 Accuracy 0.11416490486257928\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.13953488372093023\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.14059196617336153\n",
      "Epoch [1/2500], Loss: 0.0348 Accuracy 0.1522198731501057\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1553911205073996\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.16807610993657504\n",
      "Epoch [1/2500], Loss: 0.1242 Accuracy 0.0010570824524312897\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.14059196617336153\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.1649048625792812\n",
      "Epoch [1/2500], Loss: 0.0324 Accuracy 0.08773784355179703\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.15327695560253699\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.10676532769556026\n",
      "Epoch [1/2500], Loss: 0.0066 Accuracy 0.1638477801268499\n",
      "Epoch [1001/2500], Loss: 0.0007 Accuracy 0.19661733615221988\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.18604651162790697\n",
      "Epoch [1/2500], Loss: 0.0111 Accuracy 0.2346723044397463\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.12684989429175475\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.1733615221987315\n",
      "Epoch [1/2500], Loss: 0.0290 Accuracy 0.17547568710359407\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.15327695560253699\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.1744186046511628\n",
      "Epoch [1/2500], Loss: 0.0451 Accuracy 0.15010570824524314\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16173361522198731\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.11205073995771671\n",
      "Epoch [1/2500], Loss: 0.0128 Accuracy 0.0507399577167019\n",
      "Epoch [1001/2500], Loss: 0.0017 Accuracy 0.15644820295983086\n",
      "Epoch [2001/2500], Loss: 0.0017 Accuracy 0.15644820295983086\n",
      "Epoch [1/2500], Loss: 0.0508 Accuracy 0.23150105708245244\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.12579281183932348\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.1321353065539112\n",
      "Epoch [1/2500], Loss: 0.0251 Accuracy 0.2346723044397463\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.17230443974630022\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.15961945031712474\n",
      "Epoch [1/2500], Loss: 0.0556 Accuracy 0.07822410147991543\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.1321353065539112\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.13002114164904863\n",
      "Epoch [1/2500], Loss: 0.0444 Accuracy 0.026427061310782242\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.1649048625792812\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.14904862579281183\n",
      "Epoch [1/2500], Loss: 0.0280 Accuracy 0.13107822410147993\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16807610993657504\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.12367864693446089\n",
      "Epoch [1/2500], Loss: 0.0199 Accuracy 0.0708245243128964\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.17230443974630022\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.16279069767441862\n",
      "Epoch [1/2500], Loss: 0.0615 Accuracy 0.18921775898520085\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.12896405919661733\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.11945031712473574\n",
      "Epoch [1/2500], Loss: 0.0432 Accuracy 0.16701902748414377\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.13742071881606766\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.13002114164904863\n",
      "Epoch [1/2500], Loss: 0.0343 Accuracy 0.02536997885835095\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.16913319238900634\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.16913319238900634\n",
      "Epoch [1/2500], Loss: 0.0104 Accuracy 0.13002114164904863\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.1331923890063425\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.13953488372093023\n",
      "Epoch [1/2500], Loss: 0.0332 Accuracy 0.226215644820296\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.13742071881606766\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.14904862579281183\n",
      "3 0.0\n",
      "Epoch [1/2500], Loss: 0.0445 Accuracy 0.2167019027484144\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.17758985200845667\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.18181818181818182\n",
      "Epoch [1/2500], Loss: 0.0409 Accuracy 0.22410147991543342\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.16807610993657504\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.17019027484143764\n",
      "Epoch [1/2500], Loss: 0.0351 Accuracy 0.026427061310782242\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.17864693446088795\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.16807610993657504\n",
      "Epoch [1/2500], Loss: 0.0313 Accuracy 0.11416490486257928\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.11733615221987315\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.17230443974630022\n",
      "Epoch [1/2500], Loss: 0.0317 Accuracy 0.15327695560253699\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.1733615221987315\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.15750528541226216\n",
      "Epoch [1/2500], Loss: 0.0507 Accuracy 0.03805496828752643\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1427061310782241\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.1416490486257928\n",
      "Epoch [1/2500], Loss: 0.0288 Accuracy 0.15961945031712474\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.11839323467230443\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.16807610993657504\n",
      "Epoch [1/2500], Loss: 0.0234 Accuracy 0.12473572938689217\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.17124735729386892\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.16913319238900634\n",
      "Epoch [1/2500], Loss: 0.0310 Accuracy 0.06448202959830866\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.15327695560253699\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.11945031712473574\n",
      "Epoch [1/2500], Loss: 0.0332 Accuracy 0.05179704016913319\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1649048625792812\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.16913319238900634\n",
      "Epoch [1/2500], Loss: 0.0491 Accuracy 0.015856236786469344\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.1649048625792812\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.16913319238900634\n",
      "Epoch [1/2500], Loss: 0.0528 Accuracy 0.0010570824524312897\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.1416490486257928\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.14059196617336153\n",
      "Epoch [1/2500], Loss: 0.0395 Accuracy 0.22938689217758984\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.12684989429175475\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.1331923890063425\n",
      "Epoch [1/2500], Loss: 0.0306 Accuracy 0.1321353065539112\n",
      "Epoch [1001/2500], Loss: 0.0017 Accuracy 0.15644820295983086\n",
      "Epoch [2001/2500], Loss: 0.0017 Accuracy 0.15644820295983086\n",
      "Epoch [1/2500], Loss: 0.0333 Accuracy 0.13742071881606766\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.1416490486257928\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.12790697674418605\n",
      "Epoch [1/2500], Loss: 0.0567 Accuracy 0.0010570824524312897\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16807610993657504\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.16807610993657504\n",
      "Epoch [1/2500], Loss: 0.0315 Accuracy 0.19238900634249473\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.17124735729386892\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.16913319238900634\n",
      "Epoch [1/2500], Loss: 0.0401 Accuracy 0.09302325581395349\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1511627906976744\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.14904862579281183\n",
      "Epoch [1/2500], Loss: 0.0437 Accuracy 0.15644820295983086\n",
      "Epoch [1001/2500], Loss: 0.0009 Accuracy 0.13002114164904863\n",
      "Epoch [2001/2500], Loss: 0.0009 Accuracy 0.10782241014799154\n",
      "Epoch [1/2500], Loss: 0.0390 Accuracy 0.16596194503171247\n",
      "Epoch [1001/2500], Loss: 0.0017 Accuracy 0.15644820295983086\n",
      "Epoch [2001/2500], Loss: 0.0017 Accuracy 0.15644820295983086\n",
      "4 0.0\n",
      "Epoch [1/2500], Loss: 0.0253 Accuracy 0.14799154334038056\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1553911205073996\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.14587737843551796\n",
      "Epoch [1/2500], Loss: 0.0414 Accuracy 0.06976744186046512\n",
      "Epoch [1001/2500], Loss: 0.0010 Accuracy 0.1321353065539112\n",
      "Epoch [2001/2500], Loss: 0.0010 Accuracy 0.10570824524312897\n",
      "Epoch [1/2500], Loss: 0.0432 Accuracy 0.035940803382663845\n",
      "Epoch [1001/2500], Loss: 0.0006 Accuracy 0.16701902748414377\n",
      "Epoch [2001/2500], Loss: 0.0006 Accuracy 0.13953488372093023\n",
      "Epoch [1/2500], Loss: 0.0430 Accuracy 0.06448202959830866\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.12790697674418605\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.17230443974630022\n",
      "Epoch [1/2500], Loss: 0.0472 Accuracy 0.0\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.12579281183932348\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.11205073995771671\n",
      "Epoch [1/2500], Loss: 0.0233 Accuracy 0.1109936575052854\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16173361522198731\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.13636363636363635\n",
      "Epoch [1/2500], Loss: 0.0615 Accuracy 0.023255813953488372\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.17019027484143764\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.15327695560253699\n",
      "Epoch [1/2500], Loss: 0.0263 Accuracy 0.2124735729386892\n",
      "Epoch [1001/2500], Loss: 0.0005 Accuracy 0.17124735729386892\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.17547568710359407\n",
      "Epoch [1/2500], Loss: 0.0602 Accuracy 0.1522198731501057\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.14059196617336153\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.13847780126849896\n",
      "Epoch [1/2500], Loss: 0.0296 Accuracy 0.16173361522198731\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.1511627906976744\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.1511627906976744\n",
      "Epoch [1/2500], Loss: 0.0450 Accuracy 0.05496828752642706\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1226215644820296\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.12790697674418605\n",
      "Epoch [1/2500], Loss: 0.0617 Accuracy 0.2642706131078224\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.16913319238900634\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.12896405919661733\n",
      "Epoch [1/2500], Loss: 0.0304 Accuracy 0.15644820295983086\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.12050739957716702\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.13424947145877378\n",
      "Epoch [1/2500], Loss: 0.0280 Accuracy 0.14059196617336153\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.15961945031712474\n",
      "Epoch [2001/2500], Loss: 0.0005 Accuracy 0.15644820295983086\n",
      "Epoch [1/2500], Loss: 0.0303 Accuracy 0.21775898520084566\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.15327695560253699\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.15856236786469344\n",
      "Epoch [1/2500], Loss: 0.0312 Accuracy 0.22832980972515857\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1321353065539112\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.16279069767441862\n",
      "Epoch [1/2500], Loss: 0.0503 Accuracy 0.0010570824524312897\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.17019027484143764\n",
      "Epoch [2001/2500], Loss: 0.0004 Accuracy 0.15961945031712474\n",
      "Epoch [1/2500], Loss: 0.0622 Accuracy 0.0\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.1427061310782241\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.13953488372093023\n",
      "Epoch [1/2500], Loss: 0.0133 Accuracy 0.21035940803382663\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.16701902748414377\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.16596194503171247\n",
      "Epoch [1/2500], Loss: 0.0430 Accuracy 0.25052854122621565\n",
      "Epoch [1001/2500], Loss: 0.0004 Accuracy 0.11839323467230443\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.15644820295983086\n",
      "5 0.0\n",
      "Epoch [1/2500], Loss: 0.0311 Accuracy 0.1321353065539112\n",
      "Epoch [1001/2500], Loss: 0.0003 Accuracy 0.16173361522198731\n",
      "Epoch [2001/2500], Loss: 0.0003 Accuracy 0.1638477801268499\n",
      "Epoch [1/2500], Loss: 0.0505 Accuracy 0.03171247357293869\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.16807610993657504\n",
      "Epoch [2001/2500], Loss: 0.0002 Accuracy 0.1416490486257928\n",
      "Epoch [1/2500], Loss: 0.0312 Accuracy 0.10359408033826638\n",
      "Epoch [1001/2500], Loss: 0.0002 Accuracy 0.13002114164904863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 16\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     l,a\u001b[39m=\u001b[39mtrain(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     average_a[i]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39ma\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m average_a[i]\u001b[39m/\u001b[39m\u001b[39m=\u001b[39mTRIALS \u001b[39m#get average\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 16\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Zero gradients, backward pass, and update the weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss_ar\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRIALS=20\n",
    "TIMES=15\n",
    "average_a=np.zeros((TIMES,2500))\n",
    "\n",
    "for i in range(1,TIMES):\n",
    "    print(i,average_a[-1][-1])\n",
    "    d=dataset()\n",
    "    x,y=d.generate(STORE=i,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]])\n",
    "    SIZE=x.shape[1]\n",
    "    x=x/100\n",
    "    y=y/10\n",
    "    n_inputs = x.shape[1]\n",
    "    m_outputs = len(y[0])\n",
    "    for j in range(TRIALS):\n",
    "        X, data_test, Y, labels_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "        \n",
    "        model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        l,a=train(model)\n",
    "        average_a[i]+=a\n",
    "    average_a[i]/=TRIALS #get average\n",
    "#diplay\n",
    "plt.title(\"Performance of models with different sizes of temporal information\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "\n",
    "\n",
    "for i,dat in enumerate(average_a):\n",
    "    plt.plot(dat,label=\"T=\"+str(i))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "np.save(path+\"data_of_t_size\",average_a)\n",
    "print(np.max(average_a,axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 20\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mAccuricies vs T size\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39;49mmax(average_a,axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mTemporal vector size (T)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2820\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2703\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2704\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2705\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2706\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2707\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2708\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2820\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2821\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "plt.title(\"Accuricies vs T size\")\n",
    "plt.plot(np.max(average_a,axis=1))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Temporal vector size (T)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
