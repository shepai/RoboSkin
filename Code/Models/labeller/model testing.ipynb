{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import letRun #This library can be deleted, it is used for debugging\n",
    "import RoboSkin as sk\n",
    "import cv2 \n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "\n",
    "#if linux\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.version.cuda)\n",
    "print(\"GPU:\",torch.cuda.is_available())\n",
    "\n",
    "path=\"C:/Users/dexte/github/RoboSkin/Code/Models/labeller/\"\n",
    "if os.name == 'nt':\n",
    "    path=\"C:/Users/dexte/github/RoboSkin/Code/Models/labeller/\" #use standard imputer or one for small\n",
    "else:\n",
    "    path=\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/labeller/\" #for linux\n",
    "    \n",
    "def sigmoid(x):                                        \n",
    "   return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA SET CREATOR\n",
    "class dataset:\n",
    "    def __init__(self,names=[\"flat_detection.avi\",\"edge_detection.avi\",\"flat_slip_detection.avi\",\"soft_detection.avi\",\"flat_slip_detection_paper.avi\",\"nothing.avi\"]):\n",
    "        self.path=letRun.path\n",
    "        self.names=names\n",
    "        self.SIZE=0.3\n",
    "        name=\"\"\n",
    "        if os.name == 'nt':\n",
    "            name=\"C:/Users/dexte/OneDrive/Documents/AI/Data_Labeller/pickle_imputer_small.pkl\" #use standard imputer or one for small\n",
    "        else:\n",
    "            name=\"/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/TacTip reader/pickle_imputer_small.pkl\" #for linux\n",
    "        self.reg=None\n",
    "        with open(name,'rb') as file:\n",
    "            self.reg=pickle.load(file)\n",
    "    def predict(self,reg,dat):\n",
    "        p=reg.predict(dat)\n",
    "        p=(p.reshape((p.shape[0],p.shape[1]//2,2))*255/self.SIZE)\n",
    "        return p\n",
    "    def generate(self,STORE=5,y_labels=[],scale=False,gyro_=True):\n",
    "        BIG_DATA_X=None\n",
    "        BIG_DATA_y=None\n",
    "        assert len(y_labels)>=len(self.names),\"Incorrect length of labels\"\n",
    "        for j,name in enumerate(self.names):\n",
    "            skin=sk.Skin(videoFile=self.path+name)#videoFile=path+\"Movement4.avi\") #load skin object using demo video\n",
    "            cap = cv2.VideoCapture(self.path+name)\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            skin.sharpen=False #falsify the sharpness if recorded with sharpness\n",
    "            frame=skin.getFrame()\n",
    "            h=frame.shape[1]*self.SIZE\n",
    "            w=frame.shape[0]*self.SIZE\n",
    "            frame=cv2.resize(frame,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "            past=self.predict(self.reg,np.array([frame]))[0]\n",
    "            initial=past.copy()\n",
    "            initial_frame=frame.copy()\n",
    "            counter=0\n",
    "            X=[]\n",
    "            y=[] #label as [edge surface soft hard slippery]\n",
    "            lastFrames=[]\n",
    "            gyro=np.zeros((length,6))\n",
    "            try:\n",
    "                gyro=np.load(self.path+name.replace(\".avi\",\"\")+\"_gyro.npy\")\n",
    "            except:\n",
    "                pass\n",
    "            for i in range(length): #lop through all\n",
    "                frame_=skin.getFrame()\n",
    "                frame=cv2.resize(frame_,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "                points=self.predict(self.reg,np.array([frame]))[0]\n",
    "                #get pressure map\n",
    "                diff=np.sum(np.abs(initial_frame-frame))/(frame.shape[0]*3)\n",
    "                vecs=initial-points\n",
    "                if gyro_:\n",
    "                    lastFrames.append(np.concatenate((vecs.flatten(),gyro[i]*1)))\n",
    "                else:\n",
    "                    lastFrames.append(vecs.flatten())\n",
    "                if len(lastFrames)>STORE: lastFrames.pop(0)\n",
    "                if diff>0.01 and len(lastFrames)==STORE: #significant contact\n",
    "                    X.append(np.array(lastFrames).flatten()) #store temporal element\n",
    "                    y.append(y_labels[j])\n",
    "                    counter+=1\n",
    "\n",
    "            if type(BIG_DATA_y)==type(None):\n",
    "                BIG_DATA_y=np.array(y.copy())\n",
    "                BIG_DATA_X=np.array(X.copy())\n",
    "            else:\n",
    "                BIG_DATA_y= np.concatenate((np.array(y.copy()),BIG_DATA_y))\n",
    "                BIG_DATA_X= np.concatenate((np.array(X.copy()),BIG_DATA_X))\n",
    "            print(name,\"Length:\",counter)\n",
    "        \n",
    "        a,b=np.array(BIG_DATA_X),np.array(BIG_DATA_y)\n",
    "        if scale:\n",
    "            \"\"\"scaler1 = preprocessing.StandardScaler().fit(a)\n",
    "            a = scaler1.transform(a)\"\"\"\n",
    "            a=(a-np.average(a))/np.std(a)\n",
    "        return a,b\n",
    "    def gen_image_data(self,STORE=5,y_labels=[],scale=False):\n",
    "        BIG_DATA_X=None\n",
    "        BIG_DATA_y=None\n",
    "        assert len(y_labels)>=len(self.names),\"Incorrect length of labels\"\n",
    "        for j,name in enumerate(self.names):\n",
    "            skin=sk.Skin(videoFile=self.path+name)#videoFile=path+\"Movement4.avi\") #load skin object using demo video\n",
    "            cap = cv2.VideoCapture(self.path+name)\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            skin.sharpen=False #falsify the sharpness if recorded with sharpness\n",
    "            frame=skin.getFrame()\n",
    "            h=frame.shape[1]*self.SIZE\n",
    "            w=frame.shape[0]*self.SIZE\n",
    "            frame=cv2.resize(frame,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "            \n",
    "            initial_frame=frame.copy()\n",
    "            counter=0\n",
    "            X=[]\n",
    "            y=[] #label as [edge surface soft hard slippery]\n",
    "            lastFrames=[]\n",
    "            gyro=np.zeros((length,6))\n",
    "            try:\n",
    "                gyro=np.load(self.path+name.replace(\".avi\",\"\")+\"_gyro.npy\")\n",
    "            except:\n",
    "                pass\n",
    "            for i in range(length): #lop through all\n",
    "                frame_=skin.getFrame()\n",
    "                frame=cv2.resize(frame_,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                #get pressure map\n",
    "                diff=np.sum(np.abs(initial_frame-frame.flatten()))/(frame.flatten().shape[0]*3)\n",
    "                frame = np.uint8(frame)\n",
    "                #current = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #\n",
    "                frame = cv2.adaptiveThreshold(\n",
    "                        frame, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 5, 1\n",
    "                    )\n",
    "                kernel = np.ones((2, 2), np.uint8)\n",
    "                frame = cv2.erode(frame, kernel, iterations=1)\n",
    "                lastFrames.append(frame/255)\n",
    "                if len(lastFrames)>STORE: lastFrames.pop(0)\n",
    "                if diff>0.01 and len(lastFrames)==STORE: #significant contact\n",
    "                    #all_frames=np.zeros(())\n",
    "                    X.append(np.array(lastFrames).reshape((int(w)*STORE,int(h)))) #store temporal element\n",
    "                    y.append(y_labels[j])\n",
    "                    counter+=1\n",
    "\n",
    "            if type(BIG_DATA_y)==type(None):\n",
    "                BIG_DATA_y=np.array(y.copy())\n",
    "                BIG_DATA_X=np.array(X.copy())\n",
    "            else:\n",
    "                BIG_DATA_y= np.concatenate((np.array(y.copy()),BIG_DATA_y))\n",
    "                BIG_DATA_X= np.concatenate((np.array(X.copy()),BIG_DATA_X))\n",
    "            print(name,\"Length:\",counter,\"/\",str(length),\"X-size:\",BIG_DATA_X.shape)\n",
    "        print(\"Completed creation\",np.average(BIG_DATA_X),np.std(BIG_DATA_X))\n",
    "        if scale:\n",
    "            \"\"\"scaler1 = preprocessing.StandardScaler().fit(a)\n",
    "            a = scaler1.transform(a)\"\"\"\n",
    "            BIG_DATA_X=(BIG_DATA_X-np.average(BIG_DATA_X))/np.std(BIG_DATA_X)\n",
    "        return BIG_DATA_X,BIG_DATA_y\n",
    "    def generate_average(self,STORE=5,y_labels=[],scale=False):\n",
    "        BIG_DATA_X=None\n",
    "        BIG_DATA_y=None\n",
    "        assert len(y_labels)>=len(self.names),\"Incorrect length of labels\"\n",
    "        for j,name in enumerate(self.names):\n",
    "            skin=sk.Skin(videoFile=self.path+name)#videoFile=path+\"Movement4.avi\") #load skin object using demo video\n",
    "            cap = cv2.VideoCapture(self.path+name)\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            skin.sharpen=False #falsify the sharpness if recorded with sharpness\n",
    "            frame=skin.getFrame()\n",
    "            h=frame.shape[1]*self.SIZE\n",
    "            w=frame.shape[0]*self.SIZE\n",
    "            frame=cv2.resize(frame,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "            past=self.predict(self.reg,np.array([frame]))[0]\n",
    "            initial=past.copy()\n",
    "            initial_frame=frame.copy()\n",
    "\n",
    "            X=[]\n",
    "            y=[] #label as [edge surface soft hard slippery]\n",
    "            lastFrames=[]\n",
    "            gyro=np.zeros((length,6))\n",
    "            try:\n",
    "                gyro=np.load(self.path+name.replace(\".avi\",\"\")+\"_gyro.npy\")\n",
    "            except:\n",
    "                pass\n",
    "            counter=0\n",
    "            for i in range(length): #lop through all\n",
    "                frame_=skin.getFrame()\n",
    "                frame=cv2.resize(frame_,(int(h),int(w)),interpolation=cv2.INTER_AREA)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).flatten()/255\n",
    "                points=self.predict(self.reg,np.array([frame]))[0]\n",
    "                #get pressure map\n",
    "                diff=np.sum(np.abs(initial_frame-frame))/(frame.shape[0]*3)\n",
    "                vecs=initial-points\n",
    "                average=np.average(vecs,axis=0).flatten()\n",
    "                lastFrames.append(np.concatenate((average,gyro[i]/1)))\n",
    "                if len(lastFrames)>STORE: lastFrames.pop(0)\n",
    "                if diff>0.01 and len(lastFrames)==STORE: #significant contact\n",
    "                    X.append(np.array(lastFrames).flatten()) #store temporal element\n",
    "                    y.append(y_labels[j])\n",
    "                    counter+=1\n",
    "            if type(BIG_DATA_y)==type(None):\n",
    "                BIG_DATA_y=np.array(y.copy())\n",
    "                BIG_DATA_X=np.array(X.copy())\n",
    "            else:\n",
    "                BIG_DATA_y= np.concatenate((np.array(y.copy()),BIG_DATA_y))\n",
    "                BIG_DATA_X= np.concatenate((np.array(X.copy()),BIG_DATA_X))\n",
    "            print(name,\"Length:\",counter)\n",
    "        a,b=np.array(BIG_DATA_X),np.array(BIG_DATA_y)\n",
    "        #a=a.reshape((a.shape))\n",
    "        #regulise\n",
    "        if scale:\n",
    "            scaler1 = preprocessing.StandardScaler().fit(a)\n",
    "            a = scaler1.transform(a)\n",
    "        return a,b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 685 / 689 X-size: (685, 720, 192)\n",
      "edge_detection.avi Length: 728 / 732 X-size: (1413, 720, 192)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d=dataset()\n",
    "d.SIZE=0.3\n",
    "X,ya=d.gen_image_data(STORE=5,y_labels=[[1,0,0,0],[1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],scale=True)\n",
    "#x,y =d.generate(STORE=5,y_labels=[[1,0,0],[1,0,0],[0,0,1]],scale=True)\n",
    "print(np.sum(X))\n",
    "#Xa=X/10\n",
    "ya=ya/10\n",
    "#y=y/10\n",
    "X, data_test, Y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "#X, data_test, Y, labels_test = train_test_split(x, y , test_size=0.20, random_state=42)\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "print(data_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size,layers=[5000,500,50],drop_out_prob=0.2):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        self.fc=[nn.Linear(input_size, layers[0])]\n",
    "        self.fc.append(nn.Sigmoid())\n",
    "        self.fc.append(nn.Dropout(p=drop_out_prob))\n",
    "        for i in range(1,len(layers)): #create layers \n",
    "                self.fc.append(nn.Linear(layers[i-1], layers[i]))\n",
    "                self.fc.append(nn.Sigmoid())\n",
    "                self.fc.append(nn.Dropout(p=drop_out_prob))\n",
    "        self.fc.append(nn.Linear(layers[-1], output_size))\n",
    "        self.fc_layers = nn.Sequential(*self.fc)\n",
    "    def forward(self, x):\n",
    "        x=self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleConv2DNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size,layers=[1000,500,200],drop_out_prob=0.2):\n",
    "        super(SimpleConv2DNeuralNetwork, self).__init__()\n",
    "        input_channels=1 #greyscale\n",
    "        self.conv_layer = nn.Conv2d(in_channels=input_channels, out_channels=8, kernel_size=3)\n",
    "        # Add 2D convolutional layer\n",
    "        self.conv_layer = nn.Conv2d(in_channels=input_channels, out_channels=8, kernel_size=3)\n",
    "        \n",
    "        # Calculate the size of the tensor after convolution and pooling\n",
    "        conv_output_size = self._get_conv_output_size(input_channels, input_size[0], input_size[1])  # Adjust the last two dimensions\n",
    "        # Add hidden layers\n",
    "        self.fc=[nn.Linear(int(conv_output_size), layers[0])]\n",
    "        self.fc.append(nn.Sigmoid())\n",
    "        self.fc.append(nn.Dropout(p=drop_out_prob))\n",
    "        for i in range(1,len(layers)): #create layers \n",
    "                self.fc.append(nn.Linear(layers[i-1], layers[i]))\n",
    "                self.fc.append(nn.Sigmoid())\n",
    "                self.fc.append(nn.Dropout(p=drop_out_prob))\n",
    "        self.fc.append(nn.Linear(layers[-1], output_size))\n",
    "        self.fc_layers = nn.Sequential(*self.fc)\n",
    "    def forward(self, x):\n",
    "        x=self.conv_layer(x)\n",
    "        x = torch.relu(x)\n",
    "        # Reshape the tensor to match the input size of the first fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x=self.fc_layers(x)\n",
    "        return x\n",
    "    def _get_conv_output_size(self, input_channels, height, width):\n",
    "        dummy_input = torch.zeros(1, input_channels, height, width)\n",
    "        dummy_output = self.conv_layer(dummy_input)\n",
    "        return dummy_output.view(-1).size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2720, 720, 192)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.06 GiB (GPU 0; 16.00 GiB total capacity; 21.99 GiB already allocated; 0 bytes free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 7\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss\u001b[39m=\u001b[39mtrain(model,\u001b[39m150000\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(loss,label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#plt.plot(b,label=\"Accuracy\")\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss_ar\u001b[39m=\u001b[39m[]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(X_tensor\u001b[39m.\u001b[39;49mfloat())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred, y_tensor)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 7\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layer(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Reshape the tensor to match the input size of the first fully connected layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X55sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.06 GiB (GPU 0; 16.00 GiB total capacity; 21.99 GiB already allocated; 0 bytes free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "\n",
    "# Define the size of the input (n) and output (m) layers\n",
    "n_inputs = X.shape[1]\n",
    "m_outputs = Y.shape[1]\n",
    "print(X.shape)\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X).view(X.shape[0],1,X.shape[1],X.shape[2]).to(device).to(torch.float32)\n",
    "y_tensor = torch.tensor(Y).to(device).to(torch.float32)\n",
    "\n",
    "# Define the neural network model\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleConv2DNeuralNetwork([720, 192], m_outputs,layers=[1000],drop_out_prob=0.1).to(device)\n",
    "#model=Network().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train(model,num_epochs,output=True):\n",
    "    loss_ar=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_tensor.float())\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "        # Zero gradients, backward pass, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ar.append(loss.item())\n",
    "        #predict\n",
    "        # Print the current loss to monitor training progress\n",
    "        if epoch%500==0 and output:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    return np.array(loss_ar)\n",
    "plt.title(\"Loss while training marker prediction model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "loss=train(model,150000)\n",
    "plt.plot(loss,label=\"Loss\")\n",
    "#plt.plot(b,label=\"Accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the size of the input (n) and output (m) layers\n",
    "n_inputs = X.shape[1]\n",
    "m_outputs = len(Y[0])\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNeuralNetwork(n_inputs, m_outputs,drop_out_prob=0.1).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()#nn.MSELoss() #nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def get_acc(predictions,should_be=[]):\n",
    "    predictions*=10 #normalize\n",
    "    pred_array=np.zeros_like(predictions)\n",
    "\n",
    "    if len(predictions[0])>4:\n",
    "        inds=np.argmax(predictions[:,0:2],axis=1) #convert at threshold\n",
    "        inds2=np.argmax(predictions[:,2:-1],axis=1) #convert at threshold\n",
    "        for i in range(len(pred_array)):\n",
    "            pred_array[i][inds[i]]=1\n",
    "            pred_array[i][2+inds2[i]]=1\n",
    "    else:\n",
    "        inds=np.argmax(predictions,axis=1) #convert at threshold\n",
    "        for i in range(len(pred_array)):\n",
    "            pred_array[i][inds[i]]=1\n",
    "    correct=0\n",
    "    for j,pred in enumerate(pred_array): #loopthrough predictions\n",
    "        inds=np.where(pred==1) #\n",
    "        if len(should_be)>0: #validation task\n",
    "            sb=should_be[j]*10\n",
    "            sb=np.where(sb==1)\n",
    "            if len(sb[0])==len(inds[0]):\n",
    "                c=0\n",
    "                for i in range(len(sb[0])):\n",
    "                    if sb[0][i]==inds[0][i]: c+=1\n",
    "                if c==len(sb[0]): correct+=1\n",
    "    return correct/len(should_be)\n",
    "\n",
    "def train(model,num_epochs=2500,output=True):\n",
    "    loss_ar=[]\n",
    "    accuracies=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_tensor)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "\n",
    "        # Zero gradients, backward pass, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ar.append(loss.item())\n",
    "        #predict\n",
    "        predictions = model(torch.tensor(X, dtype=torch.float32).to(device))\n",
    "        accuracies.append(get_acc(predictions.cpu().detach().numpy(),should_be=Y))\n",
    "        # Print the current loss to monitor training progress\n",
    "        if epoch%1000==0 and output:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\",\"Accuracy\",accuracies[-1])\n",
    "    return np.array(loss_ar),np.array(accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.58 GiB (GPU 0; 16.00 GiB total capacity; 27.27 GiB already allocated; 0 bytes free; 28.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 8\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m a,b\u001b[39m=\u001b[39mtrain(model,\u001b[39m15000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#plt.plot(a,label=\"Loss\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(b,label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\dexte\\github\\RoboSkin\\Code\\Models\\labeller\\model testing.ipynb Cell 8\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss_ar\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dexte/github/RoboSkin/Code/Models/labeller/model%20testing.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#predict\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\dexte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:507\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    505\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    506\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m--> 507\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_add(exp_avg_sq_sqrt, eps)\n\u001b[0;32m    509\u001b[0m torch\u001b[39m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom, step_size)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.58 GiB (GPU 0; 16.00 GiB total capacity; 27.27 GiB already allocated; 0 bytes free; 28.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "plt.title(\"Loss while training tactile model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "a,b=train(model,15000)\n",
    "#plt.plot(a,label=\"Loss\")\n",
    "plt.plot(b,label=\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/its/home/drs25/Documents/GitHub/RoboSkin/Code/Models/surfaceModel/state/model1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct: 41.07142857142857 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge'],\n",
       " ['edge']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training, you can use the trained model for predictions on new data.\n",
    "# For example, if you have new input data 'X_new', you can do:\n",
    "\n",
    "\n",
    "def predict(predictions,should_be=[]):\n",
    "    predictions*=10 #normalize\n",
    "    pred_array=np.zeros_like(predictions)\n",
    "    if len(predictions[0])>4:\n",
    "        inds=np.argmax(predictions[:,0:2],axis=1) #convert at threshold\n",
    "        inds2=np.argmax(predictions[:,2:-1],axis=1) #convert at threshold\n",
    "        for i in range(len(pred_array)):\n",
    "            pred_array[i][inds[i]]=1\n",
    "            pred_array[i][2+inds2[i]]=1\n",
    "    else:\n",
    "        inds=np.argmax(predictions,axis=1) #convert at threshold\n",
    "        for i in range(len(pred_array)):\n",
    "            pred_array[i][inds[i]]=1\n",
    "    names=[\"edge\", \"surface\", \"soft\", \"hard\", \"slippery\"]\n",
    "    array=[]\n",
    "    correct=0\n",
    "    for j,pred in enumerate(pred_array): #loopthrough predictions\n",
    "        inds=np.where(pred==1) #\n",
    "        ar=[]\n",
    "        if len(should_be)>0: #validation task\n",
    "            sb=should_be[j]*10\n",
    "            sb=np.where(sb==1)\n",
    "            if len(sb[0])==len(inds[0]):\n",
    "                c=0\n",
    "                for i in range(len(sb[0])):\n",
    "                    if sb[0][i]==inds[0][i]: c+=1\n",
    "                if c==len(sb[0]): correct+=1\n",
    "        for i in inds[0]:\n",
    "            ar.append(names[i])\n",
    "        array.append(ar)\n",
    "    if correct!=0: print(\"Percentage correct:\",(correct/len(should_be))*100,\"%\")\n",
    "    return array\n",
    "\n",
    "with torch.no_grad():\n",
    "     model.eval()\n",
    "     predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "\n",
    "p=predict(predictions.cpu(),should_be=labels_test)\n",
    "p[0:20]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "### With and without slippage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m#without slippage\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#merge datasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m d\u001b[39m=\u001b[39mdataset(names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mflat_detection.avi\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39medge_detection.avi\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m X,ya\u001b[39m=\u001b[39md\u001b[39m.\u001b[39;49mgenerate(STORE\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,y_labels\u001b[39m=\u001b[39;49m[[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m]])\n\u001b[1;32m      6\u001b[0m Xa\u001b[39m=\u001b[39mX\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m      7\u001b[0m ya\u001b[39m=\u001b[39mya\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 46\u001b[0m, in \u001b[0;36mdataset.generate\u001b[0;34m(self, STORE, y_labels, scale)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length): \u001b[39m#lop through all\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     frame_\u001b[39m=\u001b[39mskin\u001b[39m.\u001b[39;49mgetFrame()\n\u001b[1;32m     47\u001b[0m     frame\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mresize(frame_,(\u001b[39mint\u001b[39m(h),\u001b[39mint\u001b[39m(w)),interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_AREA)\n\u001b[1;32m     48\u001b[0m     frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\u001b[39m.\u001b[39mflatten()\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/RoboSkin/Code/RoboSkin/__init__.py:113\u001b[0m, in \u001b[0;36mSkin.getFrame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# Applying CLAHE to L-channel\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# feel free to try different values for the limit and grid size:\u001b[39;00m\n\u001b[1;32m    112\u001b[0m clahe \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcreateCLAHE(clipLimit\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m, tileGridSize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m8\u001b[39m))\n\u001b[0;32m--> 113\u001b[0m cl \u001b[39m=\u001b[39m clahe\u001b[39m.\u001b[39;49mapply(l_channel)\n\u001b[1;32m    115\u001b[0m \u001b[39m# merge the CLAHE enhanced L-channel with the a and b channel\u001b[39;00m\n\u001b[1;32m    116\u001b[0m limg \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mmerge((cl,a,b))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRIALS=100\n",
    "#without slippage\n",
    "#merge datasets\n",
    "d=dataset(names=[\"flat_detection.avi\",\"edge_detection.avi\"])\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0]])\n",
    "Xa=X/100\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(Xa, ya, test_size=0.20, random_state=42)\n",
    "\n",
    "average_a=np.zeros((2500,))\n",
    "p_=0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial B\",i,\"Acc:\",p_/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model)\n",
    "    average_a+=np.array(a)\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p_+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "\n",
    "average_a=average_a/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "p_/=TRIALS\n",
    "\n",
    "average_a1=np.zeros((2500,))\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]])\n",
    "Xa=X/100\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(Xa, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model)\n",
    "    average_a1+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a1=average_a1/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "p/=TRIALS\n",
    "#plt.plot(average_l,'--',c=\"b\",label=\"Loss with slip data\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"Accuracy with slip data\")\n",
    "\n",
    "\n",
    "print(\"Accuracies\",p,p_)\n",
    "plt.title(\"Performance of models with and without slippage over 100 trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a,c=\"r\",label=\"Accuracy without slip data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(np.std(average_a),np.std(average_a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path+\"BignetNoSlip\",average_a)\n",
    "np.save(path+\"Bignet\",average_a1)\n",
    "\n",
    "\n",
    "ar1=np.load(path+\"40netNoSlip.npy\")\n",
    "ar2=np.load(path+\"40net.npy\")\n",
    "plt.title(\"Performance of models with and without slippage over 100 trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(ar1,c=\"g\",label=\"Accuracy compressed input without slip data\")\n",
    "plt.plot(ar2,c=\"y\",label=\"Accuracy compressed input\")\n",
    "plt.plot(average_a,c=\"r\",label=\"Accuracy without slip data\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With and without gyroscopic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 683\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "soft_detection.avi Length: 238\n",
      "flat_slip_detection_paper.avi Length: 630\n",
      "nothing.avi Length: 510\n",
      "A 0.39716981132075474\n",
      "A 0.39528301886792455\n",
      "A 0.3845911949685535\n",
      "A 0.3824292452830188\n",
      "A 0.38122641509433963\n",
      "A 0.3810534591194969\n",
      "A 0.3789757412398922\n",
      "A 0.37841981132075475\n",
      "A 0.37777777777777777\n",
      "A 0.3774056603773585\n",
      "A 0.37838765008576336\n",
      "A 0.37716194968553457\n",
      "A 0.37681422351233673\n",
      "A 0.37665094339622635\n",
      "A 0.3759119496855346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 683\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "soft_detection.avi Length: 238\n",
      "flat_slip_detection_paper.avi Length: 630\n",
      "nothing.avi Length: 510\n",
      "B 0.3650943396226415\n",
      "B 0.36721698113207546\n",
      "B 0.37028301886792453\n",
      "B 0.37016509433962264\n",
      "B 0.3692452830188679\n",
      "B 0.370440251572327\n",
      "B 0.37028301886792453\n",
      "B 0.3722877358490566\n",
      "B 0.37180293501048217\n",
      "B 0.3715566037735849\n",
      "B 0.37259862778730707\n",
      "B 0.37358490566037733\n",
      "B 0.3740203193033382\n",
      "B 0.37402291105121294\n",
      "B 0.37323899371069186\n"
     ]
    }
   ],
   "source": [
    "num_epochs=120000\n",
    "gyro=np.zeros((num_epochs,))\n",
    "non_gyro=np.zeros((num_epochs,))\n",
    "TRIALS=15\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[1,0,0,0],[1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],scale=True)\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, Y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "n_inputs=X.shape[1]\n",
    "m_outputs=Y.shape[1]\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "for i in range(TRIALS):\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs,layers=[100,50,10]).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs,output=False)\n",
    "    \n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    gyro+=a#get_acc(predictions.cpu(),should_be=labels_test)\n",
    "    print(\"A\",np.max(gyro)/(i+1))\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[1,0,0,0],[1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],scale=True,gyro_=False)\n",
    "n_inputs=X.shape[1]\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, Y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "for i in range(TRIALS):\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs,layers=[100,50,10]).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs,output=False)\n",
    "    \n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    non_gyro+=a#get_acc(predictions.cpu(),should_be=labels_test)\n",
    "    print(\"B\",non_gyro[-1]/(i+1))\n",
    "\n",
    "\n",
    "non_gyro/=TRIALS\n",
    "gyro/=TRIALS\n",
    "\n",
    "plt.title(\"Accuracy training tactile model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "#plt.plot(a,label=\"Loss\")\n",
    "plt.plot(gyro,label=\"Accuracy of gyro\")\n",
    "plt.plot(non_gyro,label=\"Accuracy of non-gyro\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3759119496855346, 0.380188679245283)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(gyro),np.max(non_gyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy training tactile model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "#plt.plot(a,label=\"Loss\")\n",
    "plt.plot(gyro,label=\"Accuracy of gyro\")\n",
    "plt.plot(non_gyro,label=\"Accuracy of non-gyro\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial A 0 Acc: 0.7710752688172042\n",
      "Epoch [1/3500], Loss: 0.0696 Accuracy 0.3468286099865047\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 1 Acc: 0.3855376344086021\n",
      "Epoch [1/3500], Loss: 0.0282 Accuracy 0.029689608636977057\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 2 Acc: 0.2570250896057347\n",
      "Epoch [1/3500], Loss: 0.0795 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 3 Acc: 0.19276881720430106\n",
      "Epoch [1/3500], Loss: 0.1012 Accuracy 0.4008097165991903\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 4 Acc: 0.15421505376344086\n",
      "Epoch [1/3500], Loss: 0.1673 Accuracy 0.18353576248313092\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 5 Acc: 0.12851254480286736\n",
      "Epoch [1/3500], Loss: 0.0335 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 6 Acc: 0.11015360983102918\n",
      "Epoch [1/3500], Loss: 0.0609 Accuracy 0.005398110661268556\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 7 Acc: 0.09638440860215053\n",
      "Epoch [1/3500], Loss: 0.1116 Accuracy 0.6558704453441295\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 8 Acc: 0.08567502986857825\n",
      "Epoch [1/3500], Loss: 0.2311 Accuracy 0.6275303643724697\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9932523616734144\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 9 Acc: 0.07710752688172043\n",
      "Epoch [1/3500], Loss: 0.2377 Accuracy 0.5182186234817814\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9878542510121457\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Trial A 10 Acc: 0.07009775171065492\n",
      "Epoch [1/3500], Loss: 0.0881 Accuracy 0.252361673414305\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 0.9986504723346828\n",
      "Trial A 11 Acc: 0.06425627240143368\n",
      "Epoch [1/3500], Loss: 0.1179 Accuracy 0.3441295546558704\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 12 Acc: 0.05931348221670802\n",
      "Epoch [1/3500], Loss: 0.0338 Accuracy 0.014844804318488529\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 13 Acc: 0.05507680491551459\n",
      "Epoch [1/3500], Loss: 0.0554 Accuracy 0.34143049932523617\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 14 Acc: 0.05140501792114695\n",
      "Epoch [1/3500], Loss: 0.1884 Accuracy 0.3265856950067476\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 15 Acc: 0.048192204301075264\n",
      "Epoch [1/3500], Loss: 0.1612 Accuracy 0.13765182186234817\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 16 Acc: 0.04535736875395319\n",
      "Epoch [1/3500], Loss: 0.2903 Accuracy 0.3076923076923077\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 17 Acc: 0.042837514934289125\n",
      "Epoch [1/3500], Loss: 0.1228 Accuracy 0.16869095816464239\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 18 Acc: 0.04058290888511601\n",
      "Epoch [1/3500], Loss: 0.0308 Accuracy 0.3441295546558704\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 19 Acc: 0.038553763440860214\n",
      "Epoch [1/3500], Loss: 0.0415 Accuracy 0.5033738191632928\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 20 Acc: 0.036717869943676394\n",
      "Epoch [1/3500], Loss: 0.0777 Accuracy 0.650472334682861\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 21 Acc: 0.03504887585532746\n",
      "Epoch [1/3500], Loss: 0.1092 Accuracy 0.15114709851551958\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 22 Acc: 0.03352501168770453\n",
      "Epoch [1/3500], Loss: 0.1006 Accuracy 0.26180836707152494\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 23 Acc: 0.03212813620071684\n",
      "Epoch [1/3500], Loss: 0.0643 Accuracy 0.2739541160593792\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 24 Acc: 0.03084301075268817\n",
      "Epoch [1/3500], Loss: 0.0584 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 25 Acc: 0.02965674110835401\n",
      "Epoch [1/3500], Loss: 0.0729 Accuracy 0.34143049932523617\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9959514170040485\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 26 Acc: 0.028558343289526084\n",
      "Epoch [1/3500], Loss: 0.1141 Accuracy 0.6518218623481782\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 27 Acc: 0.027538402457757294\n",
      "Epoch [1/3500], Loss: 0.0543 Accuracy 0.6558704453441295\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 28 Acc: 0.026588802373007044\n",
      "Epoch [1/3500], Loss: 0.2136 Accuracy 0.3765182186234818\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 29 Acc: 0.025702508960573474\n",
      "Epoch [1/3500], Loss: 0.1246 Accuracy 0.5101214574898786\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 30 Acc: 0.02487339576829691\n",
      "Epoch [1/3500], Loss: 0.3473 Accuracy 0.50472334682861\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9919028340080972\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Trial A 31 Acc: 0.024096102150537632\n",
      "Epoch [1/3500], Loss: 0.0609 Accuracy 0.05668016194331984\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 32 Acc: 0.023365917236884977\n",
      "Epoch [1/3500], Loss: 0.3770 Accuracy 0.3508771929824561\n",
      "Epoch [1001/3500], Loss: 0.0006 Accuracy 0.9392712550607287\n",
      "Epoch [2001/3500], Loss: 0.0005 Accuracy 0.9298245614035088\n",
      "Epoch [3001/3500], Loss: 0.0002 Accuracy 0.9892037786774629\n",
      "Trial A 33 Acc: 0.022678684376976593\n",
      "Epoch [1/3500], Loss: 0.0618 Accuracy 0.6558704453441295\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 34 Acc: 0.022030721966205834\n",
      "Epoch [1/3500], Loss: 0.0485 Accuracy 0.38596491228070173\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 35 Acc: 0.021418757467144563\n",
      "Epoch [1/3500], Loss: 0.1233 Accuracy 0.6072874493927125\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 36 Acc: 0.020839872130194708\n",
      "Epoch [1/3500], Loss: 0.2243 Accuracy 0.6356275303643725\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 37 Acc: 0.020291454442558005\n",
      "Epoch [1/3500], Loss: 0.1387 Accuracy 0.6437246963562753\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 38 Acc: 0.01977116073890267\n",
      "Epoch [1/3500], Loss: 0.1052 Accuracy 0.19838056680161945\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 39 Acc: 0.019276881720430107\n",
      "Epoch [1/3500], Loss: 0.0374 Accuracy 0.13090418353576247\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 40 Acc: 0.01880671387359035\n",
      "Epoch [1/3500], Loss: 0.1726 Accuracy 0.01349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 41 Acc: 0.018358934971838197\n",
      "Epoch [1/3500], Loss: 0.2248 Accuracy 0.3657219973009447\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 42 Acc: 0.017931982995748937\n",
      "Epoch [1/3500], Loss: 0.1402 Accuracy 0.2145748987854251\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9959514170040485\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 43 Acc: 0.01752443792766373\n",
      "Epoch [1/3500], Loss: 0.0278 Accuracy 0.3603238866396761\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 44 Acc: 0.017135005973715648\n",
      "Epoch [1/3500], Loss: 0.0590 Accuracy 0.3441295546558704\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 45 Acc: 0.016762505843852266\n",
      "Epoch [1/3500], Loss: 0.1059 Accuracy 0.09716599190283401\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 46 Acc: 0.01640585678334477\n",
      "Epoch [1/3500], Loss: 0.1846 Accuracy 0.4048582995951417\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 47 Acc: 0.01606406810035842\n",
      "Epoch [1/3500], Loss: 0.0416 Accuracy 0.10526315789473684\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 48 Acc: 0.01573622997586131\n",
      "Epoch [1/3500], Loss: 0.1040 Accuracy 0.11875843454790823\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 49 Acc: 0.015421505376344085\n",
      "Epoch [1/3500], Loss: 0.1195 Accuracy 0.2941970310391363\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "0.019412615544898787 0.06210372653582172\n"
     ]
    }
   ],
   "source": [
    "TRIALS=50\n",
    "num_epochs=3500\n",
    "average_a1=np.zeros((num_epochs,))\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate_average(STORE=5,y_labels=[[0,1,0],[0,1,0],[0,0,1]],scale=True)\n",
    "ya=ya/10\n",
    "n_inputs=X.shape[1]\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "m_outputs=3\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs,layers=[100,50,10]).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs)\n",
    "    average_a1+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a1=average_a1/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "p/=TRIALS\n",
    "#plt.plot(average_l,'--',c=\"b\",label=\"Loss with slip data\")\n",
    "\n",
    "average_a2=np.zeros((num_epochs,))\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0],[0,1,0],[0,0,1]],scale=True)\n",
    "ya=ya/10\n",
    "n_inputs=X.shape[1]\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "m_outputs=3\n",
    "p_=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs)\n",
    "    average_a2+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p_+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a2=average_a2/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "p_/=TRIALS\n",
    "#plt.plot(average_l,'--',c=\"b\",label=\"Loss with slip data\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"With compression\")\n",
    "\n",
    "\n",
    "plt.title(\"Performance of models with and without slippage over 50 trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a2,c=\"r\",label=\"Without compression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(np.std(average_a),np.std(average_a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Performance of models with and without slippage over 50 trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"With compression\")\n",
    "plt.plot(average_a2,c=\"r\",label=\"Without compression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial A 0 Acc: 0.0\n",
      "Epoch [1/3500], Loss: 0.1813 Accuracy 0.3643724696356275\n",
      "Epoch [1001/3500], Loss: 0.0120 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0077 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0055 Accuracy 0.26450742240215924\n",
      "Trial A 1 Acc: 0.17204301075268819\n",
      "Epoch [1/3500], Loss: 0.0760 Accuracy 0.3252361673414305\n",
      "Epoch [1001/3500], Loss: 0.0130 Accuracy 0.19973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.242914979757085\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.28475033738191635\n",
      "Trial A 2 Acc: 0.22939068100358426\n",
      "Epoch [1/3500], Loss: 0.1676 Accuracy 0.17813765182186234\n",
      "Epoch [1001/3500], Loss: 0.0132 Accuracy 0.2321187584345479\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.22941970310391363\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.24561403508771928\n",
      "Trial A 3 Acc: 0.25806451612903225\n",
      "Epoch [1/3500], Loss: 0.0617 Accuracy 0.17408906882591094\n",
      "Epoch [1001/3500], Loss: 0.0142 Accuracy 0.21592442645074225\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.2564102564102564\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.24021592442645073\n",
      "Trial A 4 Acc: 0.2752688172043011\n",
      "Epoch [1/3500], Loss: 0.0408 Accuracy 0.22402159244264508\n",
      "Epoch [1001/3500], Loss: 0.0142 Accuracy 0.22807017543859648\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.26855600539811064\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2564102564102564\n",
      "Trial A 5 Acc: 0.28673835125448033\n",
      "Epoch [1/3500], Loss: 0.1248 Accuracy 0.0728744939271255\n",
      "Epoch [1001/3500], Loss: 0.0121 Accuracy 0.242914979757085\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.26450742240215924\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.26720647773279355\n",
      "Trial A 6 Acc: 0.2949308755760369\n",
      "Epoch [1/3500], Loss: 0.1078 Accuracy 0.2874493927125506\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.21592442645074225\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.26045883940620784\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2699055330634278\n",
      "Trial A 7 Acc: 0.30107526881720437\n",
      "Epoch [1/3500], Loss: 0.0360 Accuracy 0.2699055330634278\n",
      "Epoch [1001/3500], Loss: 0.0140 Accuracy 0.19838056680161945\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.20647773279352227\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2550607287449393\n",
      "Trial A 8 Acc: 0.3058542413381124\n",
      "Epoch [1/3500], Loss: 0.0603 Accuracy 0.08367071524966262\n",
      "Epoch [1001/3500], Loss: 0.0132 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.23616734143049933\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.25371120107962214\n",
      "Trial A 9 Acc: 0.3096774193548388\n",
      "Epoch [1/3500], Loss: 0.1226 Accuracy 0.002699055330634278\n",
      "Epoch [1001/3500], Loss: 0.0137 Accuracy 0.252361673414305\n",
      "Epoch [2001/3500], Loss: 0.0090 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.27125506072874495\n",
      "Trial A 10 Acc: 0.3128054740957968\n",
      "Epoch [1/3500], Loss: 0.0752 Accuracy 0.3090418353576248\n",
      "Epoch [1001/3500], Loss: 0.0139 Accuracy 0.21052631578947367\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.252361673414305\n",
      "Trial A 11 Acc: 0.3154121863799284\n",
      "Epoch [1/3500], Loss: 0.0312 Accuracy 0.2726045883940621\n",
      "Epoch [1001/3500], Loss: 0.0120 Accuracy 0.23751686909581646\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.23616734143049933\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.25101214574898784\n",
      "Trial A 12 Acc: 0.31761786600496283\n",
      "Epoch [1/3500], Loss: 0.0599 Accuracy 0.3468286099865047\n",
      "Epoch [1001/3500], Loss: 0.0120 Accuracy 0.23751686909581646\n",
      "Epoch [2001/3500], Loss: 0.0078 Accuracy 0.2550607287449393\n",
      "Epoch [3001/3500], Loss: 0.0055 Accuracy 0.27530364372469635\n",
      "Trial A 13 Acc: 0.31950844854070665\n",
      "Epoch [1/3500], Loss: 0.1040 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0119 Accuracy 0.24156545209176788\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2807017543859649\n",
      "Trial A 14 Acc: 0.3211469534050179\n",
      "Epoch [1/3500], Loss: 0.2002 Accuracy 0.018893387314439947\n",
      "Epoch [1001/3500], Loss: 0.0132 Accuracy 0.2550607287449393\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.26450742240215924\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2901484480431849\n",
      "Trial A 15 Acc: 0.3225806451612903\n",
      "Epoch [1/3500], Loss: 0.0794 Accuracy 0.21997300944669365\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.21862348178137653\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.22807017543859648\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2591093117408907\n",
      "Trial A 16 Acc: 0.3238456672991777\n",
      "Epoch [1/3500], Loss: 0.1679 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.20647773279352227\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.242914979757085\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.25371120107962214\n",
      "Trial A 17 Acc: 0.3249701314217443\n",
      "Epoch [1/3500], Loss: 0.1634 Accuracy 0.27125506072874495\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.23076923076923078\n",
      "Epoch [2001/3500], Loss: 0.0083 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2726045883940621\n",
      "Trial A 18 Acc: 0.3259762308998302\n",
      "Epoch [1/3500], Loss: 0.1101 Accuracy 0.010796221322537112\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.22402159244264508\n",
      "Epoch [2001/3500], Loss: 0.0080 Accuracy 0.26720647773279355\n",
      "Epoch [3001/3500], Loss: 0.0056 Accuracy 0.25371120107962214\n",
      "Trial A 19 Acc: 0.32688172043010744\n",
      "Epoch [1/3500], Loss: 0.1046 Accuracy 0.012145748987854251\n",
      "Epoch [1001/3500], Loss: 0.0144 Accuracy 0.21187584345479082\n",
      "Epoch [2001/3500], Loss: 0.0092 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0063 Accuracy 0.2388663967611336\n",
      "Trial A 20 Acc: 0.32770097286226313\n",
      "Epoch [1/3500], Loss: 0.1723 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0131 Accuracy 0.2388663967611336\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.22672064777327935\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.27125506072874495\n",
      "Trial A 21 Acc: 0.32844574780058644\n",
      "Epoch [1/3500], Loss: 0.0787 Accuracy 0.2766531713900135\n",
      "Epoch [1001/3500], Loss: 0.0133 Accuracy 0.23481781376518218\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.2483130904183536\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.28205128205128205\n",
      "Trial A 22 Acc: 0.32912575970079466\n",
      "Epoch [1/3500], Loss: 0.1608 Accuracy 0.36707152496626183\n",
      "Epoch [1001/3500], Loss: 0.0131 Accuracy 0.21592442645074225\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.24156545209176788\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.28475033738191635\n",
      "Trial A 23 Acc: 0.3297491039426522\n",
      "Epoch [1/3500], Loss: 0.1272 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.22807017543859648\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.242914979757085\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2726045883940621\n",
      "Trial A 24 Acc: 0.33032258064516123\n",
      "Epoch [1/3500], Loss: 0.0496 Accuracy 0.33738191632928477\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.21592442645074225\n",
      "Epoch [2001/3500], Loss: 0.0080 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.300944669365722\n",
      "Trial A 25 Acc: 0.33085194375516946\n",
      "Epoch [1/3500], Loss: 0.1546 Accuracy 0.3252361673414305\n",
      "Epoch [1001/3500], Loss: 0.0135 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.2807017543859649\n",
      "Trial A 26 Acc: 0.3313420947829549\n",
      "Epoch [1/3500], Loss: 0.0915 Accuracy 0.06612685560053981\n",
      "Epoch [1001/3500], Loss: 0.0125 Accuracy 0.24426450742240216\n",
      "Epoch [2001/3500], Loss: 0.0083 Accuracy 0.23346828609986506\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.27125506072874495\n",
      "Trial A 27 Acc: 0.33179723502304137\n",
      "Epoch [1/3500], Loss: 0.2375 Accuracy 0.18083670715249664\n",
      "Epoch [1001/3500], Loss: 0.0139 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.25775978407557354\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.23076923076923078\n",
      "Trial A 28 Acc: 0.3322209862810529\n",
      "Epoch [1/3500], Loss: 0.0494 Accuracy 0.2483130904183536\n",
      "Epoch [1001/3500], Loss: 0.0142 Accuracy 0.20107962213225372\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.22267206477732793\n",
      "Epoch [3001/3500], Loss: 0.0063 Accuracy 0.24021592442645073\n",
      "Trial A 29 Acc: 0.332616487455197\n",
      "Epoch [1/3500], Loss: 0.0844 Accuracy 0.16059379217273953\n",
      "Epoch [1001/3500], Loss: 0.0143 Accuracy 0.22672064777327935\n",
      "Epoch [2001/3500], Loss: 0.0090 Accuracy 0.23616734143049933\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.26045883940620784\n",
      "Trial A 30 Acc: 0.33298647242455764\n",
      "Epoch [1/3500], Loss: 0.0721 Accuracy 0.2591093117408907\n",
      "Epoch [1001/3500], Loss: 0.0127 Accuracy 0.22402159244264508\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.25775978407557354\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.25101214574898784\n",
      "Trial A 31 Acc: 0.3333333333333332\n",
      "Epoch [1/3500], Loss: 0.1311 Accuracy 0.24696356275303644\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.2321187584345479\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.2321187584345479\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.25775978407557354\n",
      "Trial A 32 Acc: 0.3336591723688497\n",
      "Epoch [1/3500], Loss: 0.1035 Accuracy 0.300944669365722\n",
      "Epoch [1001/3500], Loss: 0.0138 Accuracy 0.24021592442645073\n",
      "Epoch [2001/3500], Loss: 0.0091 Accuracy 0.2726045883940621\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.2591093117408907\n",
      "Trial A 33 Acc: 0.3339658444022769\n",
      "Epoch [1/3500], Loss: 0.1062 Accuracy 0.24696356275303644\n",
      "Epoch [1001/3500], Loss: 0.0124 Accuracy 0.23481781376518218\n",
      "Epoch [2001/3500], Loss: 0.0079 Accuracy 0.2388663967611336\n",
      "Epoch [3001/3500], Loss: 0.0056 Accuracy 0.24426450742240216\n",
      "Trial A 34 Acc: 0.3342549923195083\n",
      "Epoch [1/3500], Loss: 0.2264 Accuracy 0.04723346828609987\n",
      "Epoch [1001/3500], Loss: 0.0137 Accuracy 0.20107962213225372\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2591093117408907\n",
      "Trial A 35 Acc: 0.3345280764635602\n",
      "Epoch [1/3500], Loss: 0.2083 Accuracy 0.31713900134952766\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.2078272604588394\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.24696356275303644\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.27800269905533065\n",
      "Trial A 36 Acc: 0.3347863993025282\n",
      "Epoch [1/3500], Loss: 0.0606 Accuracy 0.1700404858299595\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.21727395411605938\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.252361673414305\n",
      "Trial A 37 Acc: 0.3350311262026031\n",
      "Epoch [1/3500], Loss: 0.0644 Accuracy 0.05668016194331984\n",
      "Epoch [1001/3500], Loss: 0.0135 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0093 Accuracy 0.2483130904183536\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2631578947368421\n",
      "Trial A 38 Acc: 0.3352633030052383\n",
      "Epoch [1/3500], Loss: 0.1023 Accuracy 0.2807017543859649\n",
      "Epoch [1001/3500], Loss: 0.0132 Accuracy 0.25775978407557354\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.2550607287449393\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.28609986504723345\n",
      "Trial A 39 Acc: 0.3354838709677418\n",
      "Epoch [1/3500], Loss: 0.0882 Accuracy 0.23076923076923078\n",
      "Epoch [1001/3500], Loss: 0.0129 Accuracy 0.2213225371120108\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.22807017543859648\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2631578947368421\n",
      "Trial A 40 Acc: 0.33569367951744017\n",
      "Epoch [1/3500], Loss: 0.1535 Accuracy 0.005398110661268556\n",
      "Epoch [1001/3500], Loss: 0.0131 Accuracy 0.2145748987854251\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.24021592442645073\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2631578947368421\n",
      "Trial A 41 Acc: 0.3358934971838196\n",
      "Epoch [1/3500], Loss: 0.0517 Accuracy 0.012145748987854251\n",
      "Epoch [1001/3500], Loss: 0.0124 Accuracy 0.21862348178137653\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.2388663967611336\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.26045883940620784\n",
      "Trial A 42 Acc: 0.33608402100525114\n",
      "Epoch [1/3500], Loss: 0.0810 Accuracy 0.029689608636977057\n",
      "Epoch [1001/3500], Loss: 0.0138 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.25775978407557354\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.24156545209176788\n",
      "Trial A 43 Acc: 0.33626588465298124\n",
      "Epoch [1/3500], Loss: 0.0485 Accuracy 0.1106612685560054\n",
      "Epoch [1001/3500], Loss: 0.0132 Accuracy 0.21322537112010798\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.2388663967611336\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.2631578947368421\n",
      "Trial A 44 Acc: 0.33643966547192333\n",
      "Epoch [1/3500], Loss: 0.0967 Accuracy 0.3643724696356275\n",
      "Epoch [1001/3500], Loss: 0.0138 Accuracy 0.21322537112010798\n",
      "Epoch [2001/3500], Loss: 0.0091 Accuracy 0.22672064777327935\n",
      "Epoch [3001/3500], Loss: 0.0064 Accuracy 0.2550607287449393\n",
      "Trial A 45 Acc: 0.3366058906030854\n",
      "Epoch [1/3500], Loss: 0.0937 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0125 Accuracy 0.242914979757085\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.23616734143049933\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.26450742240215924\n",
      "Trial A 46 Acc: 0.3367650423244107\n",
      "Epoch [1/3500], Loss: 0.1299 Accuracy 0.15519568151147098\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.2078272604588394\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.21052631578947367\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.24561403508771928\n",
      "Trial A 47 Acc: 0.3369175627240142\n",
      "Epoch [1/3500], Loss: 0.0717 Accuracy 0.033738191632928474\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.24426450742240216\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.2253711201079622\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.2591093117408907\n",
      "Trial A 48 Acc: 0.33706385780118486\n",
      "Epoch [1/3500], Loss: 0.2795 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0140 Accuracy 0.20512820512820512\n",
      "Epoch [2001/3500], Loss: 0.0083 Accuracy 0.2388663967611336\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.26450742240215924\n",
      "Trial A 49 Acc: 0.33720430107526866\n",
      "Epoch [1/3500], Loss: 0.0666 Accuracy 0.2739541160593792\n",
      "Epoch [1001/3500], Loss: 0.0137 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.2564102564102564\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2388663967611336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial B 0 Acc: 0.0\n",
      "Epoch [1/3500], Loss: 0.0708 Accuracy 0.006747638326585695\n",
      "Epoch [1001/3500], Loss: 0.0149 Accuracy 0.21997300944669365\n",
      "Epoch [2001/3500], Loss: 0.0092 Accuracy 0.22267206477732793\n",
      "Epoch [3001/3500], Loss: 0.0065 Accuracy 0.26180836707152494\n",
      "Trial B 1 Acc: 0.17204301075268819\n",
      "Epoch [1/3500], Loss: 0.1317 Accuracy 0.19838056680161945\n",
      "Epoch [1001/3500], Loss: 0.0133 Accuracy 0.19838056680161945\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.24021592442645073\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.2483130904183536\n",
      "Trial B 2 Acc: 0.22939068100358426\n",
      "Epoch [1/3500], Loss: 0.1021 Accuracy 0.3157894736842105\n",
      "Epoch [1001/3500], Loss: 0.0142 Accuracy 0.2078272604588394\n",
      "Epoch [2001/3500], Loss: 0.0091 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2766531713900135\n",
      "Trial B 3 Acc: 0.25806451612903225\n",
      "Epoch [1/3500], Loss: 0.1501 Accuracy 0.3090418353576248\n",
      "Epoch [1001/3500], Loss: 0.0137 Accuracy 0.2145748987854251\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.242914979757085\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.25775978407557354\n",
      "Trial B 4 Acc: 0.2752688172043011\n",
      "Epoch [1/3500], Loss: 0.0477 Accuracy 0.0620782726045884\n",
      "Epoch [1001/3500], Loss: 0.0141 Accuracy 0.1902834008097166\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.24156545209176788\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.23346828609986506\n",
      "Trial B 5 Acc: 0.28673835125448033\n",
      "Epoch [1/3500], Loss: 0.0795 Accuracy 0.004048582995951417\n",
      "Epoch [1001/3500], Loss: 0.0120 Accuracy 0.26450742240215924\n",
      "Epoch [2001/3500], Loss: 0.0080 Accuracy 0.2321187584345479\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.2766531713900135\n",
      "Trial B 6 Acc: 0.2949308755760369\n",
      "Epoch [1/3500], Loss: 0.0506 Accuracy 0.24021592442645073\n",
      "Epoch [1001/3500], Loss: 0.0127 Accuracy 0.2078272604588394\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.25371120107962214\n",
      "Trial B 7 Acc: 0.30107526881720437\n",
      "Epoch [1/3500], Loss: 0.0693 Accuracy 0.1039136302294197\n",
      "Epoch [1001/3500], Loss: 0.0131 Accuracy 0.23076923076923078\n",
      "Epoch [2001/3500], Loss: 0.0083 Accuracy 0.24156545209176788\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.26855600539811064\n",
      "Trial B 8 Acc: 0.3058542413381124\n",
      "Epoch [1/3500], Loss: 0.0877 Accuracy 0.016194331983805668\n",
      "Epoch [1001/3500], Loss: 0.0140 Accuracy 0.23751686909581646\n",
      "Epoch [2001/3500], Loss: 0.0090 Accuracy 0.26045883940620784\n",
      "Epoch [3001/3500], Loss: 0.0063 Accuracy 0.28205128205128205\n",
      "Trial B 9 Acc: 0.3096774193548388\n",
      "Epoch [1/3500], Loss: 0.1016 Accuracy 0.2766531713900135\n",
      "Epoch [1001/3500], Loss: 0.0130 Accuracy 0.21322537112010798\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.24021592442645073\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2941970310391363\n",
      "Trial B 10 Acc: 0.3128054740957968\n",
      "Epoch [1/3500], Loss: 0.0436 Accuracy 0.2874493927125506\n",
      "Epoch [1001/3500], Loss: 0.0125 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.25371120107962214\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.27800269905533065\n",
      "Trial B 11 Acc: 0.3154121863799284\n",
      "Epoch [1/3500], Loss: 0.2755 Accuracy 0.26720647773279355\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.21322537112010798\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.23616734143049933\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.2631578947368421\n",
      "Trial B 12 Acc: 0.31761786600496283\n",
      "Epoch [1/3500], Loss: 0.0987 Accuracy 0.001349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0121 Accuracy 0.2321187584345479\n",
      "Epoch [2001/3500], Loss: 0.0077 Accuracy 0.2550607287449393\n",
      "Epoch [3001/3500], Loss: 0.0054 Accuracy 0.28609986504723345\n",
      "Trial B 13 Acc: 0.31950844854070665\n",
      "Epoch [1/3500], Loss: 0.0586 Accuracy 0.018893387314439947\n",
      "Epoch [1001/3500], Loss: 0.0125 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.2496626180836707\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2807017543859649\n",
      "Trial B 14 Acc: 0.3211469534050179\n",
      "Epoch [1/3500], Loss: 0.0530 Accuracy 0.11875843454790823\n",
      "Epoch [1001/3500], Loss: 0.0139 Accuracy 0.20917678812415655\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.23751686909581646\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.27800269905533065\n",
      "Trial B 15 Acc: 0.3225806451612903\n",
      "Epoch [1/3500], Loss: 0.1067 Accuracy 0.2941970310391363\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.21592442645074225\n",
      "Epoch [3001/3500], Loss: 0.0056 Accuracy 0.27125506072874495\n",
      "Trial B 16 Acc: 0.3238456672991777\n",
      "Epoch [1/3500], Loss: 0.1582 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0125 Accuracy 0.23481781376518218\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.26450742240215924\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2699055330634278\n",
      "Trial B 17 Acc: 0.3249701314217443\n",
      "Epoch [1/3500], Loss: 0.1524 Accuracy 0.0620782726045884\n",
      "Epoch [1001/3500], Loss: 0.0130 Accuracy 0.19568151147098514\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.2631578947368421\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.2982456140350877\n",
      "Trial B 18 Acc: 0.3259762308998302\n",
      "Epoch [1/3500], Loss: 0.0864 Accuracy 0.1767881241565452\n",
      "Epoch [1001/3500], Loss: 0.0146 Accuracy 0.2213225371120108\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.24156545209176788\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.2739541160593792\n",
      "Trial B 19 Acc: 0.32688172043010744\n",
      "Epoch [1/3500], Loss: 0.3150 Accuracy 0.08906882591093117\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.21727395411605938\n",
      "Epoch [2001/3500], Loss: 0.0077 Accuracy 0.2388663967611336\n",
      "Epoch [3001/3500], Loss: 0.0056 Accuracy 0.2726045883940621\n",
      "Trial B 20 Acc: 0.32770097286226313\n",
      "Epoch [1/3500], Loss: 0.0535 Accuracy 0.10526315789473684\n",
      "Epoch [1001/3500], Loss: 0.0136 Accuracy 0.23751686909581646\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.25775978407557354\n",
      "Epoch [3001/3500], Loss: 0.0063 Accuracy 0.2550607287449393\n",
      "Trial B 21 Acc: 0.32844574780058644\n",
      "Epoch [1/3500], Loss: 0.1096 Accuracy 0.358974358974359\n",
      "Epoch [1001/3500], Loss: 0.0135 Accuracy 0.21727395411605938\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2726045883940621\n",
      "Trial B 22 Acc: 0.32912575970079466\n",
      "Epoch [1/3500], Loss: 0.0778 Accuracy 0.1349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0128 Accuracy 0.19838056680161945\n",
      "Epoch [2001/3500], Loss: 0.0088 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.25101214574898784\n",
      "Trial B 23 Acc: 0.3297491039426522\n",
      "Epoch [1/3500], Loss: 0.0936 Accuracy 0.22402159244264508\n",
      "Epoch [1001/3500], Loss: 0.0130 Accuracy 0.21052631578947367\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.23346828609986506\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2388663967611336\n",
      "Trial B 24 Acc: 0.33032258064516123\n",
      "Epoch [1/3500], Loss: 0.0698 Accuracy 0.21862348178137653\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.23076923076923078\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.25371120107962214\n",
      "Trial B 25 Acc: 0.33085194375516946\n",
      "Epoch [1/3500], Loss: 0.2692 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0137 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.25101214574898784\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2496626180836707\n",
      "Trial B 26 Acc: 0.3313420947829549\n",
      "Epoch [1/3500], Loss: 0.1617 Accuracy 0.3657219973009447\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.252361673414305\n",
      "Epoch [2001/3500], Loss: 0.0075 Accuracy 0.26720647773279355\n",
      "Epoch [3001/3500], Loss: 0.0054 Accuracy 0.26045883940620784\n",
      "Trial B 27 Acc: 0.33179723502304137\n",
      "Epoch [1/3500], Loss: 0.0415 Accuracy 0.2766531713900135\n",
      "Epoch [1001/3500], Loss: 0.0129 Accuracy 0.23751686909581646\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.26180836707152494\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.2496626180836707\n",
      "Trial B 28 Acc: 0.3322209862810529\n",
      "Epoch [1/3500], Loss: 0.1041 Accuracy 0.20242914979757085\n",
      "Epoch [1001/3500], Loss: 0.0142 Accuracy 0.23076923076923078\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.25371120107962214\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.26180836707152494\n",
      "Trial B 29 Acc: 0.332616487455197\n",
      "Epoch [1/3500], Loss: 0.2219 Accuracy 0.001349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0123 Accuracy 0.242914979757085\n",
      "Epoch [2001/3500], Loss: 0.0078 Accuracy 0.2388663967611336\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.2793522267206478\n",
      "Trial B 30 Acc: 0.33298647242455764\n",
      "Epoch [1/3500], Loss: 0.0479 Accuracy 0.2793522267206478\n",
      "Epoch [1001/3500], Loss: 0.0131 Accuracy 0.22267206477732793\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.24156545209176788\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.26855600539811064\n",
      "Trial B 31 Acc: 0.3333333333333332\n",
      "Epoch [1/3500], Loss: 0.1585 Accuracy 0.11605937921727395\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.24696356275303644\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.23481781376518218\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.23076923076923078\n",
      "Trial B 32 Acc: 0.3336591723688497\n",
      "Epoch [1/3500], Loss: 0.2310 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0121 Accuracy 0.22402159244264508\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.26855600539811064\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.291497975708502\n",
      "Trial B 33 Acc: 0.3339658444022769\n",
      "Epoch [1/3500], Loss: 0.1011 Accuracy 0.001349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.21052631578947367\n",
      "Epoch [2001/3500], Loss: 0.0087 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.25101214574898784\n",
      "Trial B 34 Acc: 0.3342549923195083\n",
      "Epoch [1/3500], Loss: 0.2748 Accuracy 0.2874493927125506\n",
      "Epoch [1001/3500], Loss: 0.0110 Accuracy 0.22672064777327935\n",
      "Epoch [2001/3500], Loss: 0.0071 Accuracy 0.2591093117408907\n",
      "Epoch [3001/3500], Loss: 0.0053 Accuracy 0.2807017543859649\n",
      "Trial B 35 Acc: 0.3345280764635602\n",
      "Epoch [1/3500], Loss: 0.1076 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0134 Accuracy 0.22672064777327935\n",
      "Epoch [2001/3500], Loss: 0.0083 Accuracy 0.25775978407557354\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.252361673414305\n",
      "Trial B 36 Acc: 0.3347863993025282\n",
      "Epoch [1/3500], Loss: 0.0368 Accuracy 0.2807017543859649\n",
      "Epoch [1001/3500], Loss: 0.0135 Accuracy 0.2253711201079622\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.22941970310391363\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.27530364372469635\n",
      "Trial B 37 Acc: 0.3350311262026031\n",
      "Epoch [1/3500], Loss: 0.0493 Accuracy 0.010796221322537112\n",
      "Epoch [1001/3500], Loss: 0.0128 Accuracy 0.21592442645074225\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.21862348178137653\n",
      "Epoch [3001/3500], Loss: 0.0063 Accuracy 0.25775978407557354\n",
      "Trial B 38 Acc: 0.3352633030052383\n",
      "Epoch [1/3500], Loss: 0.0586 Accuracy 0.10121457489878542\n",
      "Epoch [1001/3500], Loss: 0.0135 Accuracy 0.242914979757085\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.24426450742240216\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.2726045883940621\n",
      "Trial B 39 Acc: 0.3354838709677418\n",
      "Epoch [1/3500], Loss: 0.1513 Accuracy 0.24156545209176788\n",
      "Epoch [1001/3500], Loss: 0.0119 Accuracy 0.23346828609986506\n",
      "Epoch [2001/3500], Loss: 0.0075 Accuracy 0.23481781376518218\n",
      "Epoch [3001/3500], Loss: 0.0055 Accuracy 0.24021592442645073\n",
      "Trial B 40 Acc: 0.33569367951744017\n",
      "Epoch [1/3500], Loss: 0.0979 Accuracy 0.014844804318488529\n",
      "Epoch [1001/3500], Loss: 0.0140 Accuracy 0.23481781376518218\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.24561403508771928\n",
      "Epoch [3001/3500], Loss: 0.0058 Accuracy 0.23481781376518218\n",
      "Trial B 41 Acc: 0.3358934971838196\n",
      "Epoch [1/3500], Loss: 0.2165 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0126 Accuracy 0.2253711201079622\n",
      "Epoch [2001/3500], Loss: 0.0085 Accuracy 0.25775978407557354\n",
      "Epoch [3001/3500], Loss: 0.0059 Accuracy 0.25101214574898784\n",
      "Trial B 42 Acc: 0.33608402100525114\n",
      "Epoch [1/3500], Loss: 0.1334 Accuracy 0.0796221322537112\n",
      "Epoch [1001/3500], Loss: 0.0143 Accuracy 0.20512820512820512\n",
      "Epoch [2001/3500], Loss: 0.0090 Accuracy 0.22672064777327935\n",
      "Epoch [3001/3500], Loss: 0.0060 Accuracy 0.2550607287449393\n",
      "Trial B 43 Acc: 0.33626588465298124\n",
      "Epoch [1/3500], Loss: 0.3090 Accuracy 0.005398110661268556\n",
      "Epoch [1001/3500], Loss: 0.0128 Accuracy 0.21727395411605938\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.25101214574898784\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.23481781376518218\n",
      "Trial B 44 Acc: 0.33643966547192333\n",
      "Epoch [1/3500], Loss: 0.0500 Accuracy 0.3184885290148448\n",
      "Epoch [1001/3500], Loss: 0.0130 Accuracy 0.2388663967611336\n",
      "Epoch [2001/3500], Loss: 0.0082 Accuracy 0.2145748987854251\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2739541160593792\n",
      "Trial B 45 Acc: 0.3366058906030854\n",
      "Epoch [1/3500], Loss: 0.1245 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0133 Accuracy 0.2078272604588394\n",
      "Epoch [2001/3500], Loss: 0.0081 Accuracy 0.2483130904183536\n",
      "Epoch [3001/3500], Loss: 0.0056 Accuracy 0.27125506072874495\n",
      "Trial B 46 Acc: 0.3367650423244107\n",
      "Epoch [1/3500], Loss: 0.1123 Accuracy 0.005398110661268556\n",
      "Epoch [1001/3500], Loss: 0.0140 Accuracy 0.20917678812415655\n",
      "Epoch [2001/3500], Loss: 0.0089 Accuracy 0.24156545209176788\n",
      "Epoch [3001/3500], Loss: 0.0061 Accuracy 0.2658569500674764\n",
      "Trial B 47 Acc: 0.3369175627240142\n",
      "Epoch [1/3500], Loss: 0.2253 Accuracy 0.001349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0140 Accuracy 0.2078272604588394\n",
      "Epoch [2001/3500], Loss: 0.0091 Accuracy 0.22267206477732793\n",
      "Epoch [3001/3500], Loss: 0.0064 Accuracy 0.2483130904183536\n",
      "Trial B 48 Acc: 0.33706385780118486\n",
      "Epoch [1/3500], Loss: 0.0356 Accuracy 0.3049932523616734\n",
      "Epoch [1001/3500], Loss: 0.0127 Accuracy 0.22672064777327935\n",
      "Epoch [2001/3500], Loss: 0.0084 Accuracy 0.22672064777327935\n",
      "Epoch [3001/3500], Loss: 0.0057 Accuracy 0.2496626180836707\n",
      "Trial B 49 Acc: 0.33720430107526866\n",
      "Epoch [1/3500], Loss: 0.0851 Accuracy 0.18758434547908232\n",
      "Epoch [1001/3500], Loss: 0.0138 Accuracy 0.2253711201079622\n",
      "Epoch [2001/3500], Loss: 0.0086 Accuracy 0.26045883940620784\n",
      "Epoch [3001/3500], Loss: 0.0062 Accuracy 0.24696356275303644\n"
     ]
    }
   ],
   "source": [
    "TRIALS=50\n",
    "num_epochs=3500\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]],scale=True)\n",
    "ya=ya/10\n",
    "n_inputs=X.shape[1]\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "average_a1=np.zeros((num_epochs,))\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs)\n",
    "    average_a1+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a1=average_a1/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]],scale=False)\n",
    "X=X/100\n",
    "ya=ya/10\n",
    "\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "average_a=np.zeros((num_epochs,))\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial B\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs)\n",
    "    average_a+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a=average_a/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "\n",
    "plt.title(\"Performance of models with and without preprocessing\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a,c=\"r\",label=\"Without preprocessing\")\n",
    "plt.plot(average_a1,c=\"b\",label=\"With preprocessing\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial A 0 Acc: 0.0\n",
      "Epoch [1/3500], Loss: 0.0741 Accuracy 0.46153846153846156\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 1 Acc: 0.49731182795698925\n",
      "Epoch [1/3500], Loss: 0.1171 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 2 Acc: 0.6630824372759857\n",
      "Epoch [1/3500], Loss: 0.3727 Accuracy 0.6410256410256411\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 3 Acc: 0.7459677419354839\n",
      "Epoch [1/3500], Loss: 0.1453 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 4 Acc: 0.7967741935483871\n",
      "Epoch [1/3500], Loss: 0.1563 Accuracy 0.340080971659919\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 5 Acc: 0.8297491039426523\n",
      "Epoch [1/3500], Loss: 0.3020 Accuracy 0.5060728744939271\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 6 Acc: 0.8540706605222734\n",
      "Epoch [1/3500], Loss: 0.0352 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 7 Acc: 0.8716397849462365\n",
      "Epoch [1/3500], Loss: 0.0801 Accuracy 0.6423751686909581\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 8 Acc: 0.8853046594982078\n",
      "Epoch [1/3500], Loss: 0.0854 Accuracy 0.6477732793522267\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 9 Acc: 0.8962365591397848\n",
      "Epoch [1/3500], Loss: 0.0251 Accuracy 0.6585695006747638\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 10 Acc: 0.9051808406647115\n",
      "Epoch [1/3500], Loss: 0.2386 Accuracy 0.4399460188933873\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 11 Acc: 0.9126344086021504\n",
      "Epoch [1/3500], Loss: 0.1174 Accuracy 0.6585695006747638\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 12 Acc: 0.9193548387096773\n",
      "Epoch [1/3500], Loss: 0.0505 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 13 Acc: 0.9247311827956988\n",
      "Epoch [1/3500], Loss: 0.1043 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 14 Acc: 0.929390681003584\n",
      "Epoch [1/3500], Loss: 0.1468 Accuracy 0.6585695006747638\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 15 Acc: 0.9334677419354837\n",
      "Epoch [1/3500], Loss: 0.0550 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 16 Acc: 0.9373814041745728\n",
      "Epoch [1/3500], Loss: 0.0279 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0003 Accuracy 0.99055330634278\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9959514170040485\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 17 Acc: 0.9402628434886497\n",
      "Epoch [1/3500], Loss: 0.1906 Accuracy 0.6558704453441295\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 18 Acc: 0.9431239388794567\n",
      "Epoch [1/3500], Loss: 0.0715 Accuracy 0.3738191632928475\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 19 Acc: 0.9459677419354838\n",
      "Epoch [1/3500], Loss: 0.0764 Accuracy 0.50472334682861\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 20 Acc: 0.9482846902201741\n",
      "Epoch [1/3500], Loss: 0.0404 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 21 Acc: 0.9503910068426198\n",
      "Epoch [1/3500], Loss: 0.1868 Accuracy 0.47233468286099867\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 22 Acc: 0.9523141654978964\n",
      "Epoch [1/3500], Loss: 0.0295 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 23 Acc: 0.9540770609318998\n",
      "Epoch [1/3500], Loss: 0.1436 Accuracy 0.6531713900134952\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 24 Acc: 0.955698924731183\n",
      "Epoch [1/3500], Loss: 0.0465 Accuracy 0.6545209176788124\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 25 Acc: 0.9571960297766752\n",
      "Epoch [1/3500], Loss: 0.1975 Accuracy 0.524966261808367\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 26 Acc: 0.958582238152131\n",
      "Epoch [1/3500], Loss: 0.1904 Accuracy 0.48043184885290147\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 27 Acc: 0.9598694316436255\n",
      "Epoch [1/3500], Loss: 0.0560 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 28 Acc: 0.9610678531701895\n",
      "Epoch [1/3500], Loss: 0.0396 Accuracy 0.6005398110661269\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 29 Acc: 0.9621863799283158\n",
      "Epoch [1/3500], Loss: 0.0947 Accuracy 0.50472334682861\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 30 Acc: 0.9632327436697888\n",
      "Epoch [1/3500], Loss: 0.1051 Accuracy 0.562753036437247\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 31 Acc: 0.964381720430108\n",
      "Epoch [1/3500], Loss: 0.0591 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 32 Acc: 0.965298142717498\n",
      "Epoch [1/3500], Loss: 0.0664 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 33 Acc: 0.9661606578115122\n",
      "Epoch [1/3500], Loss: 0.0419 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 34 Acc: 0.9671274961597547\n",
      "Epoch [1/3500], Loss: 0.3385 Accuracy 0.37112010796221323\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 35 Acc: 0.9678912783751498\n",
      "Epoch [1/3500], Loss: 0.1933 Accuracy 0.6558704453441295\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 36 Acc: 0.9686137750653883\n",
      "Epoch [1/3500], Loss: 0.1103 Accuracy 0.6248313090418354\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 37 Acc: 0.9694397283531412\n",
      "Epoch [1/3500], Loss: 0.3034 Accuracy 0.3508771929824561\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 38 Acc: 0.9700854700854703\n",
      "Epoch [1/3500], Loss: 0.0393 Accuracy 0.48313090418353577\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 39 Acc: 0.970698924731183\n",
      "Epoch [1/3500], Loss: 0.0534 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 40 Acc: 0.9712824547600316\n",
      "Epoch [1/3500], Loss: 0.1057 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 41 Acc: 0.9718381976446493\n",
      "Epoch [1/3500], Loss: 0.1338 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 42 Acc: 0.9723680920230058\n",
      "Epoch [1/3500], Loss: 0.1014 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 43 Acc: 0.972873900293255\n",
      "Epoch [1/3500], Loss: 0.0358 Accuracy 0.3697705802968961\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 44 Acc: 0.9733572281959378\n",
      "Epoch [1/3500], Loss: 0.0321 Accuracy 0.6572199730094467\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 45 Acc: 0.973819541841982\n",
      "Epoch [1/3500], Loss: 0.1225 Accuracy 0.3441295546558704\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 46 Acc: 0.9742621825669181\n",
      "Epoch [1/3500], Loss: 0.1212 Accuracy 0.5371120107962213\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 47 Acc: 0.9746863799283152\n",
      "Epoch [1/3500], Loss: 0.1964 Accuracy 0.6302294197031039\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0000 Accuracy 1.0\n",
      "Trial A 48 Acc: 0.9750932631116961\n",
      "Epoch [1/3500], Loss: 0.2539 Accuracy 0.5479082321187584\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9973009446693657\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.9986504723346828\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Trial A 49 Acc: 0.9754838709677416\n",
      "Epoch [1/3500], Loss: 0.0275 Accuracy 0.3427800269905533\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.9986504723346828\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 1.0\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "Trial A 0 Acc: 0.0\n",
      "Epoch [1/3500], Loss: 0.0606 Accuracy 0.18083670715249664\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 1 Acc: 0.2768817204301075\n",
      "Epoch [1/3500], Loss: 0.1013 Accuracy 0.36707152496626183\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6450742240215924\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6383265856950068\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 2 Acc: 0.37275985663082434\n",
      "Epoch [1/3500], Loss: 0.0805 Accuracy 0.28475033738191635\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 3 Acc: 0.42607526881720426\n",
      "Epoch [1/3500], Loss: 0.1133 Accuracy 0.10796221322537113\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6558704453441295\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 4 Acc: 0.4548387096774193\n",
      "Epoch [1/3500], Loss: 0.0749 Accuracy 0.30364372469635625\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 5 Acc: 0.4740143369175627\n",
      "Epoch [1/3500], Loss: 0.1391 Accuracy 0.27530364372469635\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6464237516869096\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 6 Acc: 0.488479262672811\n",
      "Epoch [1/3500], Loss: 0.1233 Accuracy 0.3319838056680162\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 7 Acc: 0.49865591397849457\n",
      "Epoch [1/3500], Loss: 0.0801 Accuracy 0.002699055330634278\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 8 Acc: 0.505973715651135\n",
      "Epoch [1/3500], Loss: 0.0851 Accuracy 0.2496626180836707\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 9 Acc: 0.5118279569892473\n",
      "Epoch [1/3500], Loss: 0.0699 Accuracy 0.006747638326585695\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6464237516869096\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 10 Acc: 0.5190615835777126\n",
      "Epoch [1/3500], Loss: 0.1615 Accuracy 0.2496626180836707\n",
      "Epoch [1001/3500], Loss: 0.0003 Accuracy 0.6140350877192983\n",
      "Epoch [2001/3500], Loss: 0.0002 Accuracy 0.6369770580296896\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6396761133603239\n",
      "Trial A 11 Acc: 0.5241935483870969\n",
      "Epoch [1/3500], Loss: 0.2099 Accuracy 0.2982456140350877\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.650472334682861\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 12 Acc: 0.5285359801488835\n",
      "Epoch [1/3500], Loss: 0.0874 Accuracy 0.11605937921727395\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 13 Acc: 0.5311059907834103\n",
      "Epoch [1/3500], Loss: 0.2157 Accuracy 0.22267206477732793\n",
      "Epoch [1001/3500], Loss: 0.0003 Accuracy 0.6059379217273954\n",
      "Epoch [2001/3500], Loss: 0.0002 Accuracy 0.631578947368421\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6437246963562753\n",
      "Trial A 14 Acc: 0.5344086021505378\n",
      "Epoch [1/3500], Loss: 0.0675 Accuracy 0.23076923076923078\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 15 Acc: 0.536962365591398\n",
      "Epoch [1/3500], Loss: 0.1346 Accuracy 0.3603238866396761\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 16 Acc: 0.5385831752055662\n",
      "Epoch [1/3500], Loss: 0.2739 Accuracy 0.2496626180836707\n",
      "Epoch [1001/3500], Loss: 0.0004 Accuracy 0.6207827260458839\n",
      "Epoch [2001/3500], Loss: 0.0002 Accuracy 0.6491228070175439\n",
      "Epoch [3001/3500], Loss: 0.0002 Accuracy 0.6531713900134952\n",
      "Trial A 17 Acc: 0.5403225806451615\n",
      "Epoch [1/3500], Loss: 0.1169 Accuracy 0.012145748987854251\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 18 Acc: 0.5415959252971139\n",
      "Epoch [1/3500], Loss: 0.1603 Accuracy 0.07557354925775979\n",
      "Epoch [1001/3500], Loss: 0.0003 Accuracy 0.6410256410256411\n",
      "Epoch [2001/3500], Loss: 0.0002 Accuracy 0.6518218623481782\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 19 Acc: 0.5430107526881722\n",
      "Epoch [1/3500], Loss: 0.1801 Accuracy 0.3252361673414305\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6518218623481782\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 20 Acc: 0.5448028673835127\n",
      "Epoch [1/3500], Loss: 0.1058 Accuracy 0.012145748987854251\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6531713900134952\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 21 Acc: 0.5459433040078202\n",
      "Epoch [1/3500], Loss: 0.1483 Accuracy 0.02564102564102564\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6369770580296896\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Trial A 22 Acc: 0.5469845722300142\n",
      "Epoch [1/3500], Loss: 0.1355 Accuracy 0.252361673414305\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6531713900134952\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 23 Acc: 0.5481630824372762\n",
      "Epoch [1/3500], Loss: 0.0615 Accuracy 0.349527665317139\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6383265856950068\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Trial A 24 Acc: 0.5490322580645163\n",
      "Epoch [1/3500], Loss: 0.0797 Accuracy 0.3076923076923077\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6410256410256411\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 25 Acc: 0.5500413564929696\n",
      "Epoch [1/3500], Loss: 0.0974 Accuracy 0.3144399460188934\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6410256410256411\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 26 Acc: 0.550378335324572\n",
      "Epoch [1/3500], Loss: 0.0912 Accuracy 0.002699055330634278\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 27 Acc: 0.5510752688172045\n",
      "Epoch [1/3500], Loss: 0.1160 Accuracy 0.02564102564102564\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6531713900134952\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 28 Acc: 0.5517241379310346\n",
      "Epoch [1/3500], Loss: 0.1505 Accuracy 0.2901484480431849\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6491228070175439\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 29 Acc: 0.5521505376344088\n",
      "Epoch [1/3500], Loss: 0.1265 Accuracy 0.23346828609986506\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6423751686909581\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 30 Acc: 0.5528962885882762\n",
      "Epoch [1/3500], Loss: 0.1411 Accuracy 0.012145748987854251\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6518218623481782\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 31 Acc: 0.5532594086021507\n",
      "Epoch [1/3500], Loss: 0.0956 Accuracy 0.2982456140350877\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 32 Acc: 0.5536005213424571\n",
      "Epoch [1/3500], Loss: 0.1232 Accuracy 0.28475033738191635\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6491228070175439\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Trial A 33 Acc: 0.554237824161923\n",
      "Epoch [1/3500], Loss: 0.1663 Accuracy 0.242914979757085\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6572199730094467\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 34 Acc: 0.5548387096774196\n",
      "Epoch [1/3500], Loss: 0.1038 Accuracy 0.016194331983805668\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 35 Acc: 0.5554062126642774\n",
      "Epoch [1/3500], Loss: 0.1862 Accuracy 0.1039136302294197\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6477732793522267\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 36 Acc: 0.5556524266201688\n",
      "Epoch [1/3500], Loss: 0.0761 Accuracy 0.3630229419703104\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 37 Acc: 0.5560271646859085\n",
      "Epoch [1/3500], Loss: 0.0879 Accuracy 0.0\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6518218623481782\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 38 Acc: 0.556244830438379\n",
      "Epoch [1/3500], Loss: 0.1135 Accuracy 0.23076923076923078\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 39 Acc: 0.5565860215053766\n",
      "Epoch [1/3500], Loss: 0.2004 Accuracy 0.3562753036437247\n",
      "Epoch [1001/3500], Loss: 0.0003 Accuracy 0.6153846153846154\n",
      "Epoch [2001/3500], Loss: 0.0002 Accuracy 0.6423751686909581\n",
      "Epoch [3001/3500], Loss: 0.0002 Accuracy 0.650472334682861\n",
      "Trial A 40 Acc: 0.5571728297928144\n",
      "Epoch [1/3500], Loss: 0.0351 Accuracy 0.0310391363022942\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Trial A 41 Acc: 0.5573476702508964\n",
      "Epoch [1/3500], Loss: 0.0411 Accuracy 0.016194331983805668\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Trial A 42 Acc: 0.557514378594649\n",
      "Epoch [1/3500], Loss: 0.1033 Accuracy 0.3697705802968961\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6545209176788124\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6356275303643725\n",
      "Trial A 43 Acc: 0.5577956989247315\n",
      "Epoch [1/3500], Loss: 0.0870 Accuracy 0.11740890688259109\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6518218623481782\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 44 Acc: 0.5584229390681007\n",
      "Epoch [1/3500], Loss: 0.2940 Accuracy 0.3157894736842105\n",
      "Epoch [1001/3500], Loss: 0.0005 Accuracy 0.5371120107962213\n",
      "Epoch [2001/3500], Loss: 0.0004 Accuracy 0.6045883940620783\n",
      "Epoch [3001/3500], Loss: 0.0003 Accuracy 0.5937921727395412\n",
      "Trial A 45 Acc: 0.558789153810192\n",
      "Epoch [1/3500], Loss: 0.1005 Accuracy 0.3616734143049933\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6531713900134952\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6545209176788124\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 46 Acc: 0.5589110043468317\n",
      "Epoch [1/3500], Loss: 0.0660 Accuracy 0.1659919028340081\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.6464237516869096\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6558704453441295\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 47 Acc: 0.5591397849462368\n",
      "Epoch [1/3500], Loss: 0.0683 Accuracy 0.08906882591093117\n",
      "Epoch [1001/3500], Loss: 0.0002 Accuracy 0.6518218623481782\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 48 Acc: 0.5595786701777489\n",
      "Epoch [1/3500], Loss: 0.0866 Accuracy 0.1241565452091768\n",
      "Epoch [1001/3500], Loss: 0.0001 Accuracy 0.650472334682861\n",
      "Epoch [2001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Epoch [3001/3500], Loss: 0.0001 Accuracy 0.6572199730094467\n",
      "Trial A 49 Acc: 0.5595698924731186\n",
      "Epoch [1/3500], Loss: 0.4137 Accuracy 0.32118758434547906\n",
      "Epoch [1001/3500], Loss: 0.0007 Accuracy 0.5856950067476383\n",
      "Epoch [2001/3500], Loss: 0.0005 Accuracy 0.5802968960863698\n",
      "Epoch [3001/3500], Loss: 0.0005 Accuracy 0.5748987854251012\n"
     ]
    }
   ],
   "source": [
    "TRIALS=50\n",
    "num_epochs=3500\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[1,0],[1,0],[0,1]],scale=True)\n",
    "ya=ya/10\n",
    "n_inputs=X.shape[1]\n",
    "m_outputs=2\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "average_a1=np.zeros((num_epochs,))\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs)\n",
    "    average_a1+=np.array(a)\n",
    "\n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a1=average_a1/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "\n",
    "\n",
    "d=dataset()\n",
    "X,ya=d.generate(STORE=5,y_labels=[[0,1,0,1,0],[1,0,0,1,0],[0,1,0,0,1]],scale=True)\n",
    "ya=ya/10\n",
    "n_inputs=X.shape[1]\n",
    "m_outputs=5\n",
    "X, data_test, y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "average_a2=np.zeros((num_epochs,))\n",
    "p=0\n",
    "for i in range(TRIALS):\n",
    "    print(\"Trial A\",i,\"Acc:\",p/(i+1))\n",
    "    # Create the neural network\n",
    "    model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    l,a=train(model,num_epochs)\n",
    "    average_a2+=np.array(a)\n",
    " \n",
    "    #unseen data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "    p+=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "average_a2=average_a2/(TRIALS) #average by 10 and then divide by 5 for scaling\n",
    "\n",
    "plt.title(\"How the architecture influences model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a1,c=\"g\",label=\"binary-classifier\")\n",
    "plt.plot(average_a2,c=\"y\",label=\"Multi-classifier\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"How the architecture influences model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.plot(average_l,c=\"r\",label=\"Loss without slip data\")\n",
    "plt.plot(average_a1,c=\"g\",label=\"binary-classifier\")\n",
    "plt.plot(average_a2,c=\"y\",label=\"Multi-classifier\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIALS=20\n",
    "TIMES=15\n",
    "average_a=np.zeros((TIMES,800))\n",
    "average_unseen=np.zeros((TIMES,))\n",
    "for i in range(1,TIMES):\n",
    "    d=dataset()\n",
    "    X,ya=d.generate(STORE=i,y_labels=[[1,0,0],[1,0,0],[0,0,1]],scale=True)\n",
    "    ya=ya/10\n",
    "    X, data_test, Y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "    n_inputs = X.shape[1]\n",
    "    m_outputs = len(y[0])\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "    for j in range(TRIALS):\n",
    "        model = SimpleNeuralNetwork(n_inputs, m_outputs).to(device)\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        l,a=train(model,800,output=False)\n",
    "        #predict\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "        acc=get_acc(predictions.cpu(),should_be=labels_test)\n",
    "        average_unseen[i]+=acc\n",
    "\n",
    "        average_a[i]+=a\n",
    "    average_a[i]/=TRIALS #get average\n",
    "    average_unseen[i]/=TRIALS\n",
    "    print(i,average_a[i][-1],average_unseen[i])\n",
    "#diplay\n",
    "\n",
    "plt.title(\"Accuricies vs T size\")\n",
    "plt.plot(np.max(average_a,axis=1))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Temporal vector size (T)\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Accuricies vs T size on unseen data\")\n",
    "plt.plot(average_unseen)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Temporal vector size (T)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "np.save(path+\"data_of_t_size\",average_a)\n",
    "print(np.max(average_a,axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.38057796 0.38239247 0.39254032 ... 0.99616935 0.99543011 0.99475806]\n",
      " [0.34301075 0.36196237 0.45248656 ... 0.9938172  0.99455645 0.99462366]\n",
      " ...\n",
      " [0.33565517 0.37110345 0.49986207 ... 0.99751724 0.99793103 0.99696552]\n",
      " [0.36850829 0.35359116 0.42071823 ... 0.99903315 0.99889503 0.99848066]\n",
      " [0.39570637 0.38400277 0.34466759 ... 0.99716066 0.99792244 0.99743767]]\n"
     ]
    }
   ],
   "source": [
    "data=np.load(path+\"data_of_t_size.npy\")\n",
    "print(data)\n",
    "for i in range(len(data)):\n",
    "    plt.title(\"Accuricies vs T size\")\n",
    "    plt.plot(data[i],label=\"T=\"+str(i+1))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuricies vs T size on unseen data\")\n",
    "plt.plot(average_unseen)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Temporal vector size (T)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "1 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "2 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "3 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "4 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "5 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "6 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "7 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_detection.avi Length: 338\n",
      "edge_detection.avi Length: 262\n",
      "flat_slip_detection.avi Length: 327\n",
      "8 0.8775978407557353\n"
     ]
    }
   ],
   "source": [
    "TRIALS=20\n",
    "TIMES=9\n",
    "average_a=np.zeros((TIMES,2500))\n",
    "\n",
    "for i in range(1,TIMES):\n",
    "    d=dataset()\n",
    "    x,y=d.generate(STORE=5,y_labels=[[1,0,0],[1,0,0],[0,0,1]])\n",
    "    SIZE=x.shape[1]\n",
    "    y=y/10\n",
    "    n_inputs = x.shape[1]\n",
    "    m_outputs = len(y[1])\n",
    "    for j in range(TRIALS):\n",
    "        X, data_test, Y, labels_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "        \n",
    "        model = SimpleNeuralNetwork(n_inputs, m_outputs,drop_out_prob=TIMES/10).to(device)\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        l,a=train(model,output=False)\n",
    "        average_a[i]+=a\n",
    "    average_a[i]/=TRIALS #get average\n",
    "    print(i,average_a[-1][-1])\n",
    "#diplay\n",
    "\n",
    "plt.title(\"How dropout rate influences accuracy\")\n",
    "plt.plot(np.max(average_a,axis=1))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Dropout rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"How dropout rate influences accuracy\")\n",
    "\n",
    "plt.plot([i/10 for i in range(len(np.max(average_a,axis=1)))],np.max(average_a,axis=1))\n",
    "y_error = np.std(average_a,axis=1)/2\n",
    " \n",
    "plt.errorbar([i/10 for i in range(len(np.max(average_a,axis=1)))],np.max(average_a,axis=1),\n",
    "             yerr = y_error,\n",
    "             fmt ='o')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Dropout rate\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lego.avi Length: 302 / 318 X-size: (302, 27654)\n",
      "smooth.avi Length: 537 / 540 X-size: (839, 27654)\n",
      "Completed creation\n",
      "1.5642683592886897e-07\n",
      "(671, 27654) (671, 2)\n",
      "(168, 27654) (168, 2)\n"
     ]
    }
   ],
   "source": [
    "d=dataset(names=[\"lego.avi\",\"smooth.avi\"])\n",
    "d.SIZE=0.3\n",
    "X,ya=d.gen_image_data(STORE=1,y_labels=[[1,0],[0,1]],scale=True)\n",
    "print(np.sum(X))\n",
    "ya=ya/10\n",
    "X, data_test, Y, labels_test = train_test_split(X, ya, test_size=0.20, random_state=42)\n",
    "print(X.shape,Y.shape)\n",
    "print(data_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/drs25/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Define the size of the input (n) and output (m) layers\n",
    "n_inputs = X.shape[1]\n",
    "m_outputs = len(Y[0])\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNeuralNetwork(n_inputs, m_outputs,[2000,500,50],drop_out_prob=0.1).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss() #nn.MSELoss() #nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150000], Loss: 0.0311 Accuracy 0.6274217585692996\n",
      "Epoch [1001/150000], Loss: 0.0046 Accuracy 0.5484351713859911\n",
      "Epoch [2001/150000], Loss: 0.0032 Accuracy 0.5812220566318927\n",
      "Epoch [3001/150000], Loss: 0.0028 Accuracy 0.6050670640834576\n",
      "Epoch [4001/150000], Loss: 0.0025 Accuracy 0.6453055141579732\n",
      "Epoch [5001/150000], Loss: 0.0024 Accuracy 0.6423248882265276\n",
      "Epoch [6001/150000], Loss: 0.0024 Accuracy 0.6527570789865872\n",
      "Epoch [7001/150000], Loss: 0.0024 Accuracy 0.6527570789865872\n",
      "Epoch [8001/150000], Loss: 0.0023 Accuracy 0.6527570789865872\n",
      "Epoch [9001/150000], Loss: 0.0023 Accuracy 0.6527570789865872\n",
      "Epoch [10001/150000], Loss: 0.0023 Accuracy 0.6527570789865872\n",
      "Epoch [11001/150000], Loss: 0.0023 Accuracy 0.6527570789865872\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m a,b\u001b[39m=\u001b[39mtrain(model,\u001b[39m150000\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39m#plt.plot(a,label=\"Loss\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(b,label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, output)\u001b[0m\n\u001b[1;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 43\u001b[0m loss_ar\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     44\u001b[0m \u001b[39m#predict\u001b[39;00m\n\u001b[1;32m     45\u001b[0m predictions \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39mtensor(X, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.title(\"Accuracy while training tactile model on different surfaces\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "a,b=train(model,150000)\n",
    "#plt.plot(a,label=\"Loss\")\n",
    "plt.plot(b,label=\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15000], Loss: 0.0744 Accuracy 0.6527570789865872\n",
      "Epoch [1001/15000], Loss: 0.0646 Accuracy 0.6527570789865872\n",
      "Epoch [2001/15000], Loss: 0.0648 Accuracy 0.6527570789865872\n",
      "Epoch [3001/15000], Loss: 0.0647 Accuracy 0.6527570789865872\n",
      "Epoch [4001/15000], Loss: 0.0646 Accuracy 0.6527570789865872\n",
      "Epoch [5001/15000], Loss: 0.0647 Accuracy 0.6527570789865872\n",
      "Epoch [6001/15000], Loss: 0.0646 Accuracy 0.6527570789865872\n",
      "Epoch [7001/15000], Loss: 0.0648 Accuracy 0.6527570789865872\n",
      "Epoch [8001/15000], Loss: 0.0641 Accuracy 0.6527570789865872\n",
      "Epoch [9001/15000], Loss: 0.0648 Accuracy 0.6527570789865872\n",
      "Epoch [10001/15000], Loss: 0.0652 Accuracy 0.6527570789865872\n",
      "Epoch [11001/15000], Loss: 0.0648 Accuracy 0.6527570789865872\n",
      "Epoch [12001/15000], Loss: 0.0649 Accuracy 0.6527570789865872\n",
      "Epoch [13001/15000], Loss: 0.0650 Accuracy 0.6527570789865872\n",
      "Epoch [14001/15000], Loss: 0.0646 Accuracy 0.6527570789865872\n"
     ]
    }
   ],
   "source": [
    "# Define the size of the input (n) and output (m) layers\n",
    "n_inputs = X.shape[1]\n",
    "m_outputs = len(Y[0])\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create the neural network\n",
    "model = SimpleNeuralNetwork(n_inputs, m_outputs,[2000,500,50],drop_out_prob=0.1).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #nn.MSELoss() #nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "plt.title(\"Accuracy while training tactile model on different surfaces\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "a,b=train(model,15000)\n",
    "#plt.plot(a,label=\"Loss\")\n",
    "plt.plot(b,label=\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "     model.eval()\n",
    "     predictions = model(torch.tensor(data_test, dtype=torch.float32).to(device))\n",
    "\n",
    "p=predict(predictions.cpu(),should_be=labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
